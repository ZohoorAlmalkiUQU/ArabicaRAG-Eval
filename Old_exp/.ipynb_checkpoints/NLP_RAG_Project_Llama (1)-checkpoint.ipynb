{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61c367aa-e0a3-4116-bda7-7b81404211fd",
      "metadata": {
        "id": "61c367aa-e0a3-4116-bda7-7b81404211fd"
      },
      "source": [
        "<center>\n",
        "<p style=\"text-align:center\"><img alt=\"Ragas\" src=\"https://github.com/explodinggradients/ragas/blob/main/docs/_static/imgs/logo.png?raw=true\" width=\"400\"><br><a href=\"https://arize.com/docs/phoenix/\">Phoenix Docs</a> | <a href=\"https://github.com/explodinggradients/ragas\">Ragas</a> | <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
        "</p>\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0baf25a1-02bc-43c7-82e9-93e362485b74",
      "metadata": {
        "id": "0baf25a1-02bc-43c7-82e9-93e362485b74"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Building a baseline for a RAG pipeline is not usually difficult, but enhancing it to make it suitable for real-world Arabic question-answering tasks requires careful evaluation, observability, and iterative refinement. Choosing the right tools, retrieval strategies, and LLM configurations can be challenging, especially when working with large and diverse datasets.\n",
        "\n",
        "This notebook demonstrates how to **evaluate, visualize, and analyze an Arabic RAG pipeline** using a combination of powerful open-source libraries:\n",
        "\n",
        "- **[Ragas](https://docs.ragas.io/en/stable/)** for evaluation metrics such as faithfulness, answer correctness, context precision, and context recall  \n",
        "- **Arize AIâ€™s [Phoenix](https://arize.com/docs/phoenix)** for tracing, debugging, embedding visualization, and cluster analysis  \n",
        "- **[LlamaIndex](https://docs.llamaindex.ai/en/stable/)** for building RAG pipelines, managing document indexes, and connecting retrieval to LLMs  \n",
        "\n",
        "In this tutorial, instead of using arXiv PDFs, we will work with the **ArabicaQA dataset**, a large-scale Arabic machine-reading comprehension (MRC) dataset containing questions, contexts, and ground-truth answers. This allows us to build and evaluate an **Arabic-focused RAG system** in a realistic setting.\n",
        "\n",
        "> â„¹ï¸ **Note:**  \n",
        "> This notebook uses **open-source HuggingFace LLMs** (such as LLaMA or Gemma) and **does not require an OpenAI API key**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dcb4058",
      "metadata": {
        "id": "1dcb4058"
      },
      "source": [
        "## 2. Install Dependencies and Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4899e7a-43ef-4ae7-8f12-0024037a0b43",
      "metadata": {
        "id": "c4899e7a-43ef-4ae7-8f12-0024037a0b43"
      },
      "source": [
        "Install and import Python dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f2d18e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f2d18e80",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "17256195-3c77-4b0c-e967-c6d81d61c7d6",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.3)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: httpx<0.28 in /usr/local/lib/python3.12/dist-packages (0.27.2)\n",
            "Requirement already satisfied: openinference-instrumentation>=0.1.38 in /usr/local/lib/python3.12/dist-packages (0.1.42)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.5)\n",
            "Requirement already satisfied: llama-index>=0.11.0 in /usr/local/lib/python3.12/dist-packages (0.14.9)\n",
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Requirement already satisfied: ragas[langchain] in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: arize-phoenix[embeddings,llama-index] in /usr/local/lib/python3.12/dist-packages (12.19.0)\n",
            "\u001b[33mWARNING: ragas 0.4.0 does not provide the extra 'langchain'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy<3.0.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (2.0.2)\n",
            "Requirement already satisfied: datasets>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (4.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.12.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.4.4)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (5.6.3)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.20.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (13.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (4.67.1)\n",
            "Requirement already satisfied: instructor in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.13.0)\n",
            "Requirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (11.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (3.6)\n",
            "Requirement already satisfied: scikit-network in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.33.5)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.4.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.1.0)\n",
            "\u001b[33mWARNING: arize-phoenix 12.19.0 does not provide the extra 'llama-index'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aioitertools in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.13.0)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.21.0)\n",
            "Requirement already satisfied: alembic<2,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.17.2)\n",
            "Requirement already satisfied: arize-phoenix-client>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.26.0)\n",
            "Requirement already satisfied: arize-phoenix-evals>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (2.6.1)\n",
            "Requirement already satisfied: arize-phoenix-otel>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.14.0)\n",
            "Requirement already satisfied: authlib in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.6.5)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (6.2.2)\n",
            "Requirement already satisfied: email-validator in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (2.3.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.118.3)\n",
            "Requirement already satisfied: grpc-interceptor in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.15.4)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.76.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (3.1.6)\n",
            "Requirement already satisfied: jmespath in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.0.1)\n",
            "Requirement already satisfied: openinference-semantic-conventions>=0.1.20 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.1.25)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.60b0)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (3.11.4)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.23.1)\n",
            "Requirement already satisfied: protobuf>=4.25.8 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (18.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (2.9.0.post0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.0.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix[embeddings,llama-index]) (2.0.44)\n",
            "Requirement already satisfied: sqlean-py>=3.45.1 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (3.50.4.5)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.48.0)\n",
            "Requirement already satisfied: strawberry-graphql==0.270.1 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.270.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (4.15.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.38.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.17.2 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (1.17.3)\n",
            "Requirement already satisfied: fast-hdbscan>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.60.0 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.60.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (from arize-phoenix[embeddings,llama-index]) (0.5.9.post2)\n",
            "Requirement already satisfied: graphql-core<3.4.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from strawberry-graphql==0.270.1->arize-phoenix[embeddings,llama-index]) (3.2.7)\n",
            "Requirement already satisfied: packaging>=23 in /usr/local/lib/python3.12/dist-packages (from strawberry-graphql==0.270.1->arize-phoenix[embeddings,llama-index]) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.28) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.28) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.28) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.28) (0.16.0)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation>=0.1.38) (1.39.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.47)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (0.5.3)\n",
            "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.9 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (0.14.9)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (0.5.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (0.9.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (0.6.10)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (0.5.5)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (0.5.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.11.0) (3.9.1)\n",
            "Requirement already satisfied: torch<3,>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-huggingface) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers<5,>=4.37.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (4.57.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic<2,>=1.3.0->arize-phoenix[embeddings,llama-index]) (1.3.10)\n",
            "Requirement already satisfied: jsonpath-ng in /usr/local/lib/python3.12/dist-packages (from arize-phoenix-evals>=2.6.1->arize-phoenix[embeddings,llama-index]) (1.7.0)\n",
            "Requirement already satisfied: pystache in /usr/local/lib/python3.12/dist-packages (from arize-phoenix-evals>=2.6.1->arize-phoenix[embeddings,llama-index]) (0.6.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (3.20.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (2.32.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->ragas[langchain]) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (0.36.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (3.13.2)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (2.11.5)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (4.5.0)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (80.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (0.9.0)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index>=0.11.0) (0.1.35)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index>=0.11.0) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index>=0.11.0) (0.7.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index>=0.11.0) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.11.0) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index>=0.11.0) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index>=0.11.0) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index>=0.11.0) (2025.11.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.60.0->arize-phoenix[embeddings,llama-index]) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->arize-phoenix[embeddings,llama-index]) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->arize-phoenix[embeddings,llama-index]) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix[embeddings,llama-index]) (3.2.4)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (3.5.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (1.12.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib->arize-phoenix[embeddings,llama-index]) (43.0.3)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator->arize-phoenix[embeddings,llama-index]) (2.8.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (0.17.0)\n",
            "Requirement already satisfied: pre-commit>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (4.5.0)\n",
            "Requirement already satisfied: ty>=0.0.1a23 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (0.0.1a31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->arize-phoenix[embeddings,llama-index]) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas[langchain]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas[langchain]) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas[langchain]) (1.5.4)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (1.0.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (0.4.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api->openinference-instrumentation>=0.1.38) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix[embeddings,llama-index]) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix[embeddings,llama-index]) (1.39.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.39.0->opentelemetry-exporter-otlp->arize-phoenix[embeddings,llama-index]) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.39.0->opentelemetry-exporter-otlp->arize-phoenix[embeddings,llama-index]) (1.39.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn->arize-phoenix[embeddings,llama-index]) (0.5.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index>=0.11.0) (2.8)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (3.26.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=4.0.0->ragas[langchain]) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.38) (3.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community->ragas[langchain]) (1.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (0.4.2)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.11.0) (0.6.54)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ragas[langchain]) (0.1.2)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (3.5.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (2.6.15)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (1.9.1)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (20.35.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->ragas[langchain]) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->ragas[langchain]) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->ragas[langchain]) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib->arize-phoenix[embeddings,llama-index]) (2.0.0)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.12/dist-packages (from jsonpath-ng->arize-phoenix-evals>=2.6.1->arize-phoenix[embeddings,llama-index]) (3.11)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix[embeddings,llama-index]) (2.23)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas[langchain]) (0.4.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.9->llama-index>=0.11.0) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U \"ragas[langchain]\" pypdf \"arize-phoenix[llama-index,embeddings]\" \"openai>=1.0.0\" pandas \"httpx<0.28\" \"openinference-instrumentation>=0.1.38\" langchain-core langchain pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "62030782-aa1d-4671-a028-484a6249109a",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "62030782-aa1d-4671-a028-484a6249109a"
      },
      "outputs": [],
      "source": [
        "!pip install \"llama-index>=0.11.0\" \"llama-index-llms-huggingface\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eb44a37e-b84b-4122-b289-c74c0e1b9820",
      "metadata": {
        "id": "eb44a37e-b84b-4122-b289-c74c0e1b9820",
        "outputId": "0f782428-c5a6-451c-ba05-2099f053ae43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True\n",
            "Torch CUDA build: 12.6\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "print(\"Torch CUDA build:\", torch.version.cuda)\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "02304338",
      "metadata": {
        "id": "02304338"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display the complete contents of dataframe cells.\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # It will prompt you for your token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "15eabfae8b5f4f66860fc8315d697d0c",
            "aa831a17faf745a697dfdb99464112df",
            "df74d8aadad1439fbb62b7950d14d581",
            "2d5089806ae64af49a3f4b4eb890bca2",
            "c79129143ee24c0e8c744949ff52f4a3",
            "0febab2713e143898bb5fa1e19067c8e",
            "91edbef68eb34676a4b1c2113b207292",
            "c77c50ab999846e9bcf645117039f116",
            "bc85590f5d9941eb96c5bdb6773dda0a",
            "8a22f4dc541d43dbab55ad6922423c23",
            "695093a2ef9342e59192506d89b2c03b",
            "e4a5fed5a95e4aeeb232a93a4da22661",
            "aaeb675240224f379e336c79f64787f6",
            "804ca7971e514a8a91d2966b36753535",
            "1bb00b2b25f74c00b42ac36352ba54a9",
            "1cdf3e9ee3ea467cbba997bcf04a1654",
            "911ba697dd77497ebc75c4abf0ea888b",
            "246ead667b7b4f8d903bbcd1a34daba1",
            "a8615b9165174e01af76f346106dae2a",
            "f6a3c4618fd049db9c15b217fecce8d8"
          ]
        },
        "id": "yeboexzS_Xf-",
        "outputId": "2e258156-57b3-4ab8-d9b2-ef4f7e3056ff"
      },
      "id": "yeboexzS_Xf-",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15eabfae8b5f4f66860fc8315d697d0c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "yIsaUyZm0o8H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIsaUyZm0o8H",
        "outputId": "c51518d3-7683-47df-8425-7ac0bb3b4d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First model parameter device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-480113908.py:52: DeprecationWarning: LlamaIndexLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
            "  evaluator_llm = LlamaIndexLLMWrapper(llm)   # deprecation warning is OK for now\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.settings import Settings\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Reset any previous LLM in LlamaIndex\n",
        "Settings.llm = None\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# --- Model ---\n",
        "if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=dtype,\n",
        "    )\n",
        "\n",
        "print(\"First model parameter device:\", next(model.parameters()).device)\n",
        "\n",
        "# --- Wrap in LlamaIndex LLM ---\n",
        "llm = HuggingFaceLLM(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    model_name=model_name,\n",
        "    tokenizer_name=model_name,\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\n",
        "        \"do_sample\": False,   # greedy, deterministic (good for evaluation)\n",
        "    },\n",
        ")\n",
        "\n",
        "from ragas.llms import LlamaIndexLLMWrapper\n",
        "evaluator_llm = LlamaIndexLLMWrapper(llm)   # deprecation warning is OK for now\n",
        "\n",
        "# Register with LlamaIndex (if you use it elsewhere)\n",
        "Settings.llm = llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9373c880",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9373c880",
        "outputId": "e06f5734-2066-4864-b5e1-902112653c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
            "  next(self.gen)\n",
            "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒ To view the Phoenix app in your browser, visit https://x3td0ccbc9n5-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
            "ðŸ“– For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
            "ðŸ“º Opening a view to the Phoenix app. The app is running at https://x3td0ccbc9n5-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7a8663c48530>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"1000\"\n",
              "            src=\"https://x3td0ccbc9n5-496ff2e9c6d22116-6006-colab.googleusercontent.com/\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import phoenix as px\n",
        "\n",
        "(session := px.launch_app()).view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a253f768-5570-46d5-976b-55fc7730c7ec",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "a253f768-5570-46d5-976b-55fc7730c7ec",
        "outputId": "03ab4b76-60ea-4b5a-924e-6dd5aa847620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openinference-instrumentation-langchain in /usr/local/lib/python3.12/dist-packages (0.1.55)\n",
            "Requirement already satisfied: openinference-instrumentation-llama-index in /usr/local/lib/python3.12/dist-packages (4.3.9)\n",
            "Requirement already satisfied: openinference-instrumentation>=0.1.27 in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-langchain) (0.1.42)\n",
            "Requirement already satisfied: openinference-semantic-conventions>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-langchain) (0.1.25)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-langchain) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-langchain) (0.60b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-langchain) (0.60b0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-langchain) (1.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-llama-index) (4.15.0)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation>=0.1.27->openinference-instrumentation-langchain) (1.39.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api->openinference-instrumentation-langchain) (8.7.0)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-langchain) (25.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->openinference-instrumentation-langchain) (3.23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openinference-instrumentation-langchain openinference-instrumentation-llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3e3c8b98",
      "metadata": {
        "id": "3e3c8b98",
        "outputId": "c9a0ef90-90e9-4b77-9680-dba88fe6b892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
            "|  Phoenix Project: default\n",
            "|  Span Processor: SimpleSpanProcessor\n",
            "|  Collector Endpoint: localhost:4317\n",
            "|  Transport: gRPC\n",
            "|  Transport Headers: {}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n",
            "âœ… LangChain tracing enabled!\n",
            "âš ï¸  LlamaIndex tracing skipped due to version compatibility\n"
          ]
        }
      ],
      "source": [
        "from phoenix.otel import register\n",
        "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "\n",
        "tracer_provider = register()\n",
        "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "\n",
        "print(\"âœ… LangChain tracing enabled!\")\n",
        "print(\"âš ï¸  LlamaIndex tracing skipped due to version compatibility\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f707d3-e921-4f81-bbfb-a2ddb917c79d",
      "metadata": {
        "id": "78f707d3-e921-4f81-bbfb-a2ddb917c79d"
      },
      "source": [
        "## 4. Load our dataset (ArabicaQA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0959578e-0082-4bbc-8bf9-b7503fc53312",
      "metadata": {
        "id": "0959578e-0082-4bbc-8bf9-b7503fc53312",
        "outputId": "b966efe4-77ac-4a7a-fb12-1646ffacfa0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "4ca2efc23701466bafa40c10f68c903f",
            "f5829965ac8d41e18f8aea6f20f34cf5",
            "ac38cadeb5c247d58815b692baef1ff3",
            "b4cb0d3f9f84482fab0dbeea0e3dacbe",
            "81399104933e47d7b5daad44eec270bc",
            "43f8f168741641bea4338cac597740a4",
            "d8344e6e031145279248afc29e96e77d",
            "3464cf19999a4cf3a6099933f7ce0bee",
            "daac160ba1fe4fcfb04460b43e9975a3",
            "1b440082ea33409aabf014e3dc0fa6fe",
            "6626c17c121d4f288cf6ea0b7891d99d",
            "276e1787033e4adca799af84406b0496",
            "64403210bcbc49759b75cd7faeab9919",
            "93e80b6099ec4eb4b4e9c6176c606934",
            "c655c4dd2fe74302951138408999195a",
            "07a2b827d22447e0bd6631a2c7bfdc0b",
            "8b7283351e084f8ab852f6c9c2a2b10e",
            "b410a344553a4b55968591ed4985434b",
            "fffe1fd7922b46a6a5cfd1f009bad314",
            "561836cb8b194f37b80b40bd6326d7b4",
            "a4b0041365cf4287ad2c94977368741a",
            "978209e344ca45e3bffedab613a9c39a",
            "1189009805f54c5db282cb2cd793b3c0",
            "189b1432533743d6978a7e43294db82c",
            "8cb6017a5f1f4365b4080a9b0a77ae9b",
            "3fdfc39d23d047e6becdde184b030044",
            "add2bc87ee4d44d1a3b75188b56f4b70",
            "11cab03a451a43c2b02433dfc8923854",
            "d7df181fba1c45caa9b234d06f961d14",
            "f35c504ebe1940b19b1b8ac3edd759d0",
            "f34a3bf099664526963205d30e9ccdf3",
            "4648748534d74556b63b2bd4bff42cd7",
            "46515601138e4b96b845e367f89e3c37"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ca2efc23701466bafa40c10f68c903f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "MRC/test.json:   0%|          | 0.00/19.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "276e1787033e4adca799af84406b0496"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1189009805f54c5db282cb2cd793b3c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    test: Dataset({\n",
              "        features: ['paragraphs'],\n",
              "        num_rows: 2108\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the MRC split of ArabicaQA from Hugging Face\n",
        "arabica_mrc = load_dataset(\n",
        "    \"abdoelsayed/ArabicaQA\",\n",
        "    data_files={\n",
        "        #\"train\": \"MRC/train.json\",\n",
        "        #\"validation\": \"MRC/val.json\",\n",
        "        \"test\": \"MRC/test.json\",\n",
        "    },\n",
        "    field=\"data\",  # the JSON has a top-level field called \"data\"\n",
        ")\n",
        "\n",
        "arabica_mrc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437cd4f0-4be6-453b-93c5-07e9f8f226a1",
      "metadata": {
        "id": "437cd4f0-4be6-453b-93c5-07e9f8f226a1"
      },
      "source": [
        "In ArabicaQA (MRC task):\n",
        "The MRC test set contains:\n",
        "- question\n",
        "-  context (the correct paragraph from Wikipedia)\n",
        "-  answers (ground truth answer spans)\n",
        "\n",
        "We will use:\n",
        "- context â†’ to build your vector index (RAG knowledge base)\n",
        "- question â†’ to query your RAG\n",
        "- answers â†’ ground truth for RAGAS evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b89939d4-115b-41c9-8753-8ec9d05a2494",
      "metadata": {
        "id": "b89939d4-115b-41c9-8753-8ec9d05a2494",
        "outputId": "3c213c7f-8280-44e1-e483-3345cdf362b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'paragraphs': [{'context': 'Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).',\n",
              "   'document_id': 1718795,\n",
              "   'qas': [{'answers': [{'answer_category': None,\n",
              "       'answer_end': 483,\n",
              "       'answer_id': 1067193,\n",
              "       'answer_start': 424,\n",
              "       'document_id': 1718795,\n",
              "       'question_id': 1166998,\n",
              "       'text': 'ÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ'}],\n",
              "     'id': 1166998,\n",
              "     'is_impossible': False,\n",
              "     'question': 'Ù…Ù† Ø§Ù„Ø°ÙŠ ÙŠØªØ±Ø£Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ'},\n",
              "    {'answers': [{'answer_category': None,\n",
              "       'answer_end': 1687,\n",
              "       'answer_id': 1067194,\n",
              "       'answer_start': 1560,\n",
              "       'document_id': 1718795,\n",
              "       'question_id': 1166999,\n",
              "       'text': ' ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD)'}],\n",
              "     'id': 1166999,\n",
              "     'is_impossible': False,\n",
              "     'question': 'Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø§Ù„Ø¨Ø§Ø±Ø²Ø© Ø§Ù„ØªÙŠ Ø­Ù‚Ù‚Ù‡Ø§ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ'},\n",
              "    {'answers': [{'answer_category': None,\n",
              "       'answer_end': 134,\n",
              "       'answer_id': 1067188,\n",
              "       'answer_start': 56,\n",
              "       'document_id': 1718795,\n",
              "       'question_id': 1166993,\n",
              "       'text': 'Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ'}],\n",
              "     'id': 1166993,\n",
              "     'is_impossible': False,\n",
              "     'question': 'Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ'},\n",
              "    {'answers': [{'answer_category': None,\n",
              "       'answer_end': 201,\n",
              "       'answer_id': 1067189,\n",
              "       'answer_start': 136,\n",
              "       'document_id': 1718795,\n",
              "       'question_id': 1166994,\n",
              "       'text': 'ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ'}],\n",
              "     'id': 1166994,\n",
              "     'is_impossible': False,\n",
              "     'question': 'Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ'},\n",
              "    {'answers': [{'answer_category': None,\n",
              "       'answer_end': 249,\n",
              "       'answer_id': 1067190,\n",
              "       'answer_start': 202,\n",
              "       'document_id': 1718795,\n",
              "       'question_id': 1166995,\n",
              "       'text': 'ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª.'}],\n",
              "     'id': 1166995,\n",
              "     'is_impossible': False,\n",
              "     'question': 'Ù…Ø§ Ù‡ÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ'},\n",
              "    {'answers': [{'answer_category': None,\n",
              "       'answer_end': 300,\n",
              "       'answer_id': 1067191,\n",
              "       'answer_start': 274,\n",
              "       'document_id': 1718795,\n",
              "       'question_id': 1166996,\n",
              "       'text': 'Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ'}],\n",
              "     'id': 1166996,\n",
              "     'is_impossible': False,\n",
              "     'question': 'Ù…ØªÙŠ ØªÙ… Ø£Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ'},\n",
              "    {'answers': [{'answer_category': None,\n",
              "       'answer_end': 337,\n",
              "       'answer_id': 1067192,\n",
              "       'answer_start': 301,\n",
              "       'document_id': 1718795,\n",
              "       'question_id': 1166997,\n",
              "       'text': 'Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶'}],\n",
              "     'id': 1166997,\n",
              "     'is_impossible': False,\n",
              "     'question': 'Ù…Ù† Ø§Ù„Ø°ÙŠ Ù‚Ø§Ù… Ø¨Ø£Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ'}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_ds = arabica_mrc[\"test\"]\n",
        "test_ds[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b9abf95-d18e-4880-a53d-70188287c5b4",
      "metadata": {
        "id": "9b9abf95-d18e-4880-a53d-70188287c5b4"
      },
      "source": [
        "## Convert the test set into a clean RAG-ready DataFrame\n",
        "For RAG, we need:\n",
        "- knowledge documents â†’ contexts\n",
        "- evaluation questions â†’ questions\n",
        "- ground truth â†’ answers\n",
        "\n",
        "ArabicaQA test samples have 1 context + multiple Q/A pairs, so we must flatten them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fca413e9-d8c9-4f4b-8b8c-975061798445",
      "metadata": {
        "scrolled": true,
        "id": "fca413e9-d8c9-4f4b-8b8c-975061798445",
        "outputId": "b89d4c67-9163-4711-b8e8-10004b509fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    context  \\\n",
              "0  Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).   \n",
              "1  Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).   \n",
              "2  Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).   \n",
              "3  Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).   \n",
              "4  Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).   \n",
              "\n",
              "                                                          question  \\\n",
              "0                       Ù…Ù† Ø§Ù„Ø°ÙŠ ÙŠØªØ±Ø£Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ   \n",
              "1  Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø§Ù„Ø¨Ø§Ø±Ø²Ø© Ø§Ù„ØªÙŠ Ø­Ù‚Ù‚Ù‡Ø§ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ   \n",
              "2                               Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ   \n",
              "3                      Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ   \n",
              "4                          Ù…Ø§ Ù‡ÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ   \n",
              "\n",
              "                                                                                                                      ground_truth  \n",
              "0                                                                      ÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ  \n",
              "1   ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD)  \n",
              "2                                                   Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ  \n",
              "3                                                                ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ  \n",
              "4                                                                                  ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f41d92e3-19b6-4f33-bf93-e22de6c93bd9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ground_truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).</td>\n",
              "      <td>Ù…Ù† Ø§Ù„Ø°ÙŠ ÙŠØªØ±Ø£Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ</td>\n",
              "      <td>ÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).</td>\n",
              "      <td>Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø§Ù„Ø¨Ø§Ø±Ø²Ø© Ø§Ù„ØªÙŠ Ø­Ù‚Ù‚Ù‡Ø§ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ</td>\n",
              "      <td>ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).</td>\n",
              "      <td>Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ</td>\n",
              "      <td>Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).</td>\n",
              "      <td>Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ</td>\n",
              "      <td>ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆÙŠØ¹Ø±Ù Ø§Ø®ØªØµØ§Ø±Ù‹Ø§ Ø¨Ù…Ø±ØµØ¯ Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡Ùˆ Ù…Ø±ÙƒØ² Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ø¸Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø¶Ø±ÙŠØ© Ù„Ø±ØµØ¯ Ø³ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙˆØ¶Ø¹ ØªØµÙˆØ± Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª. Ø§Ù„ØªØ£Ø³ÙŠØ³. Ø¨Ø¯Ø£Øª Ø£ÙˆÙ„ Ø®Ø·ÙˆØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±ØµØ¯ ÙÙŠ Ø¹Ø§Ù… 2009Ù…ØŒ Ù…Ù† Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø› ÙˆÙ…Ù† Ø«Ù… Ø£ÙÙ‚Ø±ÙŽÙ‘ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù„Ø³ ÙˆÙ„Ø¬Ù†Ø© ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙ…Ø±ÙƒØ². ÙˆÙŠØªØ±Ø£Ø³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø±ØµØ¯ Ø£Ù…ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø± Ø¹Ø¶ÙˆØ§ØŒ ÙˆØªØªÙƒÙˆÙ† Ø§Ù„Ù„ÙŽÙ‘Ø¬Ù†Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ù† 15 Ø¹Ø¶ÙˆÙ‹Ø§ ÙŠÙ…Ø«Ù„ÙˆÙ† Ø£Ù‡Ù… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ. Ø§Ù„Ù…Ù‡Ø§Ù…. ÙÙŠ Ø¹Ø§Ù… 1434Ù‡Ù€ Ø£Ø·Ù„Ù‚ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ Ø¶Ù…Øª (80) Ù…Ø¤Ø´Ø±Ø§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø¥Ù‚Ø±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ ØªÙ„Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ø¹Ø§Ù… 1435Ù‡Ù€ØŒ ÙˆØ´Ù…Ù„Øª Ø§Ù„Ø­Ø¯Ù‘ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©. ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø±ØµØ¯ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ØªÙ†Ù…ÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¶Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬Ø²Ø© Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø±Ø§Ù‡Ù† ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ø®Ø·Ø· Ø§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ¯Ø§Ù…Ø©ØŒ ÙˆÙŠØªÙ… Ø°Ù„Ùƒ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø±ØµØ¯ Ø¹Ø§Ù„Ù…ÙŠ ÙŠÙˆÙØ± Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø±ØµØ¯ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆÙŠØ³Ù‡Ù„ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¶Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ØªÙÙ…ÙƒÙÙ‘Ù† Ù…Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ¹ÙƒØ³ Ø®ØµÙˆØµÙŠØ© Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© 2030ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠ. Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª. ÙÙŠ Ù…Ø§Ø±Ø³ 2017 Ø­ØµÙ„ Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© WCCD ISO 37120 Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙÙŠ ÙƒÙ†Ø¯Ø§ (WCCD).</td>\n",
              "      <td>Ù…Ø§ Ù‡ÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø±ØµØ¯ Ø§Ù„Ø­Ø¶Ø±ÙŠ Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ</td>\n",
              "      <td>ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f41d92e3-19b6-4f33-bf93-e22de6c93bd9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f41d92e3-19b6-4f33-bf93-e22de6c93bd9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f41d92e3-19b6-4f33-bf93-e22de6c93bd9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def flatten_arabica_mrc(dataset):\n",
        "    rows = []\n",
        "\n",
        "    for item in dataset:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answers = qa[\"answers\"]\n",
        "                # Most have exactly 1 answer\n",
        "                gt_answer = answers[0][\"text\"] if len(answers) > 0 else \"\"\n",
        "\n",
        "                rows.append({\n",
        "                    \"context\": context,\n",
        "                    \"question\": question,\n",
        "                    \"ground_truth\": gt_answer\n",
        "                })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "df_test = flatten_arabica_mrc(test_ds)\n",
        "df_test.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bec44903-dc65-4e6e-b3f9-10afeb90f469",
      "metadata": {
        "id": "bec44903-dc65-4e6e-b3f9-10afeb90f469",
        "outputId": "e11c0ec9-c137-424f-d051-2f3c5c0e0fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13970, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9bdc6975-5a54-4004-a12c-572dedca968a",
      "metadata": {
        "id": "9bdc6975-5a54-4004-a12c-572dedca968a",
        "outputId": "652e39f4-6f58-42bd-b442-c42a690d0ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Shuffle the dataset safely\n",
        "df_shuffled = df_test.sample(frac=1, random_state=42)\n",
        "\n",
        "# Take a representative sample\n",
        "df_sample = df_shuffled.head(300)\n",
        "\n",
        "df_sample.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d45d76b-ac44-45dd-82d3-9842bb9f499d",
      "metadata": {
        "id": "4d45d76b-ac44-45dd-82d3-9842bb9f499d"
      },
      "source": [
        "## Deduplicate contexts â†’ build RAG documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2c7b0ac5-7468-441d-90ff-cd04da08219f",
      "metadata": {
        "id": "2c7b0ac5-7468-441d-90ff-cd04da08219f",
        "outputId": "7f093c6b-9841-4521-d8a6-ce19d0c53ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Make a list of unique contexts (documents)\n",
        "unique_contexts = df_sample[\"context\"].drop_duplicates().tolist()\n",
        "\n",
        "len(unique_contexts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b7fe25c2-f691-4d57-bbef-a079c89ec34c",
      "metadata": {
        "scrolled": true,
        "id": "b7fe25c2-f691-4d57-bbef-a079c89ec34c",
        "outputId": "2bbcfea0-ca7f-412b-a1e4-c8312dfa28e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø§Ù„Ø³ÙŠØ®ÙŠØ© (Ø¨Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ÙŠÙŽÙ‘Ø©: à¨¸à¨¿à©±à¨–à©€) Ù‡ÙŠ Ø¯ÙŠØ§Ù†Ø© ØªÙˆØ­ÙŠØ¯ÙŠØ© Ø¯Ø§Ø±Ù…ÙŠÙŽÙ‘Ø© Ù†Ø´Ø£Øª ÙÙŠ Ø´Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù†Ø¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø±. ÙˆØªØ£ØªÙŠ ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®ÙŠØ©Â» Ù…Ù† ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®Â» ÙˆÙ‡ÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ ØªØ£ØªÙŠ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø³Ù†Ø³ÙƒØ±ÙŠØªÙŠ Ø§Ù„ØªÙŠ ØªØ¹Ù†ÙŠ Ø§Ù„ØªÙ„Ù…ÙŠØ° Ùˆ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¯ Ø£Ùˆ Ø§Ù„ØªØ§Ø¨Ø¹. ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø¯ÙŠØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£ÙƒØ¨Ø± Ø§Ù„Ø¯ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…. ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ØªÙ‚Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙˆØ¶ÙØ­ÙŽØª ÙÙŠ ÙƒØªØ§Ø¨Ù‡Ù… Ø§Ù„Ù…Ù‚Ø¯Ø³ Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ØŒ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† ÙˆØ§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ø®Ø§Ù„Ù‚ Ø§Ù„ÙˆØ§Ø­Ø¯ØŒ ÙˆØ§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙˆØ§Ø© Ù„Ù„Ø¨Ø´Ø±ÙŠØ© Ø¬Ù…Ø¹Ø§Ø¡ØŒ ÙˆØ§Ù„Ø§Ù†Ø®Ø±Ø§Ø· ÙÙŠ Ø®Ø¯Ù…Ø© Ù†ÙƒØ±Ø§Ù† Ø§Ù„Ø°Ø§ØªØŒ ÙˆØ§Ù„Ø³Ø¹ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆØ§Ø²Ø¯Ù‡Ø§Ø±Ù‡Ø§ØŒ ÙˆØ¥ØªØ¨Ø§Ø¹ Ø³Ù„ÙˆÙƒ Ù…Ø¹ÙŠØ´Ø© ØµØ§Ø¯Ù‚. ÙˆÙÙŠ Ø£ÙˆØ§Ø¦Ù„ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ÙˆØ§Ø­Ø¯ ÙˆØ§Ù„Ø¹Ø´Ø±ÙŠÙ† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø­ÙˆØ§Ù„ÙŠ 25 Ù…Ù„ÙŠÙˆÙ† Ø³ÙŠØ®ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆØªØ¹ÙŠØ´ Ø§Ù„ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ø£Ùˆ 76% (20 Ù…Ù„ÙŠÙˆÙ†) Ù…Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ØŒ Ù…ÙˆØ·Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø§Ù„Ù‡Ù†Ø¯ØŒ ÙˆÙŠØ¹ÙŠØ´ Ø­ÙˆØ§Ù„ÙŠ Ù…Ù„ÙŠÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø³Ø§Ø¨Ù‚Ø§Ù‹. ÙˆØ³Ø¨Ø¨ Ø§Ù†ØªØ´Ø§Ø±Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù‡Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ² Ø¹Ù„ÙŠÙ‡Ù… ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆØ¨ ÙˆÙ‡Ø¬Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ® Ø®Ø§Ø±Ø¬ Ø¨Ù„Ø§Ø¯Ù‡Ù…ØŒ Ø­ÙŠØ« Ø¨Ø¯Ø£Øª Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø®Ù„Ø§Ù„ Ø§Ù„Ù†ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø±ØŒ Ø¹Ù†Ø¯Ù…Ø§ Ø£ÙƒÙ…Ù„ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠÙˆÙ† Ø¶Ù…Ù‡Ù… Ù„Ù„Ø¨Ù†Ø¬Ø§Ø¨.\n",
            "ØªØ³ØªÙ†Ø¯ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù„ÙŠÙ… Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ù„Ù…Ø¤Ø³Ø³ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© ÙˆÙ‡Ùˆ Ø§Ù„ØºÙˆØ±Ùˆ Ù†Ø§Ù†Ø§ÙƒØŒ ÙˆØ®Ù„ÙØ§Ø¦Ù‡ Ø§Ù„ØªØ³Ø¹Ø© Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø±. Ù„Ù‚Ø¨ ØºÙˆØ±Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù…. Ø£Ù…Ø§ Ø§Ù„ØºÙˆØ±Ùˆ ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ù…Ù„Ù‚Ø¨ Ø¨Ø§Ù„Ø¹Ø§Ø´Ø±ØŒ Ø³Ø§Ù‡Ù… ÙÙŠ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø£Ø¬Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆÙƒØ§Ù† Ø¥Ø³Ù‡Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø¥Ø¶ÙØ§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„ØªÙŠ Ø£Ø³Ø³Ù‡Ø§ Ø£ÙˆÙ„Ù‹Ø§ Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù†Ø§Ù†Ø§Ùƒ Ø¯ÙŠÙ Ø¬ÙŠ ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø± Ø¥Ø³Ù‡Ø§Ù…Ù‹Ø§ Ø¬Ø¯ÙŠØ±Ù‹Ø§ Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©. ÙˆØ³Ù…Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ ÙƒØ®Ù„ÙŠÙØ© Ù„Ù‡ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ø£Ù†Ù‡Ù‰ Ø®Ø· Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø± ÙˆØ¬Ø¹Ù„ Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ­ÙŠ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ†ÙˆÙŠ Ù„Ù„Ø³ÙŠØ®. ÙˆØªØ±ÙØ¶ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø¨Ø£Ù† Ø£ÙŠ ØªÙ‚Ù„ÙŠØ¯ Ø¯ÙŠÙ†ÙŠ Ù…Ø¹ÙŠÙ† Ù„Ù‡ Ø§Ø­ØªÙƒØ§Ø± Ù„Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©. ÙˆØªØ·ÙˆØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø¯ÙŠÙ†ÙŠ. Ø­ÙŠØ« ØªØ¹Ø±Ø¶ Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø£ØªØ¨Ø§Ø¹ Ø§Ù„Ø³ÙŠØ® ÙˆÙ‡Ù… Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ± Ù„Ù„ØªØ¹Ø°ÙŠØ¨ ÙˆØ£Ø¹Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø¨Ø¹Ø¯ Ø±ÙØ¶Ù‡Ù… Ø§Ø¹ØªÙ†Ø§Ù‚ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. ÙˆØ£Ø«Ø§Ø± Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø®Ø§Ù„Ø³Ø§ ÙƒØ·Ù„Ø¨ Ù„Ø­Ù…Ø§ÙŠØ© Ø­Ø±ÙŠØ© Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø¯ÙŠÙ†.\n",
            "Ø§Ù„ØªØ§Ø±ÙŠØ®.\n",
            "ÙŠØ±ØªØ¨Ø· ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙˆØ«ÙŠÙ‚Ù‹Ø§ Ø¨ØªØ§Ø±ÙŠØ® Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±. Ù…Ù†Ø° Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…ØºÙˆÙ„ÙŠ Ù„Ù„Ù‡Ù†Ø¯ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ± Ø¬Ù‡Ø§Ù†ÙƒÙŠØ± (1605-1707)ØŒ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ ØµØ±Ø§Ø¹ Ù…Ø¹ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ù…ØºÙˆÙ„ØŒ Ù„Ø£Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªØ¤Ø«Ø± ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù‚Ø¨ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ù„Ù„Ù…ØºÙˆÙ„ ÙÙŠ Ø­ÙŠÙ† ØªØ¹ØªØ² Ø¨Ø§Ù„Ø£ÙˆÙ„ÙŠØ§Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. Ù‚ÙØªÙ„ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ø§Ø±Ø²ÙŠÙ† Ø¹Ù„Ù‰ ÙŠØ¯ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø§Ù†ØµÙŠØ§Ø¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ®. Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹ 10 Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®ØŒ Ø¹ÙØ°Ø¨ ÙˆØ£ÙØ¹Ø¯Ù… Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø£Ù†ÙØ³Ù‡Ù… (Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ±)ØŒ ÙˆØ£Ù‚Ø±Ø¨Ø§Ø¡ Ù…Ù‚Ø±Ø¨ÙŠÙ† Ù„Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù‚ÙØªÙ„Ùˆ Ø¨ÙˆØ­Ø´ÙŠØ© Ø¯ÙˆÙ† Ø±Ø­Ù…Ø© (Ù…Ø«Ù„ Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ† Ù…Ù† Ø§Ù„Ø¹Ù…Ø± 6 Ùˆ9 Ø³Ù†ÙˆØ§Øª)ØŒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„ØªÙŠ Ø¹ÙØ°Ø¨Øª ÙˆÙ‚ÙØªÙ„Øª (Ù…Ø«Ù„ Ø¨Ø§Ù†Ø¯Ø§ Ø¨Ù‡Ø§Ø¯ÙˆØ±ØŒ Ø¨Ù‡Ø§ÙŠ Ù…Ø§ØªÙŠ Ø¯Ø§Ø³ØŒ Ø¨Ù‡Ø§ÙŠ Ø³Ø§ØªÙŠ Ø¯Ø§Ø³ ÙˆØ¨Ø§Ù‡Ø§ÙŠ Ø¯ÙŠØ§Ù„Ø§)ØŒ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ø¨Ø±ÙŠÙ† Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø®Ø¶ÙˆØ¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ø³ÙƒØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù„Ù…Ø¹Ø§Ø±Ø¶Ø© Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ù…ØºÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ù….\n",
            "ØªÙ…ÙŠÙ‘Ø² Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙˆÙ†ÙØ¯Ø±Ø§Ù„ÙŠØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ø£Ù…Ø±Ø§Ø¡ ÙˆØ§Ù„Ø³ÙŠØ® ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ù…Ù‡Ø±Ø§Ø¬Ø§ Ø±Ø§Ù†Ø¬ÙŠØª Ø³ÙŠÙ†Øº Ø¨Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„ØªØ¹Ø§ÙŠØ´ Ø§Ù„Ø³Ù„Ù…ÙŠ ÙˆØ§Ù„ØªØ¹Ø¯Ø¯ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ù„Ø·Ø©. ÙŠØ¹Ø¯ ØªØ£Ø³ÙŠØ³ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¹Ø§Ø¯Ø©Ù‹ Ø°Ø±ÙˆØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØŒ Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙØªØ±Ø© Ø¬Ø§Ø¡Øª Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù„ØªØ´Ù…Ù„ ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§). Ø§Ø¹ØªÙ†Ù‚ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„ÙÙ„Ø§Ø­ÙŠÙ† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ø§Ù„Ø³ÙŠØ®ÙŠØ©. Ø£Ø®Ø° Ù‡Ø§Ø±ÙŠ Ø³ÙŠÙ†Øº Ù†Ø§Ù„ÙˆØ§ØŒ Ø§Ù„Ù‚Ø§Ø¦Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ø¬ÙŠØ´ Ø§Ù„Ø³ÙŠØ® Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ© Ø§Ù„ØºØ±Ø¨ÙŠØ©ØŒ Ø­Ø¯ÙˆØ¯ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥Ù„Ù‰ Ù…ØµØ¨ Ù…Ù…Ø± Ø®ÙŠØ¨Ø± (Ù‡Ùˆ Ù…Ù…Ø± Ø¬Ø¨Ù„ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø¨Ø§ÙƒØ³ØªØ§Ù†ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ø¹ Ø£ÙØºØ§Ù†Ø³ØªØ§Ù†). Ø¯Ù…Ø¬Øª Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ù„Ù…Ø§Ù†ÙŠØ© Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© ÙˆØ§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆØ­ÙƒÙˆÙ…ÙŠØ© Ù…Ø¨ØªÙƒØ±Ø©.\n",
            "Ø´Ù‡Ø¯Øª Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„ØªÙŠ Ø³Ø¨Ù‚Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ (ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø¥Ù„Ù‰ Ø¯ÙˆÙ„ØªÙŠÙ† Ù…Ø³ØªÙ‚Ù„ØªÙŠÙ†ØŒ Ø§Ù„Ù‡Ù†Ø¯ ÙˆØ¨Ø§ÙƒØ³ØªØ§Ù†) Ø³Ù†Ø© 1947ØŒ ØµØ±Ø§Ø¹Ù‹Ø§ Ø­Ø§Ø¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ (Ù…Ù†Ø·Ù‚Ø© Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ© ÙˆØ«Ù‚Ø§ÙÙŠØ© ÙˆØªØ§Ø±ÙŠØ®ÙŠØ© ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø¢Ø³ÙŠØ§ØŒ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©ØŒ ÙˆØªØ¶Ù… Ù…Ù†Ø§Ø·Ù‚ ÙÙŠ Ø´Ø±Ù‚ Ø¨Ø§ÙƒØ³ØªØ§Ù† ÙˆØ´Ù…Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯) Ø¨ÙŠÙ† Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†ØŒ Ø´Ù‡Ø¯ Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ù…Ù† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„ØºØ±Ø¨ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ù‡Ø¬Ø±Ø© Ø¯ÙŠÙ†ÙŠØ© Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø´Ø±Ù‚ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨. ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ø¶Ø±ØŒ ÙŠØ¹ÙŠØ´ ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø³ÙŠØ® ÙÙŠ ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯.\n",
            "Ø§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙƒØ§Ø±Ø³ Ø£Ùˆ Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³Ø©.\n",
            "Ø§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙØ§Øª Ù‡Ù… ÙŠØµØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„ØªØ²Ø§Ù… Ø¨Ù‡Ø§ ÙˆØ¹Ù†Ø¯Ù‡Ù… Ù…Ù† Ù„Ø§ ÙŠÙ„ØªØ²Ù… Ø¨Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³ ÙŠØµÙÙˆÙ‡ Ø¨ØµÙØ© Ø¨Ø§ØªØª patitØ£ÙŠ Ø§Ù„Ù…Ø±ØªØ¯ØŒ ÙˆÙ…Ù† ÙŠØ¯Ø®Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¬Ø¯ÙŠØ¯ ÙŠØ¬Ø¹Ù„ÙˆÙ‡ ÙŠØªØ¹ÙˆØ¯ Ø¹Ù„ÙŠÙ‡Ø§ ÙˆÙŠØ³Ù…ÙˆÙ‡ Ø§Ù„Ù…ØªÙƒÙŠÙ Ø§Ù„Ø¨Ø·Ø¦.\n",
            "Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®.\n",
            "Ø§Ù„ØºÙˆØ±Ùˆ ÙƒÙ„Ù…Ø© ØªØ¹Ù†ÙŠ Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆÙ‡Ù… Ø¹Ø´Ø±Ø© ØºÙˆØ±Ùˆ Ù„Ù„Ø³ÙŠØ®ÙŠØ© ÙˆÙ‡Ù…:\n",
            "\n",
            "---------------------------------------\n",
            "ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ (Ù‡Ø§Ù†ØºÙ„: ê·¸ í•´ ìš°ë¦¬ëŠ”)Ø› Ù‡Ùˆ Ù…Ø³Ù„Ø³Ù„ ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ ÙƒÙˆÙ…ÙŠØ¯ÙŠ ÙƒÙˆØ±ÙŠ Ø¬Ù†ÙˆØ¨ÙŠØŒ ÙŠØµÙ†Ù Ø¨Ø£Ù†Ù‡ Â«Ø£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø£ØµÙ„ÙŠ Ù„ÙŠ Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†Â». Ù…Ù† Ø¨Ø·ÙˆÙ„Ø© ÙˆÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠØŒ Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ SBS TV Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021 Ø¥Ù„Ù‰ 25 ÙŠÙ†Ø§ÙŠØ± 2022ØŒ Ø¨Ø« ÙƒÙ„ ÙŠÙˆÙ… Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† ÙˆØ§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¹Ø© 22:00 Ø¨ØªÙˆÙ‚ÙŠØª (KST). ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­Ù‹Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ù†ØªÙÙ„ÙŠÙƒØ³. Ø§Ù„Ù‚ØµØ©. Ø§Ù†ÙØµÙ„ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº (ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ) ÙˆÙŠÙˆÙ† Ø³Ùˆ (ÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠ) Ù‚Ø¨Ù„ 10 Ø³Ù†ÙˆØ§ØªØŒ Ù„ÙƒÙ† ÙŠØµØ¨Ø­ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø°ÙŠ ØµÙˆØ±ÙˆÙ‡ Ø®Ù„Ø§Ù„ Ø£ÙŠØ§Ù… Ø¯Ø±Ø§Ø³ØªÙ‡Ù…Ø§ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø´Ø§Ø¦Ø¹Ù‹Ø§ØŒ Ø¨Ø³Ø¨Ø¨Ù‡ Ø¹Ù„ÙŠÙ‡Ù…Ø§ Ø§Ù„ÙˆÙ‚ÙˆÙ Ø£Ù…Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¬Ø¯Ø¯Ø§ØŒ Ø¹Ù„Ù‰ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ø§ÙŠØ±ÙŠØ¯ÙˆÙ† Ø°Ù„Ùƒ. Ø·Ø§Ù‚Ù…. Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ø±Ø³Ø§Ù… ØªØ´ÙƒÙŠÙ„ÙŠ Ø­Ø± Ù…ÙØ¹Ù… Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ©. Ø®Ø¨ÙŠØ±Ø© Ø¹Ù„Ø§Ù‚Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©. Ù…Ø®Ø±Ø¬ ÙˆØ«Ø§Ø¦Ù‚ÙŠØŒ ÙˆÙ‡Ùˆ ÙƒØ§Ù† Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ù„ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº ÙˆØ§ÙŠÙˆÙ† Ø³Ùˆ. Ø£ÙŠØ¯ÙˆÙ„ Ù…Ø´Ù‡ÙˆØ±Ø© ØªØ­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ Ù…Ù†Ø° Ø¸Ù‡ÙˆØ±Ù‡Ø§. Ø§Ù„Ø¥Ù†ØªØ§Ø¬. ØµÙ†Ø§Ø¹Ø©. ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†ØŒ ÙˆÙ‡ÙŠ Ø£ÙˆÙ„ Ø¯Ø±Ø§Ù…Ø§ Ø£ØµÙ„ÙŠØ© Ù„Ù‡Ù…ØŒ Ø³ÙŠØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ© Ø³ÙˆØ¨Ø± Ù…ÙˆÙ† Ø¨ÙŠÙƒØªØ´Ø±Ø²ØŒ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ Ø§Ù„ØªØ§Ø¨Ø¹ Ù„Ø¥Ø³ Ø¨ÙŠ Ø¥Ø³ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø¨Ù€ Â«Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ø³Â». Ø·Ø§Ù‚Ù…. ÙÙŠ Ù…Ø§Ø±Ø³ 2021ØŒ Ø£Ø¹Ù„Ù† ØªØ£ÙƒÙŠØ¯ Ø§Ù†Ø¶Ù…Ø§Ù… ÙƒÙ„ Ù…Ù† ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ ÙˆÙƒÙŠÙ… Ø¯Ø§ Ù…ÙŠ ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØŒ ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨Ù‡Ù… Ø¨Ø¹Ø¯ Ø«Ù„Ø§Ø« Ø³Ù†ÙˆØ§ØªØŒ Ø¢Ø®Ø± Ù…Ø±Ø© Ø¸Ù‡Ø±ÙˆØ§ ÙÙŠÙ‡ ÙÙŠ Ù…Ø¹Ø§ Ø¨ÙÙŠÙ„Ù… Ø§Ù„ØºÙ…ÙˆØ¶ \"\" Ù„Ø¹Ø§Ù… 2018. ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø§Ù‚Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠ ÙÙŠ 8 ÙŠÙˆÙ„ÙŠÙˆ. Ø§Ù„ØªØµÙˆÙŠØ±. Ø¨Ø¯Ø£ ØªØµÙˆÙŠØ± Ø§Ù„Ù…Ø³Ù„Ø³Ù„ ÙÙŠ Ø£ÙˆØ§Ø¦Ù„ ÙŠÙˆÙ„ÙŠÙˆ 2021. Ø·Ø¨Ø¹Ø© Ø£Ø®Ø±Ù‰. Ø³ÙŠØªÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†ØªØ§Ø¬ ÙˆÙŠØ¨ ØªÙˆÙ† Ù…Ù† Â«ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨Â»ØŒ Ø§Ù„ÙˆÙŠØ¨ ØªÙˆÙ† Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ø´Ø®ØµÙŠØªÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØªÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ (ØªØ´ÙˆÙŠ ÙˆØ§Ù†Øº ÙˆØ§ÙŠÙˆÙ† Ø³ÙˆÙˆ)ØŒ ÙˆÙ…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ÙŠØªÙ… Ø¥ØµØ¯Ø§Ø±Ù‡ Ø¹Ù„Ù‰ Â«Ù†Ø§ÙŠÙØ± ÙˆÙŠØ¨ØªÙˆÙˆÙ†Â» Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù…. Ù‡ÙŠ Ù†Ø§Ø´Ø± ÙˆÙ…Ù†ØµØ© Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨ Ø£Ø·Ù„Ù‚ØªÙ‡ Ø´Ø±ÙƒØ© Ù†Ø§ÙÙŠØ± ÙÙŠ ÙƒÙˆØ±ÙŠØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ© ÙÙŠ Ø¹Ø§Ù… 2004.\n",
            "---------------------------------------\n",
            "\"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ù‡Ùˆ Ù…ØµØ·Ù„Ø­ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†ÙŠ ÙˆØ§Ù„Ø¹Ù†ÙŠÙ Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø´Ø£Ù†Ù‡ Ø£Ù† ÙŠØ³Ø¨Ø¨ Ù„Ù‡Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¶Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙŠÙŠÙ† Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ø³Ø¯ÙŠ. Ù‚Ø¯ ÙŠØªÙ… Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†Ù‹ ÙƒØ§Ù† ÙˆØ¹Ù„Ù‰ ÙŠØ¯ÙŠ Ø£ÙŠ Ø´Ø®Øµ ÙÙ‡Ùˆ Ù„Ø§ ÙŠÙ†Ø·ÙˆÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø£Ùˆ Ø³Ø¨Ø¨ ÙˆØ§Ø­Ø¯ ÙˆÙ‡Ùˆ ÙŠØ¹ØªØ¨Ø± Ø¸Ø§Ù‡Ø±Ø© Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± ÙˆÙ…Ù† Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªÙÙ‚Ù„ÙÙ‚ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© ÙˆØªØ¯ÙØ¹Ù‡Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø·Ø±Ù‚ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù…ÙØ³Ù†ÙŠÙ† ÙˆØ­Ù…Ø§ÙŠØªÙ‡Ù… Ù…Ù†Ù‡. ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù… Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø© ÙˆÙ‡Ù… Ø£Ø´Ø®Ø§Øµ ÙŠØ­ØµÙ„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù…ØªÙŠØ§Ø²Ø§Øª ÙˆØ­Ù…Ø§ÙŠØ© Ù…ÙƒØ«ÙØ© ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆØ°Ù„Ùƒ Ø¨Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø±Ù‚Ù… 26 Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆÙ‡Ùˆ Ù‚Ø§Ù†ÙˆÙ† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§ØµØ±ÙŠÙ† ÙˆØ§Ù„Ø¹Ø§Ø¬Ø²ÙŠÙ† ÙƒÙ…Ø§ ÙˆÙŠØ¹ÙˆØ¯ Ø°Ù„Ùƒ Ù„Ø¶Ø¹ÙÙ‡Ù… ÙˆÙ„ÙƒØ«Ø±Ø© Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡Ù… ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ§Ø¬Ù‡Ù‡Ø§ Ù‚Ø³Ù… ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ù…. ÙÙŠ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙŠØªÙ… Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" ÙˆØ¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø¥Ø³Ø§Ø¡Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ø­ÙŠØ« Ø£Ù†Ù‡ ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø³Ø§Ø¡Ø© Ù„ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù‚Ø¯ ØªØ¶Ù…Ù† Ø§Ù„Ø¹Ù†Ù Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªØ­Ø¯Ø« Ù…Ù† Ø´Ø®Øµ Ù…Ù‚Ø±Ø¨ Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø³Ù† Ù…Ø«Ù„ Ø§Ù„Ø£Ù‚Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø®ØµØµÙŠÙ† Ù„Ø±Ø¹Ø§ÙŠØªÙ‡Ù… ÙˆØªØ³ØªÙ…Ø± Ù„ÙØªØ±Ø© ÙˆÙ‡Ùˆ Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø§Ø¯Ø« Ø§Ù„Ø°ÙŠ ÙÙŠÙ‡ ÙŠØªÙ… Ø§Ù„ØªÙ‡Ø¬Ù… Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙƒØ¨ÙŠØ± Ø¨Ø§Ù„Ø³Ù† Ù…Ù† Ù‚Ø¨Ù„ Ø´Ø®Øµ ØºØ±ÙŠØ¨ ÙˆÙŠØ­Ø¯Ø« Ø°Ù„Ùƒ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙŠÙƒÙˆÙ† Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ Ø§Ù„Ø´Ø±Ø·Ø©. Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ± Ø£Ù† Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ØªÙ‚ÙˆÙ… Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ ÙƒÙ„ØªØ§ Ø§Ù„Ø­Ø§Ù„ØªÙŠÙ†ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ø¥Ø³Ø§Ø¡Ø© Ø§Ùˆ Ø¹Ù†ÙØŒ Ø¨Ù‡Ø¯Ù Ø§Ù„Ø­Ø¯ Ù…Ù† Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø­Ø³Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø°ÙŠ Ø£ØµØ¯Ø±ØªÙ‡ Ø§Ù„ÙƒÙ†ÙŠØ³Øª Ø¹Ø§Ù… 2007 ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¹Ø±Ø¶ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙØ£Ù†Ù‡ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø£Ø¬Ø±ÙŠØª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø³Ù†Ø© 2006 Ø¹Ø§Ø´ Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ Ø§Ù„ 70ØŒ000 Ù…ÙØ³Ù† ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ø§Ø¯Ù„ 10% Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³ÙƒØ§Ù†ØŒ 30% Ù…Ù†Ù‡Ù… Ù‡Ù… Ù†Ø§Ø¬ÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ø­Ø±Ù‚Ø© Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ© Ø§Ùˆ Ø§Ù„Ù‡ÙˆÙ„ÙˆÙƒÙˆØ³Øª ÙƒÙ…Ø§ Ø§Ù† Ø­ÙˆØ§Ù„ÙŠ 12ØŒ000 Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙŠØ¹ÙŠØ´ÙˆÙ† Ù…Ø¹ Ø´Ø®Øµ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø§ ØªØ±Ø¨Ø·Ù‡ Ù…Ø¹Ù‡Ù… ØµÙ„Ø© Ø¯Ù… (Ø¹Ø§Ø¯Ø© Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ùˆ Ù…Ø±Ø§ÙÙ‚ Ù„Ù„Ø§Ø¹ØªÙ†Ø§Ø¡ Ø¨Ù‡Ù…). Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø´Ø±Ø·Ø© ØªØªÙ„Ù‚Ù‰ Ø¹Ø¯Ø¯ Ù‡Ø§Ø¦Ù„ Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª ÙˆÙØªØ­Øª Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 12,228 Ù…Ù„Ù ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¹Ù†Ù ÙˆØ§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù…Ù† Ø¹Ø§Ù… 2002 Ø­ØªÙ‰ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù…Ù† Ø³Ø¨ØªÙ…Ø¨Ø± Ø¹Ø§Ù… 2007 (Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ØªØªØ¶Ù…Ù† Ø§Ù„Ù‚ØªÙ„ØŒ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù‚ØªÙ„ØŒ Ø§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ÙˆØ§Ù„Ø§ØºØªØµØ§Ø¨ØŒ Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø³Ø±Ù‚Ø© ÙˆØ§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠÙˆØª ÙƒÙ…Ø§ ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ ÙˆØ§Ù„ØªØ®Ø±ÙŠØ¨) Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙŠ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ù„Ø§ ÙŠØªØ¶Ù…Ù† Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙˆØµÙ„ÙˆØ§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø±Ø³ Ø§Ù„Ù…Ø¯Ù†ÙŠ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ù‚Ø³Ø§Ù…. Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ÙØ¥Ù†: 32% Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ù‡ÙŠ Ø¨Ù„Ø§ØºØ§Øª Ø¹Ù† Ø§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø¨ÙŠÙˆØªÙ‡Ù…ØŒ 17% Ù‡ÙŠ Ø¹Ù† ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ØªÙ„Ù‚ÙˆÙ‡Ø§ØŒ 31% Ø¹Ù† Ø³Ø±Ù‚Ø§Øª ØªØ¹Ø±Ø¶ÙˆØ§ Ù„Ù‡Ø§ Ø¨ÙŠÙ†Ù…Ø§ 11% Ù‡ÙŠ ÙÙ‚Ø· Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ø³Ø¯ÙŠØ©. Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ø³Ù†ÙˆØ§Øª (2002-2007) ÙƒØ§Ù† ÙŠÙ‚Ø¯Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø§Ù„Ù‚Ø§Ø¯Ù…ÙˆÙ† Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø¯ÙˆÙ„Ø© (Ø§Ù„Ù…Ù‡Ø§Ø¬Ø±ÙˆÙ†) Ø­ÙˆØ§Ù„ÙŠ 9% Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ø±ØºÙ… Ø£Ù† Ù†Ø³Ø¨ØªÙ‡Ù… Ù…Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ù„Ø§ ØªØªØ¹Ø¯Ù‰ 25% Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø³Ù†ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙƒØ§Ù†ÙˆØ§ ÙŠÙ‚Ø¯Ù…ÙˆÙ† 5% Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª. ÙˆÙÙ‚Ù‹Ø§ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø¥Ù† Ù„Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ Ø£Ø«Ø±Ù‹Ø§ Ø³Ù„Ø¨ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙ‚Ø¯ Ù„ÙˆØ­Ø¸ Ø§Ø±ØªÙØ§Ø¹ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ÙÙŠÙ‡Ø§ ØªØ¹Ù†ÙŠÙ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ØªÙ…Øª Ù…Ù† Ù‚Ø¨Ù„ Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙˆÙ…Ù‚Ø¯Ù…ÙŠ Ø§Ù„Ø±Ø¹Ø§ÙŠØ©) ÙØ¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø¹Ø§Ù… 2018 ØªÙ… Ø¹Ù„Ø§Ø¬ 6707 Ø­Ø§Ù„Ø© Ø¨ÙŠÙ†Ù…Ø§ Ø­ØªÙ‰ Ù…Ù†ØªØµÙ Ø¹Ø§Ù… 2019 ÙÙ‚Ø· ØªÙ… Ø¹Ù„Ø§Ø¬ Ø£ÙƒØ«Ø± Ù…Ù† 7000 Ø­Ø§Ù„Ø© ÙˆÙ‡Ø°Ø§ Ø¹Ø¯Ø§ Ø¹Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ\n"
          ]
        }
      ],
      "source": [
        "print(unique_contexts[0])\n",
        "print(\"---------------------------------------\")\n",
        "print(unique_contexts[1])\n",
        "print(\"---------------------------------------\")\n",
        "print(unique_contexts[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1cb6222-36d2-4047-b5f5-9daa08a12aea",
      "metadata": {
        "id": "c1cb6222-36d2-4047-b5f5-9daa08a12aea"
      },
      "source": [
        "## Build the Vector Index (Embedding + Index + QueryEngine)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1ccd678e-0a50-4da5-8f00-ca085e808f8b",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "1ccd678e-0a50-4da5-8f00-ca085e808f8b",
        "outputId": "a6b43d0f-0c2b-4ba4-aa76-1e9c9dba632d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3\n",
            "Name: llama-index\n",
            "Version: 0.14.9\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: \n",
            "Author-email: Jerry Liu <jerry@llamaindex.ai>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-llms-openai, llama-index-readers-file, llama-index-readers-llama-parse, nltk\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "%pip show llama-index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "87089d87-3767-44ab-969a-3b15a4d7301a",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "87089d87-3767-44ab-969a-3b15a4d7301a",
        "outputId": "3afd3f01-7e57-43fd-ced8-4d44dadb5e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-embeddings-huggingface>=0.1.4\n",
            "  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (0.36.0)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface>=0.1.4) (0.14.9)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface>=0.1.4) (5.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (1.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (3.13.2)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.2.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.27.2)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (2.11.5)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (3.6)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (2.12.5)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.12.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.17.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (4.57.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.1.4) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (3.2.4)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (3.5.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (0.7.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.0.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.1.4) (1.3.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface>=0.1.4) (3.0.3)\n",
            "Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\n",
            "Installing collected packages: llama-index-embeddings-huggingface\n",
            "Successfully installed llama-index-embeddings-huggingface-0.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_index"
                ]
              },
              "id": "dc21ba35748949a4bbd7032b278f7f23"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"llama-index-embeddings-huggingface>=0.1.4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c0e3196d-c152-42ad-affa-04013215c992",
      "metadata": {
        "id": "c0e3196d-c152-42ad-affa-04013215c992",
        "outputId": "c5d09603-8c56-4d69-b54f-9011524ea532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "10563c75a606450a875e740ea35eb318",
            "42b298b735a64f8ca22bfb15044d1efc",
            "d0b79e3642df4640b9fc3306f7a01383",
            "4a1f71372b5643748a55c41abcdf9a13",
            "20558a96219a45b693768956827335c0",
            "e5bbe01aa3ae49c385a8626d48f87c39",
            "87c35e9000354d3b9017a85637971c0e",
            "86590d32648848b3b59f59ffdce8b110",
            "462b60cbb0214116b80d431bd64952f6",
            "00ffe4cf1b96469380c0dddb2b7c59b5",
            "3c93a5f1969947738b17780392f851d7",
            "c9e4b144fbf74e14b33c62fba302ef97",
            "7827acb88c504301987cffc8bab7a371",
            "7afddf68cdce4514b269473f0cf8d350",
            "a78a60405da64fa5947c83a07c3ff98d",
            "e81d486013be436eac613e9058aa2232",
            "f5a9717d523b4809988843cef40163c9",
            "e6aa2770bc7c4f76b8226f42cc2b686c",
            "ea38b592774141deb22a78654cdb0940",
            "1eb8721a704a42cdbcb03977aa08adad",
            "54c02f2a708e43ca90c78cb16d14e432",
            "7ad5a6379f2d42ce8a74a8322e069ab2",
            "d42d0d040688495193b7d4177ae7e5f5",
            "f12a85cc657d450ea612b94dc0d96a83",
            "4992c383e42348b6bd659cf9230224d1",
            "2bde0f3320044da48461406e9a21d29a",
            "b610289fc687407e956d372f52be093d",
            "2cc5a231f0cf4b629bd3ece963ac73b9",
            "5d2d43b6f2224965b2956106933116ef",
            "7d0fc64fa5ed44c394e277feca927b7b",
            "13dd14803828466e94f111d523792829",
            "afa98715420c4da5930e72a4aa884c7f",
            "3d178bd37e6e4adda5a8ecb5c68db2e7",
            "c0b18ea273734bc3b0ca84515851e5ee",
            "fb4b438d28ea4c8499b84c528971d109",
            "5d7e27e98d9d466381399b2493b80c12",
            "dc3bc90ebef545098ac357b56a460a62",
            "95bb729f3b904924992136920ef4e253",
            "7267ec223d3b4b039289b6adcd817a1b",
            "e572086b64624906a7e47c67b2901d81",
            "55eb1a4141f940b49476fa4803c7e21e",
            "7740b715c9aa43b7b6b976b78d75ed24",
            "89e853ae292144aea45056e3fd8bf464",
            "5b1764879a854b1597781ab33dab01f4",
            "93bf2f6dafc64faf98082a8dcc324cb2",
            "9f0b51f388b04b378d886ce9114e5a18",
            "72680f0f8f9b4071901d0021fc3b08d6",
            "0f8f3cd8c80c43ca9300bbcaf3302126",
            "6d2f7d718d9840948bbc896d07159f84",
            "3c59158468b749d8baf3e6d758ed5023",
            "d42a355564e14367a07d5bc66441ff07",
            "abfece9a7cf3400680b693ea329cdc32",
            "422443d3fc2944bc9c50595e4c4f6646",
            "be2f0a6e71ba472399a4d0bc548ba904",
            "ac60a985cd4f4f118b93a2cc01189a46",
            "42723c8b89004de3aa2503a00a757b8c",
            "cab57d0a146245b49ca5cf174e17d307",
            "fc8c7db507c04f809e89e45f8a46b3bf",
            "583913ea82264e2daa8da6a8f0e4af06",
            "2e33ccd50cfc48e98d82d091b4e734ca",
            "71bf7535a9a24070905026e32e56fe90",
            "2eea999b096b49dea1c431ef551b7cc2",
            "50b2c2fa1e154c238e6ed31952bdf996",
            "ce783044e98a4375a9c3e63ecbc3de29",
            "f55eb2ddd4e94b8794e92b8fdca794c8",
            "15bb56cd237648f39d2a21c29a8ed50b",
            "95e7622a4cec461ebc086569e388e41c",
            "b78dcf5d38cf4358b00a38e8cf638548",
            "56d4213688be46fbb3094276e7e9e917",
            "888f82537fb94963935bc7908907e39f",
            "840fa17466364b45894c02b78cb28bb7",
            "f22bcda7374b48008e0c5a9d5a4c7edd",
            "77cde7d04cad4254965c9b6bd3d23afb",
            "a5fc92af6929467c8b2dcd9f6f5e75d3",
            "bb50f6ebaf9f40e88a5257f42bbc6e19",
            "8693452d66034eca807fc2171967212a",
            "675b607c09fc417ba080bd98d5c52b5b",
            "dd273561337a4a099acdd46659a17dc4",
            "f367852a02e44b7baa2b38a203fad414",
            "5e3eae4284884fa4980e37ed1ad30ed5",
            "0517a408ef42459d908a3b4259dd2398",
            "8de8bccd26ad4e9fbab4f465729f7a27",
            "131cc16fba5a42d7b864aefb800d3ae6",
            "ac582cf376de4077970abd0e01955ce1",
            "2305adf72d554299ac652cd497940b27",
            "68d714f3a8f448afb0002b523e0735ea",
            "d7ffaa6e88f74717a06d5c961be2a9d2",
            "ae2110a8c32b45f69cb97493b0faa6f6",
            "bda54c4ec35b480bb3857977205dac0c",
            "f67a3c8c2d3349ebb092e54a42883d1b",
            "a8d20d7b755f49d286139da56acb2b5a",
            "18cb2616ac6f4d5a96d0212f6762b081",
            "5627259f679f48d2a8991b388967624d",
            "4afc13f33cc1471d97762587f1f28a90",
            "b7c5ffc16ca14acb8faee3c265f6b00c",
            "2fda69e439f748c0ac030c8d6eeca29a",
            "7e7b948ddb994afca11efabab6e2e825",
            "5500c6a27aa946dab742bfabbd35a8c9",
            "a0d5b8a940a24135b3a9c5d4586b4a8c",
            "906c41b7e690427b9dece97f22fa70ae",
            "f4ffa1cc686a489c94bf7e8da1b71ba3",
            "3edd8ec71a494d83aeb52dc8c9bc1021",
            "732f5e35f3104e5f8f41cf103f845195",
            "38f703000f154bb0b6f3f06a5fb02801",
            "d21379d3cf4e4aacac8abfe8ada77661",
            "782f5079c2a04b5f871c61bbc1ef3714",
            "a4758750fbc6492c84cdacc21da986d3",
            "ebc7093d9e684c799f0a788fe54972f1",
            "3f601c5306704e5bbb8aebcde2aa1358",
            "d5a749b7576c42d88edade4ac192ab1a",
            "e231e0bd5f1947a7ac9d7664b09fa842",
            "f27cbfbb85034276b6b261eb2ff91809",
            "1535b6e133604ea29a564bb1d99a4299",
            "05fe2bfaf68244a98fa3a75f415457c5",
            "c493783657da43bf8ae5c904fc7668cd",
            "48c41116f8c14ea2bde0497f3113663e",
            "ab976ec6901f4a1989cf0bcb91195e9d",
            "e83441f2556540468ae87ca89f92406b",
            "de507e9fcb7f4bfa98ea1f8525b08c0f",
            "116f21794d4b42e4bccce037d6073b3d",
            "a6a09d0651b24639ac8371bbd028f432"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10563c75a606450a875e740ea35eb318"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9e4b144fbf74e14b33c62fba302ef97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d42d0d040688495193b7d4177ae7e5f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0b18ea273734bc3b0ca84515851e5ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93bf2f6dafc64faf98082a8dcc324cb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42723c8b89004de3aa2503a00a757b8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95e7622a4cec461ebc086569e388e41c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd273561337a4a099acdd46659a17dc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bda54c4ec35b480bb3857977205dac0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "906c41b7e690427b9dece97f22fa70ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e231e0bd5f1947a7ac9d7664b09fa842"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test embedding length: 384\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import torch\n",
        "from llama_index.core import Settings\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "test_vec = embed_model.get_text_embedding(\"hello world\")\n",
        "print(\"Test embedding length:\", len(test_vec))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10a24268-8d8d-44cb-b217-1a446e6955c8",
      "metadata": {
        "id": "10a24268-8d8d-44cb-b217-1a446e6955c8"
      },
      "source": [
        "## Create LlamaIndex Documents from ArabicaQA contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "83376434-f1a4-4bac-8926-ce45c610cfa3",
      "metadata": {
        "id": "83376434-f1a4-4bac-8926-ce45c610cfa3",
        "outputId": "0e7f9720-0ff3-44d1-d5ea-668c9f30d470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "documents = [Document(text=c) for c in unique_contexts]\n",
        "len(documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f2d9cb-9ae2-4d16-91bc-4f0d7cea730a",
      "metadata": {
        "id": "e0f2d9cb-9ae2-4d16-91bc-4f0d7cea730a"
      },
      "source": [
        "## Build the RAG Vector Store using HuggingFace Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cc0c4b83-e6fb-4890-ad75-583c29897de9",
      "metadata": {
        "id": "cc0c4b83-e6fb-4890-ad75-583c29897de9"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "from llama_index.core import Document\n",
        "\n",
        "def build_query_engine(documents):\n",
        "\n",
        "    # reset old config\n",
        "    Settings.llm = llm\n",
        "    Settings.embed_model = embed_model\n",
        "    Settings.callback_manager = None\n",
        "\n",
        "    # create the index\n",
        "    vector_index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "\n",
        "    # query engine\n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
        "\n",
        "    return query_engine\n",
        "\n",
        "query_engine = build_query_engine(documents)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "231aa3d0-4401-4687-a3b0-5391f80a6764",
      "metadata": {
        "id": "231aa3d0-4401-4687-a3b0-5391f80a6764"
      },
      "source": [
        "## Generate model answers for the test questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e4ea4e59-9933-4efd-ad03-fc95fe099dd8",
      "metadata": {
        "scrolled": true,
        "id": "e4ea4e59-9933-4efd-ad03-fc95fe099dd8",
        "outputId": "0fdf5cde-a65f-4b6d-e502-e24029c4c1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "/tmp/ipython-input-1133229017.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_sample[\"model_answer\"] = model_answers\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
              "13845  Ø§Ù„Ø³ÙŠØ®ÙŠØ© (Ø¨Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ÙŠÙŽÙ‘Ø©: à¨¸à¨¿à©±à¨–à©€) Ù‡ÙŠ Ø¯ÙŠØ§Ù†Ø© ØªÙˆØ­ÙŠØ¯ÙŠØ© Ø¯Ø§Ø±Ù…ÙŠÙŽÙ‘Ø© Ù†Ø´Ø£Øª ÙÙŠ Ø´Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù†Ø¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø±. ÙˆØªØ£ØªÙŠ ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®ÙŠØ©Â» Ù…Ù† ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®Â» ÙˆÙ‡ÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ ØªØ£ØªÙŠ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø³Ù†Ø³ÙƒØ±ÙŠØªÙŠ Ø§Ù„ØªÙŠ ØªØ¹Ù†ÙŠ Ø§Ù„ØªÙ„Ù…ÙŠØ° Ùˆ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¯ Ø£Ùˆ Ø§Ù„ØªØ§Ø¨Ø¹. ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø¯ÙŠØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£ÙƒØ¨Ø± Ø§Ù„Ø¯ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…. ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ØªÙ‚Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙˆØ¶ÙØ­ÙŽØª ÙÙŠ ÙƒØªØ§Ø¨Ù‡Ù… Ø§Ù„Ù…Ù‚Ø¯Ø³ Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ØŒ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† ÙˆØ§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ø®Ø§Ù„Ù‚ Ø§Ù„ÙˆØ§Ø­Ø¯ØŒ ÙˆØ§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙˆØ§Ø© Ù„Ù„Ø¨Ø´Ø±ÙŠØ© Ø¬Ù…Ø¹Ø§Ø¡ØŒ ÙˆØ§Ù„Ø§Ù†Ø®Ø±Ø§Ø· ÙÙŠ Ø®Ø¯Ù…Ø© Ù†ÙƒØ±Ø§Ù† Ø§Ù„Ø°Ø§ØªØŒ ÙˆØ§Ù„Ø³Ø¹ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆØ§Ø²Ø¯Ù‡Ø§Ø±Ù‡Ø§ØŒ ÙˆØ¥ØªØ¨Ø§Ø¹ Ø³Ù„ÙˆÙƒ Ù…Ø¹ÙŠØ´Ø© ØµØ§Ø¯Ù‚. ÙˆÙÙŠ Ø£ÙˆØ§Ø¦Ù„ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ÙˆØ§Ø­Ø¯ ÙˆØ§Ù„Ø¹Ø´Ø±ÙŠÙ† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø­ÙˆØ§Ù„ÙŠ 25 Ù…Ù„ÙŠÙˆÙ† Ø³ÙŠØ®ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆØªØ¹ÙŠØ´ Ø§Ù„ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ø£Ùˆ 76% (20 Ù…Ù„ÙŠÙˆÙ†) Ù…Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ØŒ Ù…ÙˆØ·Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø§Ù„Ù‡Ù†Ø¯ØŒ ÙˆÙŠØ¹ÙŠØ´ Ø­ÙˆØ§Ù„ÙŠ Ù…Ù„ÙŠÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø³Ø§Ø¨Ù‚Ø§Ù‹. ÙˆØ³Ø¨Ø¨ Ø§Ù†ØªØ´Ø§Ø±Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù‡Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ² Ø¹Ù„ÙŠÙ‡Ù… ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆØ¨ ÙˆÙ‡Ø¬Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ® Ø®Ø§Ø±Ø¬ Ø¨Ù„Ø§Ø¯Ù‡Ù…ØŒ Ø­ÙŠØ« Ø¨Ø¯Ø£Øª Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø®Ù„Ø§Ù„ Ø§Ù„Ù†ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø±ØŒ Ø¹Ù†Ø¯Ù…Ø§ Ø£ÙƒÙ…Ù„ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠÙˆÙ† Ø¶Ù…Ù‡Ù… Ù„Ù„Ø¨Ù†Ø¬Ø§Ø¨.\\nØªØ³ØªÙ†Ø¯ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù„ÙŠÙ… Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ù„Ù…Ø¤Ø³Ø³ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© ÙˆÙ‡Ùˆ Ø§Ù„ØºÙˆØ±Ùˆ Ù†Ø§Ù†Ø§ÙƒØŒ ÙˆØ®Ù„ÙØ§Ø¦Ù‡ Ø§Ù„ØªØ³Ø¹Ø© Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø±. Ù„Ù‚Ø¨ ØºÙˆØ±Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù…. Ø£Ù…Ø§ Ø§Ù„ØºÙˆØ±Ùˆ ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ù…Ù„Ù‚Ø¨ Ø¨Ø§Ù„Ø¹Ø§Ø´Ø±ØŒ Ø³Ø§Ù‡Ù… ÙÙŠ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø£Ø¬Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆÙƒØ§Ù† Ø¥Ø³Ù‡Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø¥Ø¶ÙØ§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„ØªÙŠ Ø£Ø³Ø³Ù‡Ø§ Ø£ÙˆÙ„Ù‹Ø§ Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù†Ø§Ù†Ø§Ùƒ Ø¯ÙŠÙ Ø¬ÙŠ ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø± Ø¥Ø³Ù‡Ø§Ù…Ù‹Ø§ Ø¬Ø¯ÙŠØ±Ù‹Ø§ Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©. ÙˆØ³Ù…Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ ÙƒØ®Ù„ÙŠÙØ© Ù„Ù‡ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ø£Ù†Ù‡Ù‰ Ø®Ø· Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø± ÙˆØ¬Ø¹Ù„ Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ­ÙŠ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ†ÙˆÙŠ Ù„Ù„Ø³ÙŠØ®. ÙˆØªØ±ÙØ¶ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø¨Ø£Ù† Ø£ÙŠ ØªÙ‚Ù„ÙŠØ¯ Ø¯ÙŠÙ†ÙŠ Ù…Ø¹ÙŠÙ† Ù„Ù‡ Ø§Ø­ØªÙƒØ§Ø± Ù„Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©. ÙˆØªØ·ÙˆØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø¯ÙŠÙ†ÙŠ. Ø­ÙŠØ« ØªØ¹Ø±Ø¶ Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø£ØªØ¨Ø§Ø¹ Ø§Ù„Ø³ÙŠØ® ÙˆÙ‡Ù… Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ± Ù„Ù„ØªØ¹Ø°ÙŠØ¨ ÙˆØ£Ø¹Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø¨Ø¹Ø¯ Ø±ÙØ¶Ù‡Ù… Ø§Ø¹ØªÙ†Ø§Ù‚ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. ÙˆØ£Ø«Ø§Ø± Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø®Ø§Ù„Ø³Ø§ ÙƒØ·Ù„Ø¨ Ù„Ø­Ù…Ø§ÙŠØ© Ø­Ø±ÙŠØ© Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø¯ÙŠÙ†.\\nØ§Ù„ØªØ§Ø±ÙŠØ®.\\nÙŠØ±ØªØ¨Ø· ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙˆØ«ÙŠÙ‚Ù‹Ø§ Ø¨ØªØ§Ø±ÙŠØ® Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±. Ù…Ù†Ø° Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…ØºÙˆÙ„ÙŠ Ù„Ù„Ù‡Ù†Ø¯ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ± Ø¬Ù‡Ø§Ù†ÙƒÙŠØ± (1605-1707)ØŒ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ ØµØ±Ø§Ø¹ Ù…Ø¹ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ù…ØºÙˆÙ„ØŒ Ù„Ø£Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªØ¤Ø«Ø± ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù‚Ø¨ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ù„Ù„Ù…ØºÙˆÙ„ ÙÙŠ Ø­ÙŠÙ† ØªØ¹ØªØ² Ø¨Ø§Ù„Ø£ÙˆÙ„ÙŠØ§Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. Ù‚ÙØªÙ„ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ø§Ø±Ø²ÙŠÙ† Ø¹Ù„Ù‰ ÙŠØ¯ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø§Ù†ØµÙŠØ§Ø¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ®. Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹ 10 Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®ØŒ Ø¹ÙØ°Ø¨ ÙˆØ£ÙØ¹Ø¯Ù… Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø£Ù†ÙØ³Ù‡Ù… (Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ±)ØŒ ÙˆØ£Ù‚Ø±Ø¨Ø§Ø¡ Ù…Ù‚Ø±Ø¨ÙŠÙ† Ù„Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù‚ÙØªÙ„Ùˆ Ø¨ÙˆØ­Ø´ÙŠØ© Ø¯ÙˆÙ† Ø±Ø­Ù…Ø© (Ù…Ø«Ù„ Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ† Ù…Ù† Ø§Ù„Ø¹Ù…Ø± 6 Ùˆ9 Ø³Ù†ÙˆØ§Øª)ØŒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„ØªÙŠ Ø¹ÙØ°Ø¨Øª ÙˆÙ‚ÙØªÙ„Øª (Ù…Ø«Ù„ Ø¨Ø§Ù†Ø¯Ø§ Ø¨Ù‡Ø§Ø¯ÙˆØ±ØŒ Ø¨Ù‡Ø§ÙŠ Ù…Ø§ØªÙŠ Ø¯Ø§Ø³ØŒ Ø¨Ù‡Ø§ÙŠ Ø³Ø§ØªÙŠ Ø¯Ø§Ø³ ÙˆØ¨Ø§Ù‡Ø§ÙŠ Ø¯ÙŠØ§Ù„Ø§)ØŒ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ø¨Ø±ÙŠÙ† Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø®Ø¶ÙˆØ¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ø³ÙƒØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù„Ù…Ø¹Ø§Ø±Ø¶Ø© Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ù…ØºÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ù….\\nØªÙ…ÙŠÙ‘Ø² Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙˆÙ†ÙØ¯Ø±Ø§Ù„ÙŠØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ø£Ù…Ø±Ø§Ø¡ ÙˆØ§Ù„Ø³ÙŠØ® ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ù…Ù‡Ø±Ø§Ø¬Ø§ Ø±Ø§Ù†Ø¬ÙŠØª Ø³ÙŠÙ†Øº Ø¨Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„ØªØ¹Ø§ÙŠØ´ Ø§Ù„Ø³Ù„Ù…ÙŠ ÙˆØ§Ù„ØªØ¹Ø¯Ø¯ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ù„Ø·Ø©. ÙŠØ¹Ø¯ ØªØ£Ø³ÙŠØ³ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¹Ø§Ø¯Ø©Ù‹ Ø°Ø±ÙˆØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØŒ Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙØªØ±Ø© Ø¬Ø§Ø¡Øª Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù„ØªØ´Ù…Ù„ ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§). Ø§Ø¹ØªÙ†Ù‚ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„ÙÙ„Ø§Ø­ÙŠÙ† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ø§Ù„Ø³ÙŠØ®ÙŠØ©. Ø£Ø®Ø° Ù‡Ø§Ø±ÙŠ Ø³ÙŠÙ†Øº Ù†Ø§Ù„ÙˆØ§ØŒ Ø§Ù„Ù‚Ø§Ø¦Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ø¬ÙŠØ´ Ø§Ù„Ø³ÙŠØ® Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ© Ø§Ù„ØºØ±Ø¨ÙŠØ©ØŒ Ø­Ø¯ÙˆØ¯ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥Ù„Ù‰ Ù…ØµØ¨ Ù…Ù…Ø± Ø®ÙŠØ¨Ø± (Ù‡Ùˆ Ù…Ù…Ø± Ø¬Ø¨Ù„ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø¨Ø§ÙƒØ³ØªØ§Ù†ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ø¹ Ø£ÙØºØ§Ù†Ø³ØªØ§Ù†). Ø¯Ù…Ø¬Øª Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ù„Ù…Ø§Ù†ÙŠØ© Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© ÙˆØ§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆØ­ÙƒÙˆÙ…ÙŠØ© Ù…Ø¨ØªÙƒØ±Ø©.\\nØ´Ù‡Ø¯Øª Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„ØªÙŠ Ø³Ø¨Ù‚Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ (ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø¥Ù„Ù‰ Ø¯ÙˆÙ„ØªÙŠÙ† Ù…Ø³ØªÙ‚Ù„ØªÙŠÙ†ØŒ Ø§Ù„Ù‡Ù†Ø¯ ÙˆØ¨Ø§ÙƒØ³ØªØ§Ù†) Ø³Ù†Ø© 1947ØŒ ØµØ±Ø§Ø¹Ù‹Ø§ Ø­Ø§Ø¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ (Ù…Ù†Ø·Ù‚Ø© Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ© ÙˆØ«Ù‚Ø§ÙÙŠØ© ÙˆØªØ§Ø±ÙŠØ®ÙŠØ© ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø¢Ø³ÙŠØ§ØŒ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©ØŒ ÙˆØªØ¶Ù… Ù…Ù†Ø§Ø·Ù‚ ÙÙŠ Ø´Ø±Ù‚ Ø¨Ø§ÙƒØ³ØªØ§Ù† ÙˆØ´Ù…Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯) Ø¨ÙŠÙ† Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†ØŒ Ø´Ù‡Ø¯ Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ù…Ù† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„ØºØ±Ø¨ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ù‡Ø¬Ø±Ø© Ø¯ÙŠÙ†ÙŠØ© Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø´Ø±Ù‚ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨. ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ø¶Ø±ØŒ ÙŠØ¹ÙŠØ´ ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø³ÙŠØ® ÙÙŠ ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙƒØ§Ø±Ø³ Ø£Ùˆ Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³Ø©.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙØ§Øª Ù‡Ù… ÙŠØµØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„ØªØ²Ø§Ù… Ø¨Ù‡Ø§ ÙˆØ¹Ù†Ø¯Ù‡Ù… Ù…Ù† Ù„Ø§ ÙŠÙ„ØªØ²Ù… Ø¨Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³ ÙŠØµÙÙˆÙ‡ Ø¨ØµÙØ© Ø¨Ø§ØªØª patitØ£ÙŠ Ø§Ù„Ù…Ø±ØªØ¯ØŒ ÙˆÙ…Ù† ÙŠØ¯Ø®Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¬Ø¯ÙŠØ¯ ÙŠØ¬Ø¹Ù„ÙˆÙ‡ ÙŠØªØ¹ÙˆØ¯ Ø¹Ù„ÙŠÙ‡Ø§ ÙˆÙŠØ³Ù…ÙˆÙ‡ Ø§Ù„Ù…ØªÙƒÙŠÙ Ø§Ù„Ø¨Ø·Ø¦.\\nØ§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®.\\nØ§Ù„ØºÙˆØ±Ùˆ ÙƒÙ„Ù…Ø© ØªØ¹Ù†ÙŠ Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆÙ‡Ù… Ø¹Ø´Ø±Ø© ØºÙˆØ±Ùˆ Ù„Ù„Ø³ÙŠØ®ÙŠØ© ÙˆÙ‡Ù…:\\n   \n",
              "10056                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ (Ù‡Ø§Ù†ØºÙ„: ê·¸ í•´ ìš°ë¦¬ëŠ”)Ø› Ù‡Ùˆ Ù…Ø³Ù„Ø³Ù„ ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ ÙƒÙˆÙ…ÙŠØ¯ÙŠ ÙƒÙˆØ±ÙŠ Ø¬Ù†ÙˆØ¨ÙŠØŒ ÙŠØµÙ†Ù Ø¨Ø£Ù†Ù‡ Â«Ø£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø£ØµÙ„ÙŠ Ù„ÙŠ Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†Â». Ù…Ù† Ø¨Ø·ÙˆÙ„Ø© ÙˆÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠØŒ Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ SBS TV Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021 Ø¥Ù„Ù‰ 25 ÙŠÙ†Ø§ÙŠØ± 2022ØŒ Ø¨Ø« ÙƒÙ„ ÙŠÙˆÙ… Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† ÙˆØ§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¹Ø© 22:00 Ø¨ØªÙˆÙ‚ÙŠØª (KST). ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­Ù‹Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ù†ØªÙÙ„ÙŠÙƒØ³. Ø§Ù„Ù‚ØµØ©. Ø§Ù†ÙØµÙ„ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº (ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ) ÙˆÙŠÙˆÙ† Ø³Ùˆ (ÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠ) Ù‚Ø¨Ù„ 10 Ø³Ù†ÙˆØ§ØªØŒ Ù„ÙƒÙ† ÙŠØµØ¨Ø­ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø°ÙŠ ØµÙˆØ±ÙˆÙ‡ Ø®Ù„Ø§Ù„ Ø£ÙŠØ§Ù… Ø¯Ø±Ø§Ø³ØªÙ‡Ù…Ø§ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø´Ø§Ø¦Ø¹Ù‹Ø§ØŒ Ø¨Ø³Ø¨Ø¨Ù‡ Ø¹Ù„ÙŠÙ‡Ù…Ø§ Ø§Ù„ÙˆÙ‚ÙˆÙ Ø£Ù…Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¬Ø¯Ø¯Ø§ØŒ Ø¹Ù„Ù‰ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ø§ÙŠØ±ÙŠØ¯ÙˆÙ† Ø°Ù„Ùƒ. Ø·Ø§Ù‚Ù…. Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ø±Ø³Ø§Ù… ØªØ´ÙƒÙŠÙ„ÙŠ Ø­Ø± Ù…ÙØ¹Ù… Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ©. Ø®Ø¨ÙŠØ±Ø© Ø¹Ù„Ø§Ù‚Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©. Ù…Ø®Ø±Ø¬ ÙˆØ«Ø§Ø¦Ù‚ÙŠØŒ ÙˆÙ‡Ùˆ ÙƒØ§Ù† Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ù„ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº ÙˆØ§ÙŠÙˆÙ† Ø³Ùˆ. Ø£ÙŠØ¯ÙˆÙ„ Ù…Ø´Ù‡ÙˆØ±Ø© ØªØ­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ Ù…Ù†Ø° Ø¸Ù‡ÙˆØ±Ù‡Ø§. Ø§Ù„Ø¥Ù†ØªØ§Ø¬. ØµÙ†Ø§Ø¹Ø©. ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†ØŒ ÙˆÙ‡ÙŠ Ø£ÙˆÙ„ Ø¯Ø±Ø§Ù…Ø§ Ø£ØµÙ„ÙŠØ© Ù„Ù‡Ù…ØŒ Ø³ÙŠØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ© Ø³ÙˆØ¨Ø± Ù…ÙˆÙ† Ø¨ÙŠÙƒØªØ´Ø±Ø²ØŒ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ Ø§Ù„ØªØ§Ø¨Ø¹ Ù„Ø¥Ø³ Ø¨ÙŠ Ø¥Ø³ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø¨Ù€ Â«Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ø³Â». Ø·Ø§Ù‚Ù…. ÙÙŠ Ù…Ø§Ø±Ø³ 2021ØŒ Ø£Ø¹Ù„Ù† ØªØ£ÙƒÙŠØ¯ Ø§Ù†Ø¶Ù…Ø§Ù… ÙƒÙ„ Ù…Ù† ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ ÙˆÙƒÙŠÙ… Ø¯Ø§ Ù…ÙŠ ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØŒ ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨Ù‡Ù… Ø¨Ø¹Ø¯ Ø«Ù„Ø§Ø« Ø³Ù†ÙˆØ§ØªØŒ Ø¢Ø®Ø± Ù…Ø±Ø© Ø¸Ù‡Ø±ÙˆØ§ ÙÙŠÙ‡ ÙÙŠ Ù…Ø¹Ø§ Ø¨ÙÙŠÙ„Ù… Ø§Ù„ØºÙ…ÙˆØ¶ \"\" Ù„Ø¹Ø§Ù… 2018. ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø§Ù‚Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠ ÙÙŠ 8 ÙŠÙˆÙ„ÙŠÙˆ. Ø§Ù„ØªØµÙˆÙŠØ±. Ø¨Ø¯Ø£ ØªØµÙˆÙŠØ± Ø§Ù„Ù…Ø³Ù„Ø³Ù„ ÙÙŠ Ø£ÙˆØ§Ø¦Ù„ ÙŠÙˆÙ„ÙŠÙˆ 2021. Ø·Ø¨Ø¹Ø© Ø£Ø®Ø±Ù‰. Ø³ÙŠØªÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†ØªØ§Ø¬ ÙˆÙŠØ¨ ØªÙˆÙ† Ù…Ù† Â«ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨Â»ØŒ Ø§Ù„ÙˆÙŠØ¨ ØªÙˆÙ† Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ø´Ø®ØµÙŠØªÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØªÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ (ØªØ´ÙˆÙŠ ÙˆØ§Ù†Øº ÙˆØ§ÙŠÙˆÙ† Ø³ÙˆÙˆ)ØŒ ÙˆÙ…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ÙŠØªÙ… Ø¥ØµØ¯Ø§Ø±Ù‡ Ø¹Ù„Ù‰ Â«Ù†Ø§ÙŠÙØ± ÙˆÙŠØ¨ØªÙˆÙˆÙ†Â» Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù…. Ù‡ÙŠ Ù†Ø§Ø´Ø± ÙˆÙ…Ù†ØµØ© Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨ Ø£Ø·Ù„Ù‚ØªÙ‡ Ø´Ø±ÙƒØ© Ù†Ø§ÙÙŠØ± ÙÙŠ ÙƒÙˆØ±ÙŠØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ© ÙÙŠ Ø¹Ø§Ù… 2004.   \n",
              "360                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ù‡Ùˆ Ù…ØµØ·Ù„Ø­ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†ÙŠ ÙˆØ§Ù„Ø¹Ù†ÙŠÙ Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø´Ø£Ù†Ù‡ Ø£Ù† ÙŠØ³Ø¨Ø¨ Ù„Ù‡Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¶Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙŠÙŠÙ† Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ø³Ø¯ÙŠ. Ù‚Ø¯ ÙŠØªÙ… Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†Ù‹ ÙƒØ§Ù† ÙˆØ¹Ù„Ù‰ ÙŠØ¯ÙŠ Ø£ÙŠ Ø´Ø®Øµ ÙÙ‡Ùˆ Ù„Ø§ ÙŠÙ†Ø·ÙˆÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø£Ùˆ Ø³Ø¨Ø¨ ÙˆØ§Ø­Ø¯ ÙˆÙ‡Ùˆ ÙŠØ¹ØªØ¨Ø± Ø¸Ø§Ù‡Ø±Ø© Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± ÙˆÙ…Ù† Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªÙÙ‚Ù„ÙÙ‚ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© ÙˆØªØ¯ÙØ¹Ù‡Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø·Ø±Ù‚ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù…ÙØ³Ù†ÙŠÙ† ÙˆØ­Ù…Ø§ÙŠØªÙ‡Ù… Ù…Ù†Ù‡. ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù… Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø© ÙˆÙ‡Ù… Ø£Ø´Ø®Ø§Øµ ÙŠØ­ØµÙ„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù…ØªÙŠØ§Ø²Ø§Øª ÙˆØ­Ù…Ø§ÙŠØ© Ù…ÙƒØ«ÙØ© ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆØ°Ù„Ùƒ Ø¨Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø±Ù‚Ù… 26 Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆÙ‡Ùˆ Ù‚Ø§Ù†ÙˆÙ† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§ØµØ±ÙŠÙ† ÙˆØ§Ù„Ø¹Ø§Ø¬Ø²ÙŠÙ† ÙƒÙ…Ø§ ÙˆÙŠØ¹ÙˆØ¯ Ø°Ù„Ùƒ Ù„Ø¶Ø¹ÙÙ‡Ù… ÙˆÙ„ÙƒØ«Ø±Ø© Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡Ù… ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ§Ø¬Ù‡Ù‡Ø§ Ù‚Ø³Ù… ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ù…. ÙÙŠ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙŠØªÙ… Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" ÙˆØ¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø¥Ø³Ø§Ø¡Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ø­ÙŠØ« Ø£Ù†Ù‡ ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø³Ø§Ø¡Ø© Ù„ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù‚Ø¯ ØªØ¶Ù…Ù† Ø§Ù„Ø¹Ù†Ù Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªØ­Ø¯Ø« Ù…Ù† Ø´Ø®Øµ Ù…Ù‚Ø±Ø¨ Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø³Ù† Ù…Ø«Ù„ Ø§Ù„Ø£Ù‚Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø®ØµØµÙŠÙ† Ù„Ø±Ø¹Ø§ÙŠØªÙ‡Ù… ÙˆØªØ³ØªÙ…Ø± Ù„ÙØªØ±Ø© ÙˆÙ‡Ùˆ Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø§Ø¯Ø« Ø§Ù„Ø°ÙŠ ÙÙŠÙ‡ ÙŠØªÙ… Ø§Ù„ØªÙ‡Ø¬Ù… Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙƒØ¨ÙŠØ± Ø¨Ø§Ù„Ø³Ù† Ù…Ù† Ù‚Ø¨Ù„ Ø´Ø®Øµ ØºØ±ÙŠØ¨ ÙˆÙŠØ­Ø¯Ø« Ø°Ù„Ùƒ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙŠÙƒÙˆÙ† Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ Ø§Ù„Ø´Ø±Ø·Ø©. Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ± Ø£Ù† Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ØªÙ‚ÙˆÙ… Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ ÙƒÙ„ØªØ§ Ø§Ù„Ø­Ø§Ù„ØªÙŠÙ†ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ø¥Ø³Ø§Ø¡Ø© Ø§Ùˆ Ø¹Ù†ÙØŒ Ø¨Ù‡Ø¯Ù Ø§Ù„Ø­Ø¯ Ù…Ù† Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø­Ø³Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø°ÙŠ Ø£ØµØ¯Ø±ØªÙ‡ Ø§Ù„ÙƒÙ†ÙŠØ³Øª Ø¹Ø§Ù… 2007 ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¹Ø±Ø¶ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙØ£Ù†Ù‡ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø£Ø¬Ø±ÙŠØª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø³Ù†Ø© 2006 Ø¹Ø§Ø´ Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ Ø§Ù„ 70ØŒ000 Ù…ÙØ³Ù† ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ø§Ø¯Ù„ 10% Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³ÙƒØ§Ù†ØŒ 30% Ù…Ù†Ù‡Ù… Ù‡Ù… Ù†Ø§Ø¬ÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ø­Ø±Ù‚Ø© Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ© Ø§Ùˆ Ø§Ù„Ù‡ÙˆÙ„ÙˆÙƒÙˆØ³Øª ÙƒÙ…Ø§ Ø§Ù† Ø­ÙˆØ§Ù„ÙŠ 12ØŒ000 Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙŠØ¹ÙŠØ´ÙˆÙ† Ù…Ø¹ Ø´Ø®Øµ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø§ ØªØ±Ø¨Ø·Ù‡ Ù…Ø¹Ù‡Ù… ØµÙ„Ø© Ø¯Ù… (Ø¹Ø§Ø¯Ø© Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ùˆ Ù…Ø±Ø§ÙÙ‚ Ù„Ù„Ø§Ø¹ØªÙ†Ø§Ø¡ Ø¨Ù‡Ù…). Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø´Ø±Ø·Ø© ØªØªÙ„Ù‚Ù‰ Ø¹Ø¯Ø¯ Ù‡Ø§Ø¦Ù„ Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª ÙˆÙØªØ­Øª Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 12,228 Ù…Ù„Ù ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¹Ù†Ù ÙˆØ§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù…Ù† Ø¹Ø§Ù… 2002 Ø­ØªÙ‰ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù…Ù† Ø³Ø¨ØªÙ…Ø¨Ø± Ø¹Ø§Ù… 2007 (Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ØªØªØ¶Ù…Ù† Ø§Ù„Ù‚ØªÙ„ØŒ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù‚ØªÙ„ØŒ Ø§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ÙˆØ§Ù„Ø§ØºØªØµØ§Ø¨ØŒ Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø³Ø±Ù‚Ø© ÙˆØ§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠÙˆØª ÙƒÙ…Ø§ ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ ÙˆØ§Ù„ØªØ®Ø±ÙŠØ¨) Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙŠ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ù„Ø§ ÙŠØªØ¶Ù…Ù† Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙˆØµÙ„ÙˆØ§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø±Ø³ Ø§Ù„Ù…Ø¯Ù†ÙŠ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ù‚Ø³Ø§Ù…. Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ÙØ¥Ù†: 32% Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ù‡ÙŠ Ø¨Ù„Ø§ØºØ§Øª Ø¹Ù† Ø§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø¨ÙŠÙˆØªÙ‡Ù…ØŒ 17% Ù‡ÙŠ Ø¹Ù† ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ØªÙ„Ù‚ÙˆÙ‡Ø§ØŒ 31% Ø¹Ù† Ø³Ø±Ù‚Ø§Øª ØªØ¹Ø±Ø¶ÙˆØ§ Ù„Ù‡Ø§ Ø¨ÙŠÙ†Ù…Ø§ 11% Ù‡ÙŠ ÙÙ‚Ø· Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ø³Ø¯ÙŠØ©. Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ø³Ù†ÙˆØ§Øª (2002-2007) ÙƒØ§Ù† ÙŠÙ‚Ø¯Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø§Ù„Ù‚Ø§Ø¯Ù…ÙˆÙ† Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø¯ÙˆÙ„Ø© (Ø§Ù„Ù…Ù‡Ø§Ø¬Ø±ÙˆÙ†) Ø­ÙˆØ§Ù„ÙŠ 9% Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ø±ØºÙ… Ø£Ù† Ù†Ø³Ø¨ØªÙ‡Ù… Ù…Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ù„Ø§ ØªØªØ¹Ø¯Ù‰ 25% Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø³Ù†ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙƒØ§Ù†ÙˆØ§ ÙŠÙ‚Ø¯Ù…ÙˆÙ† 5% Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª. ÙˆÙÙ‚Ù‹Ø§ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø¥Ù† Ù„Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ Ø£Ø«Ø±Ù‹Ø§ Ø³Ù„Ø¨ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙ‚Ø¯ Ù„ÙˆØ­Ø¸ Ø§Ø±ØªÙØ§Ø¹ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ÙÙŠÙ‡Ø§ ØªØ¹Ù†ÙŠÙ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ØªÙ…Øª Ù…Ù† Ù‚Ø¨Ù„ Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙˆÙ…Ù‚Ø¯Ù…ÙŠ Ø§Ù„Ø±Ø¹Ø§ÙŠØ©) ÙØ¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø¹Ø§Ù… 2018 ØªÙ… Ø¹Ù„Ø§Ø¬ 6707 Ø­Ø§Ù„Ø© Ø¨ÙŠÙ†Ù…Ø§ Ø­ØªÙ‰ Ù…Ù†ØªØµÙ Ø¹Ø§Ù… 2019 ÙÙ‚Ø· ØªÙ… Ø¹Ù„Ø§Ø¬ Ø£ÙƒØ«Ø± Ù…Ù† 7000 Ø­Ø§Ù„Ø© ÙˆÙ‡Ø°Ø§ Ø¹Ø¯Ø§ Ø¹Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ   \n",
              "12766                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯. ÙŠØ¹Ø¯ Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„ØªÙƒÙ„ÙŠÙ Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø·Ø§Ù‚Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù…Ù„ Ø§Ù„ÙÙ‚Ø¯ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ Ù„Ù„ØªØ­Ù…Ù„ ÙˆØ§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø¸Ø±ÙˆÙ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©.   \n",
              "8351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ ØªØ®ØµØµ ÙŠÙ‡ØªÙ… Ø¨Ø¯Ø±Ø§Ø³Ø© ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©.\\nØ£ØµØ¨Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø±ÙˆÙØ§Ù‹ ÙÙŠ Ø£ÙˆØ§Ø®Ø± Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø± ÙˆØ°Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙ„ØºØ±Ø§Ù ÙˆÙ…Ø­Ø·Ø§Øª Ø¥Ù…Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù‚Ø©. ÙˆØ§Ù„Ø¢Ù† ÙŠØºØ·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¹Ø¯Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ÙØ±Ø¹ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ†Ø¸Ù… Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù„Ø§Ø³Ù„ÙƒÙŠØ©.\\nÙˆÙ…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£Ù† Ù†Ù‚ÙˆÙ„ Ø£Ù† Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‚Ø¯ ØªØªØ¶Ù…Ù† Ø£ÙŠØ¶Ø§Ù‹ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ‚Ø¯ Ù„Ø§ ØªØªØ¶Ù…Ù†Ù‡Ø§. ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ†Ù‡Ù…Ø§ Ø­ÙŠØ« ØªÙ‡ØªÙ… Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¨Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬Ù‡Ø¯ Ù…Ø«Ù„ Ù†Ù‚Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ¹Ø§Ù…Ù„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù…Ø¹ Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ù†Ø¸Ù… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (ØªÙŠØ§Ø± Ù…Ù†Ø®ÙØ¶ â€“Ø¬Ù‡Ø¯ Ù…Ù†Ø®ÙØ¶)ØŒ ÙˆÙŠØªØ¶Ù…Ù† Ø°Ù„Ùƒ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„Ø©.\\nÙˆØªØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¯Ø±Ø§Ø³Ø© ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø§ØªØŒ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§ØªØŒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù‚Ø¯Ø±Ø© ØºÙŠØ± Ø§Ù„Ù…Ù†Ù‚Ø·Ø¹Ø© UPSØŒ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆÙ…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©.\\nØªØ§Ø±ÙŠØ® ÙˆØ£Ø¹Ù„Ø§Ù… Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©.\\nØ¸Ù‡Ø± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø¨Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…Ù†Ø° Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø¹Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„. ÙÙŠØ¹ØªÙ‚Ø¯ Ø£Ù† Ø£ÙˆÙ„ Ù…Ù‡Ù†Ø¯Ø³ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù‡Ùˆ ÙˆÙ„ÙŠØ§Ù… Ø¬Ù„Ø¨Ø±Øª Ø§Ù„Ø°ÙŠ ØµÙ…Ù… Ø¢Ù„Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø°Ø§Øª Ø§Ù„Ø´Ø­Ù†Ø§Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©. ÙˆÙ‡Ùˆ Ù…Ù† ÙØ±ÙŽÙ‘Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©ØŒ ÙƒÙ…Ø§ ÙŠØ¹ØªÙ‚Ø¯ Ø¨Ø£Ù†Ù‡ Ø£ÙˆÙ„ Ù…Ù† Ø£Ù†Ø´Ø£ Ù…ØµØ·Ù„Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.\\nÙˆÙÙŠ Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ù…Ø± ÙƒØ§Ù†Øª ÙƒÙ„ Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª ÙˆØ§Ù„Ø§Ø®ØªØ±Ø§Ø¹Ø§Øª ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø´Ø­Ù†Ø©. ÙˆØ¨Ø¯Ø£ ÙØµÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¹Ù† Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ ÙÙŠ Ø²Ù…Ù† ØªÙˆÙ…Ø§Ø³ Ø§Ø¯ÙŠØ³ÙˆÙ† ÙˆÙÙŠØ±Ù†Ø± ÙÙˆÙ† Ø³ÙŠÙ…Ù†Ø³. ÙˆÙÙŠ Ø¹Ø§Ù… 1752 Ø§Ø®ØªØ±Ø¹ Ø¨ÙŠÙ†ÙŠØ§Ù…ÙŠÙ† ÙØ±Ø§Ù†ÙƒÙ„ÙŠÙ† Ù…ÙˆØµÙ„Ø© Ø§Ù„ØµÙˆØ§Ø¹Ù‚ ÙˆÙ†Ø´Ø± Ø¨ÙŠÙ† 1751 Ùˆ1753 Ù†ØªØ§Ø¦Ø¬ ØªØ¬Ø§Ø±Ø¨Ù‡ ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† Â«ØªØ¬Ø§Ø±Ø¨ ÙˆÙ…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡Â» (Experiments and Observations on Electricity). ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1800 Ù‚Ø§Ù… Ø£Ù„Ø³Ø§Ù†Ø¯Ø±Ùˆ ÙÙˆÙ„ØªØ§ Ø¨Ø¨Ù†Ø§Ø¡ Ø¨Ø·Ø§Ø±ÙŠØªÙ‡ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù…Ø³Ù…Ø§Ø© Â«Ø¹Ù…ÙˆØ¯ ÙÙˆÙ„ØªØ§Â» Ø¨Ø¹Ø¯ Ø§Ø¹Ø¬Ø§Ø¨Ù‡ Ø¨ØªØ¬Ø±Ø¨Ø© Ø§Ø¬Ø±Ø§Ù‡Ø§ Ù„ÙˆÙŠØ¬ÙŠ Ø¬Ø§Ù„ÙØ§Ù†ÙŠ Ø¹Ø§Ù… 1792. ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1820 Ù‚Ø§Ù… Ù‡Ø§Ù†Ø² ÙƒØ±ÙŠØ³ØªÙŠØ§Ù† Ø§ÙˆØ±Ø³ØªØ¯ Ø¨Ø¹Ù…Ù„ ØªØ¬Ø§Ø±Ø¨ Ø¹Ù† Ø§Ù†Ø­Ù†Ø§Ø¡ Ø§Ø¨Ø±Ø© Ø§Ù„Ø¨ÙˆØµÙ„Ø© Ø¨ØªØ§Ø«ÙŠØ± Ø§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ. ÙˆÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¹Ø§Ù… ÙƒØ±Ø± Ø§Ù†Ø¯Ø±ÙŠÙ‡ Ù…Ø§Ø±ÙŠ Ø§Ù…Ø¨ÙŠØ± ØªÙ„Ùƒ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ø«Ø¨Øª Ø§Ù† Ø³Ù„ÙƒÙŠÙ† ÙŠÙ…Ø± ÙÙŠÙ‡Ù…Ø§ Ø§Ù„ØªÙŠØ§Ø± ÙŠØ¤Ø«Ø±Ø§Ù† Ø¨Ù‚ÙˆÙ‰ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶ ÙˆØ¹Ø±Ù Ø®Ù„Ø§Ù„Ù‡Ø§ Ø§Ù„Ø¬Ù‡Ø¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ.\\nÙ…Ø§ÙŠÙƒÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ (ÙŠÙ†Ø·Ù‚ Ø£ÙŠØ¶Ø§ Ù…ÙŠØ´ÙŠÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ) Ù‚Ø¯Ù… Ø£Ø¹Ù…Ø§Ù„ ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙÙŠØ¶ÙŠÙ† Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØŒ ÙˆØ¹Ø±Ù Ø£ÙŠØ¶Ø§ Ø®Ø·ÙˆØ· Ø§Ù„Ù…Ø¬Ø§Ù„. ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø§Ù„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ Ù‚Ø¯Ù… Ø¬ÙŠÙ…Ø³ ÙƒÙ„ÙŠØ±Ùƒ Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ Ø£Ø¹Ù…Ø§Ù„Ø§ ÙÙŠ Ø¥ÙƒÙ…Ø§Ù„ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆØ¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©Ù€ ÙˆÙ‚Ø¯Ù… Ø¹Ø§Ù… 1864 Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ ÙˆØ§Ù„ØªÙŠ ØªØ¹ØªØ¨Ø± Ø£Ø­Ø¯ Ø£Ù‡Ù… Ø£Ø³Ø³ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ©.\\nØªØ·ÙˆØ± Ù…Ø¬Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¶ÙˆØ¡ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø£Ø¯Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø°ÙˆÙŠ ØªØ¬Ø±Ø¨Ø©. ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙØªØ±Ø© ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ù‡Ù„Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ®ØµØµØ§Øª Ø¨Ø£Ø·Ø± Ø£Ø¹Ù…Ø§Ù„ Ù‡Ù†Ø¯Ø³ÙŠØ©. Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù‡Ù„Øª Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø¨Ù†Ø§Ø¡ ÙˆÙ…Ù‡Ù†Ø¯Ø³Ùˆ Ù…Ø§ÙƒÙŠÙ†Ø§Øª Ù„ÙƒÙ†Ù‡Ø§ ÙØ´Ù„Øª Ø¨Ø§Ù† ØªØ¤Ù‡Ù„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ù„Ù‰ Ø¶ÙˆØ¡ ØªØ³Ø§Ø±Ø¹ Ø§Ù„Ù…Ø¬Ø§Ù„ØŒ ÙˆÙ„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ù‡Ù†Ø¯Ø³Ùˆ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙŠØ¯Ø±Ø³ÙˆÙ† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù„Ø·Ù„Ø§Ø¨ Ù‡Ù†Ø¯Ø³Ù‡ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.Â \\nØ¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ù…Ù‡Ù…Ø© ØªØ¹Ù„ÙŠÙ… ÙˆØªØ£Ù‡ÙŠÙ„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆÙ‚Ø¹Øª Ø¹Ù„Ù‰ Ø¹Ø§ØªÙ‚ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠÙ†ØŒ Ø­ÙŠØ« Ø§Ù†Ù‡ Ù„Ù… ÙŠÙƒÙ† Ù„Ø§Ø­Ø¯ Ø§Ø®Ø± Ø§Ù„Ø¹Ù„Ù… Ø¨Ù…Ø¬Ø§Ù„ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙˆØ§Ù„Ù‚Ø¯Ø±Ø© Ù„ØªÙˆØµÙŠÙ„Ù‡ Ù„Ø§ÙƒØ¨Ø± Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨.\\n   \n",
              "\n",
              "                                                   question  \\\n",
              "13845         \\nÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙŠ Ø´Ù…Ù„ØªÙ‡Ø§ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ®ØŸ   \n",
              "10056                 Ù…ØªÙ‰ ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\\n   \n",
              "360             Ù…Ù† Ù‡Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ØŸ\\n\\n   \n",
              "12766         Ù…Ø§ Ù‡ÙŠ Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n   \n",
              "8351   Ù…Ø§ Ù‡Ùˆ Ø¯ÙˆØ± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n   \n",
              "\n",
              "                                                                                                                                  ground_truth  \\\n",
              "13845  ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§)   \n",
              "10056                                                                                                                         Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021   \n",
              "360                                                                                                                Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø©   \n",
              "12766                                            Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯   \n",
              "8351                                                                                                                                         .   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                model_answer  \n",
              "13845                                                              Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù‡ÙŠ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1479 Ø¥Ù„Ù‰ 1516. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1516 Ø¥Ù„Ù‰ 1568. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1568 Ø¥Ù„Ù‰ 1700. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1700 Ø¥Ù„Ù‰ 1808. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1808 Ø¥Ù„Ù‰ 1833. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1833 Ø¥Ù„Ù‰ 1868. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1868 Ø¥Ù„Ù‰ 1870. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1870 Ø¥Ù„Ù‰ 1873. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1873 Ø¥Ù„Ù‰ 1874. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1874 Ø¥Ù„Ù‰ 1876. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1876 Ø¥Ù„Ù‰ 1877. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1877 Ø¥Ù„Ù‰ 1878. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1878 Ø¥Ù„Ù‰ 1880. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1880 Ø¥Ù„Ù‰ 1881. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1881 Ø¥Ù„Ù‰ 1882. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1882 Ø¥Ù„Ù‰ 1883. Ùˆ  \n",
              "10056                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\" ÙÙŠ  2023Ù….  \n",
              "360                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù…Ø§ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ØªØªØ±Ø§ÙˆØ­ Ø£Ø¹Ù…Ø§Ø±Ù‡Ù… Ø¨ÙŠÙ† 65 Ùˆ 100 Ø¹Ø§Ù….  \n",
              "12766                                                                                                                                                                                                                                                                                                                        Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø·Ø§Ù‚Ø© ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ø£Ùˆ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©ØŒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©. ÙŠØ¹ØªØ¨Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù†Ø¸Ø§Ù…Ù‹Ø§ Ù…ØªÙ‚Ø¯Ù…Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø§Ù‚Ø©ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ²ÙŠØ¯ Ù…Ù† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©.  \n",
              "8351    Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù‡ÙŠ Ø£Ø¯ÙˆØ§Øª ØªØªØ¨Ø¹ ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØª  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1dd901f9-80ec-49d3-b577-8ae17280a32f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>model_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13845</th>\n",
              "      <td>Ø§Ù„Ø³ÙŠØ®ÙŠØ© (Ø¨Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ÙŠÙŽÙ‘Ø©: à¨¸à¨¿à©±à¨–à©€) Ù‡ÙŠ Ø¯ÙŠØ§Ù†Ø© ØªÙˆØ­ÙŠØ¯ÙŠØ© Ø¯Ø§Ø±Ù…ÙŠÙŽÙ‘Ø© Ù†Ø´Ø£Øª ÙÙŠ Ø´Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù†Ø¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø±. ÙˆØªØ£ØªÙŠ ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®ÙŠØ©Â» Ù…Ù† ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®Â» ÙˆÙ‡ÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ ØªØ£ØªÙŠ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø³Ù†Ø³ÙƒØ±ÙŠØªÙŠ Ø§Ù„ØªÙŠ ØªØ¹Ù†ÙŠ Ø§Ù„ØªÙ„Ù…ÙŠØ° Ùˆ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¯ Ø£Ùˆ Ø§Ù„ØªØ§Ø¨Ø¹. ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø¯ÙŠØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£ÙƒØ¨Ø± Ø§Ù„Ø¯ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…. ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ØªÙ‚Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙˆØ¶ÙØ­ÙŽØª ÙÙŠ ÙƒØªØ§Ø¨Ù‡Ù… Ø§Ù„Ù…Ù‚Ø¯Ø³ Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ØŒ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† ÙˆØ§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ø®Ø§Ù„Ù‚ Ø§Ù„ÙˆØ§Ø­Ø¯ØŒ ÙˆØ§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙˆØ§Ø© Ù„Ù„Ø¨Ø´Ø±ÙŠØ© Ø¬Ù…Ø¹Ø§Ø¡ØŒ ÙˆØ§Ù„Ø§Ù†Ø®Ø±Ø§Ø· ÙÙŠ Ø®Ø¯Ù…Ø© Ù†ÙƒØ±Ø§Ù† Ø§Ù„Ø°Ø§ØªØŒ ÙˆØ§Ù„Ø³Ø¹ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆØ§Ø²Ø¯Ù‡Ø§Ø±Ù‡Ø§ØŒ ÙˆØ¥ØªØ¨Ø§Ø¹ Ø³Ù„ÙˆÙƒ Ù…Ø¹ÙŠØ´Ø© ØµØ§Ø¯Ù‚. ÙˆÙÙŠ Ø£ÙˆØ§Ø¦Ù„ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ÙˆØ§Ø­Ø¯ ÙˆØ§Ù„Ø¹Ø´Ø±ÙŠÙ† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø­ÙˆØ§Ù„ÙŠ 25 Ù…Ù„ÙŠÙˆÙ† Ø³ÙŠØ®ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆØªØ¹ÙŠØ´ Ø§Ù„ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ø£Ùˆ 76% (20 Ù…Ù„ÙŠÙˆÙ†) Ù…Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ØŒ Ù…ÙˆØ·Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø§Ù„Ù‡Ù†Ø¯ØŒ ÙˆÙŠØ¹ÙŠØ´ Ø­ÙˆØ§Ù„ÙŠ Ù…Ù„ÙŠÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø³Ø§Ø¨Ù‚Ø§Ù‹. ÙˆØ³Ø¨Ø¨ Ø§Ù†ØªØ´Ø§Ø±Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù‡Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ² Ø¹Ù„ÙŠÙ‡Ù… ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆØ¨ ÙˆÙ‡Ø¬Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ® Ø®Ø§Ø±Ø¬ Ø¨Ù„Ø§Ø¯Ù‡Ù…ØŒ Ø­ÙŠØ« Ø¨Ø¯Ø£Øª Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø®Ù„Ø§Ù„ Ø§Ù„Ù†ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø±ØŒ Ø¹Ù†Ø¯Ù…Ø§ Ø£ÙƒÙ…Ù„ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠÙˆÙ† Ø¶Ù…Ù‡Ù… Ù„Ù„Ø¨Ù†Ø¬Ø§Ø¨.\\nØªØ³ØªÙ†Ø¯ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù„ÙŠÙ… Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ù„Ù…Ø¤Ø³Ø³ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© ÙˆÙ‡Ùˆ Ø§Ù„ØºÙˆØ±Ùˆ Ù†Ø§Ù†Ø§ÙƒØŒ ÙˆØ®Ù„ÙØ§Ø¦Ù‡ Ø§Ù„ØªØ³Ø¹Ø© Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø±. Ù„Ù‚Ø¨ ØºÙˆØ±Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù…. Ø£Ù…Ø§ Ø§Ù„ØºÙˆØ±Ùˆ ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ù…Ù„Ù‚Ø¨ Ø¨Ø§Ù„Ø¹Ø§Ø´Ø±ØŒ Ø³Ø§Ù‡Ù… ÙÙŠ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø£Ø¬Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆÙƒØ§Ù† Ø¥Ø³Ù‡Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø¥Ø¶ÙØ§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„ØªÙŠ Ø£Ø³Ø³Ù‡Ø§ Ø£ÙˆÙ„Ù‹Ø§ Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù†Ø§Ù†Ø§Ùƒ Ø¯ÙŠÙ Ø¬ÙŠ ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø± Ø¥Ø³Ù‡Ø§Ù…Ù‹Ø§ Ø¬Ø¯ÙŠØ±Ù‹Ø§ Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©. ÙˆØ³Ù…Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ ÙƒØ®Ù„ÙŠÙØ© Ù„Ù‡ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ø£Ù†Ù‡Ù‰ Ø®Ø· Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø± ÙˆØ¬Ø¹Ù„ Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ­ÙŠ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ†ÙˆÙŠ Ù„Ù„Ø³ÙŠØ®. ÙˆØªØ±ÙØ¶ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø¨Ø£Ù† Ø£ÙŠ ØªÙ‚Ù„ÙŠØ¯ Ø¯ÙŠÙ†ÙŠ Ù…Ø¹ÙŠÙ† Ù„Ù‡ Ø§Ø­ØªÙƒØ§Ø± Ù„Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©. ÙˆØªØ·ÙˆØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø¯ÙŠÙ†ÙŠ. Ø­ÙŠØ« ØªØ¹Ø±Ø¶ Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø£ØªØ¨Ø§Ø¹ Ø§Ù„Ø³ÙŠØ® ÙˆÙ‡Ù… Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ± Ù„Ù„ØªØ¹Ø°ÙŠØ¨ ÙˆØ£Ø¹Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø¨Ø¹Ø¯ Ø±ÙØ¶Ù‡Ù… Ø§Ø¹ØªÙ†Ø§Ù‚ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. ÙˆØ£Ø«Ø§Ø± Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø®Ø§Ù„Ø³Ø§ ÙƒØ·Ù„Ø¨ Ù„Ø­Ù…Ø§ÙŠØ© Ø­Ø±ÙŠØ© Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø¯ÙŠÙ†.\\nØ§Ù„ØªØ§Ø±ÙŠØ®.\\nÙŠØ±ØªØ¨Ø· ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙˆØ«ÙŠÙ‚Ù‹Ø§ Ø¨ØªØ§Ø±ÙŠØ® Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±. Ù…Ù†Ø° Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…ØºÙˆÙ„ÙŠ Ù„Ù„Ù‡Ù†Ø¯ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ± Ø¬Ù‡Ø§Ù†ÙƒÙŠØ± (1605-1707)ØŒ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ ØµØ±Ø§Ø¹ Ù…Ø¹ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ù…ØºÙˆÙ„ØŒ Ù„Ø£Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªØ¤Ø«Ø± ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù‚Ø¨ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ù„Ù„Ù…ØºÙˆÙ„ ÙÙŠ Ø­ÙŠÙ† ØªØ¹ØªØ² Ø¨Ø§Ù„Ø£ÙˆÙ„ÙŠØ§Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. Ù‚ÙØªÙ„ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ø§Ø±Ø²ÙŠÙ† Ø¹Ù„Ù‰ ÙŠØ¯ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø§Ù†ØµÙŠØ§Ø¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ®. Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹ 10 Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®ØŒ Ø¹ÙØ°Ø¨ ÙˆØ£ÙØ¹Ø¯Ù… Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø£Ù†ÙØ³Ù‡Ù… (Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ±)ØŒ ÙˆØ£Ù‚Ø±Ø¨Ø§Ø¡ Ù…Ù‚Ø±Ø¨ÙŠÙ† Ù„Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù‚ÙØªÙ„Ùˆ Ø¨ÙˆØ­Ø´ÙŠØ© Ø¯ÙˆÙ† Ø±Ø­Ù…Ø© (Ù…Ø«Ù„ Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ† Ù…Ù† Ø§Ù„Ø¹Ù…Ø± 6 Ùˆ9 Ø³Ù†ÙˆØ§Øª)ØŒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„ØªÙŠ Ø¹ÙØ°Ø¨Øª ÙˆÙ‚ÙØªÙ„Øª (Ù…Ø«Ù„ Ø¨Ø§Ù†Ø¯Ø§ Ø¨Ù‡Ø§Ø¯ÙˆØ±ØŒ Ø¨Ù‡Ø§ÙŠ Ù…Ø§ØªÙŠ Ø¯Ø§Ø³ØŒ Ø¨Ù‡Ø§ÙŠ Ø³Ø§ØªÙŠ Ø¯Ø§Ø³ ÙˆØ¨Ø§Ù‡Ø§ÙŠ Ø¯ÙŠØ§Ù„Ø§)ØŒ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ø¨Ø±ÙŠÙ† Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø®Ø¶ÙˆØ¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ø³ÙƒØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù„Ù…Ø¹Ø§Ø±Ø¶Ø© Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ù…ØºÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ù….\\nØªÙ…ÙŠÙ‘Ø² Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙˆÙ†ÙØ¯Ø±Ø§Ù„ÙŠØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ø£Ù…Ø±Ø§Ø¡ ÙˆØ§Ù„Ø³ÙŠØ® ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ù…Ù‡Ø±Ø§Ø¬Ø§ Ø±Ø§Ù†Ø¬ÙŠØª Ø³ÙŠÙ†Øº Ø¨Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„ØªØ¹Ø§ÙŠØ´ Ø§Ù„Ø³Ù„Ù…ÙŠ ÙˆØ§Ù„ØªØ¹Ø¯Ø¯ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ù„Ø·Ø©. ÙŠØ¹Ø¯ ØªØ£Ø³ÙŠØ³ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¹Ø§Ø¯Ø©Ù‹ Ø°Ø±ÙˆØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØŒ Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙØªØ±Ø© Ø¬Ø§Ø¡Øª Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù„ØªØ´Ù…Ù„ ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§). Ø§Ø¹ØªÙ†Ù‚ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„ÙÙ„Ø§Ø­ÙŠÙ† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ø§Ù„Ø³ÙŠØ®ÙŠØ©. Ø£Ø®Ø° Ù‡Ø§Ø±ÙŠ Ø³ÙŠÙ†Øº Ù†Ø§Ù„ÙˆØ§ØŒ Ø§Ù„Ù‚Ø§Ø¦Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ø¬ÙŠØ´ Ø§Ù„Ø³ÙŠØ® Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ© Ø§Ù„ØºØ±Ø¨ÙŠØ©ØŒ Ø­Ø¯ÙˆØ¯ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥Ù„Ù‰ Ù…ØµØ¨ Ù…Ù…Ø± Ø®ÙŠØ¨Ø± (Ù‡Ùˆ Ù…Ù…Ø± Ø¬Ø¨Ù„ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø¨Ø§ÙƒØ³ØªØ§Ù†ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ø¹ Ø£ÙØºØ§Ù†Ø³ØªØ§Ù†). Ø¯Ù…Ø¬Øª Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ù„Ù…Ø§Ù†ÙŠØ© Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© ÙˆØ§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆØ­ÙƒÙˆÙ…ÙŠØ© Ù…Ø¨ØªÙƒØ±Ø©.\\nØ´Ù‡Ø¯Øª Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„ØªÙŠ Ø³Ø¨Ù‚Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ (ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø¥Ù„Ù‰ Ø¯ÙˆÙ„ØªÙŠÙ† Ù…Ø³ØªÙ‚Ù„ØªÙŠÙ†ØŒ Ø§Ù„Ù‡Ù†Ø¯ ÙˆØ¨Ø§ÙƒØ³ØªØ§Ù†) Ø³Ù†Ø© 1947ØŒ ØµØ±Ø§Ø¹Ù‹Ø§ Ø­Ø§Ø¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ (Ù…Ù†Ø·Ù‚Ø© Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ© ÙˆØ«Ù‚Ø§ÙÙŠØ© ÙˆØªØ§Ø±ÙŠØ®ÙŠØ© ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø¢Ø³ÙŠØ§ØŒ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©ØŒ ÙˆØªØ¶Ù… Ù…Ù†Ø§Ø·Ù‚ ÙÙŠ Ø´Ø±Ù‚ Ø¨Ø§ÙƒØ³ØªØ§Ù† ÙˆØ´Ù…Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯) Ø¨ÙŠÙ† Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†ØŒ Ø´Ù‡Ø¯ Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ù…Ù† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„ØºØ±Ø¨ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ù‡Ø¬Ø±Ø© Ø¯ÙŠÙ†ÙŠØ© Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø´Ø±Ù‚ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨. ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ø¶Ø±ØŒ ÙŠØ¹ÙŠØ´ ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø³ÙŠØ® ÙÙŠ ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙƒØ§Ø±Ø³ Ø£Ùˆ Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³Ø©.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙØ§Øª Ù‡Ù… ÙŠØµØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„ØªØ²Ø§Ù… Ø¨Ù‡Ø§ ÙˆØ¹Ù†Ø¯Ù‡Ù… Ù…Ù† Ù„Ø§ ÙŠÙ„ØªØ²Ù… Ø¨Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³ ÙŠØµÙÙˆÙ‡ Ø¨ØµÙØ© Ø¨Ø§ØªØª patitØ£ÙŠ Ø§Ù„Ù…Ø±ØªØ¯ØŒ ÙˆÙ…Ù† ÙŠØ¯Ø®Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¬Ø¯ÙŠØ¯ ÙŠØ¬Ø¹Ù„ÙˆÙ‡ ÙŠØªØ¹ÙˆØ¯ Ø¹Ù„ÙŠÙ‡Ø§ ÙˆÙŠØ³Ù…ÙˆÙ‡ Ø§Ù„Ù…ØªÙƒÙŠÙ Ø§Ù„Ø¨Ø·Ø¦.\\nØ§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®.\\nØ§Ù„ØºÙˆØ±Ùˆ ÙƒÙ„Ù…Ø© ØªØ¹Ù†ÙŠ Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆÙ‡Ù… Ø¹Ø´Ø±Ø© ØºÙˆØ±Ùˆ Ù„Ù„Ø³ÙŠØ®ÙŠØ© ÙˆÙ‡Ù…:\\n</td>\n",
              "      <td>\\nÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙŠ Ø´Ù…Ù„ØªÙ‡Ø§ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ®ØŸ</td>\n",
              "      <td>ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§)</td>\n",
              "      <td>Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù‡ÙŠ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1479 Ø¥Ù„Ù‰ 1516. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1516 Ø¥Ù„Ù‰ 1568. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1568 Ø¥Ù„Ù‰ 1700. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1700 Ø¥Ù„Ù‰ 1808. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1808 Ø¥Ù„Ù‰ 1833. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1833 Ø¥Ù„Ù‰ 1868. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1868 Ø¥Ù„Ù‰ 1870. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1870 Ø¥Ù„Ù‰ 1873. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1873 Ø¥Ù„Ù‰ 1874. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1874 Ø¥Ù„Ù‰ 1876. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1876 Ø¥Ù„Ù‰ 1877. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1877 Ø¥Ù„Ù‰ 1878. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1878 Ø¥Ù„Ù‰ 1880. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1880 Ø¥Ù„Ù‰ 1881. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1881 Ø¥Ù„Ù‰ 1882. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1882 Ø¥Ù„Ù‰ 1883. Ùˆ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10056</th>\n",
              "      <td>ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ (Ù‡Ø§Ù†ØºÙ„: ê·¸ í•´ ìš°ë¦¬ëŠ”)Ø› Ù‡Ùˆ Ù…Ø³Ù„Ø³Ù„ ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ ÙƒÙˆÙ…ÙŠØ¯ÙŠ ÙƒÙˆØ±ÙŠ Ø¬Ù†ÙˆØ¨ÙŠØŒ ÙŠØµÙ†Ù Ø¨Ø£Ù†Ù‡ Â«Ø£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø£ØµÙ„ÙŠ Ù„ÙŠ Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†Â». Ù…Ù† Ø¨Ø·ÙˆÙ„Ø© ÙˆÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠØŒ Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ SBS TV Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021 Ø¥Ù„Ù‰ 25 ÙŠÙ†Ø§ÙŠØ± 2022ØŒ Ø¨Ø« ÙƒÙ„ ÙŠÙˆÙ… Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† ÙˆØ§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¹Ø© 22:00 Ø¨ØªÙˆÙ‚ÙŠØª (KST). ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­Ù‹Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ù†ØªÙÙ„ÙŠÙƒØ³. Ø§Ù„Ù‚ØµØ©. Ø§Ù†ÙØµÙ„ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº (ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ) ÙˆÙŠÙˆÙ† Ø³Ùˆ (ÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠ) Ù‚Ø¨Ù„ 10 Ø³Ù†ÙˆØ§ØªØŒ Ù„ÙƒÙ† ÙŠØµØ¨Ø­ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø°ÙŠ ØµÙˆØ±ÙˆÙ‡ Ø®Ù„Ø§Ù„ Ø£ÙŠØ§Ù… Ø¯Ø±Ø§Ø³ØªÙ‡Ù…Ø§ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø´Ø§Ø¦Ø¹Ù‹Ø§ØŒ Ø¨Ø³Ø¨Ø¨Ù‡ Ø¹Ù„ÙŠÙ‡Ù…Ø§ Ø§Ù„ÙˆÙ‚ÙˆÙ Ø£Ù…Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¬Ø¯Ø¯Ø§ØŒ Ø¹Ù„Ù‰ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ø§ÙŠØ±ÙŠØ¯ÙˆÙ† Ø°Ù„Ùƒ. Ø·Ø§Ù‚Ù…. Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ø±Ø³Ø§Ù… ØªØ´ÙƒÙŠÙ„ÙŠ Ø­Ø± Ù…ÙØ¹Ù… Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ©. Ø®Ø¨ÙŠØ±Ø© Ø¹Ù„Ø§Ù‚Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©. Ù…Ø®Ø±Ø¬ ÙˆØ«Ø§Ø¦Ù‚ÙŠØŒ ÙˆÙ‡Ùˆ ÙƒØ§Ù† Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ù„ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº ÙˆØ§ÙŠÙˆÙ† Ø³Ùˆ. Ø£ÙŠØ¯ÙˆÙ„ Ù…Ø´Ù‡ÙˆØ±Ø© ØªØ­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ Ù…Ù†Ø° Ø¸Ù‡ÙˆØ±Ù‡Ø§. Ø§Ù„Ø¥Ù†ØªØ§Ø¬. ØµÙ†Ø§Ø¹Ø©. ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†ØŒ ÙˆÙ‡ÙŠ Ø£ÙˆÙ„ Ø¯Ø±Ø§Ù…Ø§ Ø£ØµÙ„ÙŠØ© Ù„Ù‡Ù…ØŒ Ø³ÙŠØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ© Ø³ÙˆØ¨Ø± Ù…ÙˆÙ† Ø¨ÙŠÙƒØªØ´Ø±Ø²ØŒ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ Ø§Ù„ØªØ§Ø¨Ø¹ Ù„Ø¥Ø³ Ø¨ÙŠ Ø¥Ø³ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø¨Ù€ Â«Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ø³Â». Ø·Ø§Ù‚Ù…. ÙÙŠ Ù…Ø§Ø±Ø³ 2021ØŒ Ø£Ø¹Ù„Ù† ØªØ£ÙƒÙŠØ¯ Ø§Ù†Ø¶Ù…Ø§Ù… ÙƒÙ„ Ù…Ù† ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ ÙˆÙƒÙŠÙ… Ø¯Ø§ Ù…ÙŠ ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØŒ ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨Ù‡Ù… Ø¨Ø¹Ø¯ Ø«Ù„Ø§Ø« Ø³Ù†ÙˆØ§ØªØŒ Ø¢Ø®Ø± Ù…Ø±Ø© Ø¸Ù‡Ø±ÙˆØ§ ÙÙŠÙ‡ ÙÙŠ Ù…Ø¹Ø§ Ø¨ÙÙŠÙ„Ù… Ø§Ù„ØºÙ…ÙˆØ¶ \"\" Ù„Ø¹Ø§Ù… 2018. ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø§Ù‚Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠ ÙÙŠ 8 ÙŠÙˆÙ„ÙŠÙˆ. Ø§Ù„ØªØµÙˆÙŠØ±. Ø¨Ø¯Ø£ ØªØµÙˆÙŠØ± Ø§Ù„Ù…Ø³Ù„Ø³Ù„ ÙÙŠ Ø£ÙˆØ§Ø¦Ù„ ÙŠÙˆÙ„ÙŠÙˆ 2021. Ø·Ø¨Ø¹Ø© Ø£Ø®Ø±Ù‰. Ø³ÙŠØªÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†ØªØ§Ø¬ ÙˆÙŠØ¨ ØªÙˆÙ† Ù…Ù† Â«ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨Â»ØŒ Ø§Ù„ÙˆÙŠØ¨ ØªÙˆÙ† Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ø´Ø®ØµÙŠØªÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØªÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ (ØªØ´ÙˆÙŠ ÙˆØ§Ù†Øº ÙˆØ§ÙŠÙˆÙ† Ø³ÙˆÙˆ)ØŒ ÙˆÙ…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ÙŠØªÙ… Ø¥ØµØ¯Ø§Ø±Ù‡ Ø¹Ù„Ù‰ Â«Ù†Ø§ÙŠÙØ± ÙˆÙŠØ¨ØªÙˆÙˆÙ†Â» Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù…. Ù‡ÙŠ Ù†Ø§Ø´Ø± ÙˆÙ…Ù†ØµØ© Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨ Ø£Ø·Ù„Ù‚ØªÙ‡ Ø´Ø±ÙƒØ© Ù†Ø§ÙÙŠØ± ÙÙŠ ÙƒÙˆØ±ÙŠØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ© ÙÙŠ Ø¹Ø§Ù… 2004.</td>\n",
              "      <td>Ù…ØªÙ‰ ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\\n</td>\n",
              "      <td>Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021</td>\n",
              "      <td>ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\" ÙÙŠ  2023Ù….</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>\"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ù‡Ùˆ Ù…ØµØ·Ù„Ø­ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†ÙŠ ÙˆØ§Ù„Ø¹Ù†ÙŠÙ Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø´Ø£Ù†Ù‡ Ø£Ù† ÙŠØ³Ø¨Ø¨ Ù„Ù‡Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¶Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙŠÙŠÙ† Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ø³Ø¯ÙŠ. Ù‚Ø¯ ÙŠØªÙ… Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†Ù‹ ÙƒØ§Ù† ÙˆØ¹Ù„Ù‰ ÙŠØ¯ÙŠ Ø£ÙŠ Ø´Ø®Øµ ÙÙ‡Ùˆ Ù„Ø§ ÙŠÙ†Ø·ÙˆÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø£Ùˆ Ø³Ø¨Ø¨ ÙˆØ§Ø­Ø¯ ÙˆÙ‡Ùˆ ÙŠØ¹ØªØ¨Ø± Ø¸Ø§Ù‡Ø±Ø© Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± ÙˆÙ…Ù† Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªÙÙ‚Ù„ÙÙ‚ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© ÙˆØªØ¯ÙØ¹Ù‡Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø·Ø±Ù‚ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù…ÙØ³Ù†ÙŠÙ† ÙˆØ­Ù…Ø§ÙŠØªÙ‡Ù… Ù…Ù†Ù‡. ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù… Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø© ÙˆÙ‡Ù… Ø£Ø´Ø®Ø§Øµ ÙŠØ­ØµÙ„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù…ØªÙŠØ§Ø²Ø§Øª ÙˆØ­Ù…Ø§ÙŠØ© Ù…ÙƒØ«ÙØ© ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆØ°Ù„Ùƒ Ø¨Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø±Ù‚Ù… 26 Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆÙ‡Ùˆ Ù‚Ø§Ù†ÙˆÙ† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§ØµØ±ÙŠÙ† ÙˆØ§Ù„Ø¹Ø§Ø¬Ø²ÙŠÙ† ÙƒÙ…Ø§ ÙˆÙŠØ¹ÙˆØ¯ Ø°Ù„Ùƒ Ù„Ø¶Ø¹ÙÙ‡Ù… ÙˆÙ„ÙƒØ«Ø±Ø© Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡Ù… ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ§Ø¬Ù‡Ù‡Ø§ Ù‚Ø³Ù… ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ù…. ÙÙŠ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙŠØªÙ… Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" ÙˆØ¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø¥Ø³Ø§Ø¡Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ø­ÙŠØ« Ø£Ù†Ù‡ ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø³Ø§Ø¡Ø© Ù„ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù‚Ø¯ ØªØ¶Ù…Ù† Ø§Ù„Ø¹Ù†Ù Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªØ­Ø¯Ø« Ù…Ù† Ø´Ø®Øµ Ù…Ù‚Ø±Ø¨ Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø³Ù† Ù…Ø«Ù„ Ø§Ù„Ø£Ù‚Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø®ØµØµÙŠÙ† Ù„Ø±Ø¹Ø§ÙŠØªÙ‡Ù… ÙˆØªØ³ØªÙ…Ø± Ù„ÙØªØ±Ø© ÙˆÙ‡Ùˆ Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø§Ø¯Ø« Ø§Ù„Ø°ÙŠ ÙÙŠÙ‡ ÙŠØªÙ… Ø§Ù„ØªÙ‡Ø¬Ù… Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙƒØ¨ÙŠØ± Ø¨Ø§Ù„Ø³Ù† Ù…Ù† Ù‚Ø¨Ù„ Ø´Ø®Øµ ØºØ±ÙŠØ¨ ÙˆÙŠØ­Ø¯Ø« Ø°Ù„Ùƒ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙŠÙƒÙˆÙ† Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ Ø§Ù„Ø´Ø±Ø·Ø©. Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ± Ø£Ù† Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ØªÙ‚ÙˆÙ… Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ ÙƒÙ„ØªØ§ Ø§Ù„Ø­Ø§Ù„ØªÙŠÙ†ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ø¥Ø³Ø§Ø¡Ø© Ø§Ùˆ Ø¹Ù†ÙØŒ Ø¨Ù‡Ø¯Ù Ø§Ù„Ø­Ø¯ Ù…Ù† Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø­Ø³Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø°ÙŠ Ø£ØµØ¯Ø±ØªÙ‡ Ø§Ù„ÙƒÙ†ÙŠØ³Øª Ø¹Ø§Ù… 2007 ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¹Ø±Ø¶ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙØ£Ù†Ù‡ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø£Ø¬Ø±ÙŠØª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø³Ù†Ø© 2006 Ø¹Ø§Ø´ Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ Ø§Ù„ 70ØŒ000 Ù…ÙØ³Ù† ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ø§Ø¯Ù„ 10% Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³ÙƒØ§Ù†ØŒ 30% Ù…Ù†Ù‡Ù… Ù‡Ù… Ù†Ø§Ø¬ÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ø­Ø±Ù‚Ø© Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ© Ø§Ùˆ Ø§Ù„Ù‡ÙˆÙ„ÙˆÙƒÙˆØ³Øª ÙƒÙ…Ø§ Ø§Ù† Ø­ÙˆØ§Ù„ÙŠ 12ØŒ000 Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙŠØ¹ÙŠØ´ÙˆÙ† Ù…Ø¹ Ø´Ø®Øµ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø§ ØªØ±Ø¨Ø·Ù‡ Ù…Ø¹Ù‡Ù… ØµÙ„Ø© Ø¯Ù… (Ø¹Ø§Ø¯Ø© Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ùˆ Ù…Ø±Ø§ÙÙ‚ Ù„Ù„Ø§Ø¹ØªÙ†Ø§Ø¡ Ø¨Ù‡Ù…). Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø´Ø±Ø·Ø© ØªØªÙ„Ù‚Ù‰ Ø¹Ø¯Ø¯ Ù‡Ø§Ø¦Ù„ Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª ÙˆÙØªØ­Øª Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 12,228 Ù…Ù„Ù ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¹Ù†Ù ÙˆØ§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù…Ù† Ø¹Ø§Ù… 2002 Ø­ØªÙ‰ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù…Ù† Ø³Ø¨ØªÙ…Ø¨Ø± Ø¹Ø§Ù… 2007 (Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ØªØªØ¶Ù…Ù† Ø§Ù„Ù‚ØªÙ„ØŒ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù‚ØªÙ„ØŒ Ø§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ÙˆØ§Ù„Ø§ØºØªØµØ§Ø¨ØŒ Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø³Ø±Ù‚Ø© ÙˆØ§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠÙˆØª ÙƒÙ…Ø§ ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ ÙˆØ§Ù„ØªØ®Ø±ÙŠØ¨) Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙŠ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ù„Ø§ ÙŠØªØ¶Ù…Ù† Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙˆØµÙ„ÙˆØ§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø±Ø³ Ø§Ù„Ù…Ø¯Ù†ÙŠ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ù‚Ø³Ø§Ù…. Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ÙØ¥Ù†: 32% Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ù‡ÙŠ Ø¨Ù„Ø§ØºØ§Øª Ø¹Ù† Ø§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø¨ÙŠÙˆØªÙ‡Ù…ØŒ 17% Ù‡ÙŠ Ø¹Ù† ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ØªÙ„Ù‚ÙˆÙ‡Ø§ØŒ 31% Ø¹Ù† Ø³Ø±Ù‚Ø§Øª ØªØ¹Ø±Ø¶ÙˆØ§ Ù„Ù‡Ø§ Ø¨ÙŠÙ†Ù…Ø§ 11% Ù‡ÙŠ ÙÙ‚Ø· Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ø³Ø¯ÙŠØ©. Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ø³Ù†ÙˆØ§Øª (2002-2007) ÙƒØ§Ù† ÙŠÙ‚Ø¯Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø§Ù„Ù‚Ø§Ø¯Ù…ÙˆÙ† Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø¯ÙˆÙ„Ø© (Ø§Ù„Ù…Ù‡Ø§Ø¬Ø±ÙˆÙ†) Ø­ÙˆØ§Ù„ÙŠ 9% Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ø±ØºÙ… Ø£Ù† Ù†Ø³Ø¨ØªÙ‡Ù… Ù…Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ù„Ø§ ØªØªØ¹Ø¯Ù‰ 25% Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø³Ù†ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙƒØ§Ù†ÙˆØ§ ÙŠÙ‚Ø¯Ù…ÙˆÙ† 5% Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª. ÙˆÙÙ‚Ù‹Ø§ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø¥Ù† Ù„Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ Ø£Ø«Ø±Ù‹Ø§ Ø³Ù„Ø¨ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙ‚Ø¯ Ù„ÙˆØ­Ø¸ Ø§Ø±ØªÙØ§Ø¹ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ÙÙŠÙ‡Ø§ ØªØ¹Ù†ÙŠÙ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ØªÙ…Øª Ù…Ù† Ù‚Ø¨Ù„ Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙˆÙ…Ù‚Ø¯Ù…ÙŠ Ø§Ù„Ø±Ø¹Ø§ÙŠØ©) ÙØ¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø¹Ø§Ù… 2018 ØªÙ… Ø¹Ù„Ø§Ø¬ 6707 Ø­Ø§Ù„Ø© Ø¨ÙŠÙ†Ù…Ø§ Ø­ØªÙ‰ Ù…Ù†ØªØµÙ Ø¹Ø§Ù… 2019 ÙÙ‚Ø· ØªÙ… Ø¹Ù„Ø§Ø¬ Ø£ÙƒØ«Ø± Ù…Ù† 7000 Ø­Ø§Ù„Ø© ÙˆÙ‡Ø°Ø§ Ø¹Ø¯Ø§ Ø¹Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ</td>\n",
              "      <td>Ù…Ù† Ù‡Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ØŸ\\n\\n</td>\n",
              "      <td>Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø©</td>\n",
              "      <td>ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù…Ø§ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ØªØªØ±Ø§ÙˆØ­ Ø£Ø¹Ù…Ø§Ø±Ù‡Ù… Ø¨ÙŠÙ† 65 Ùˆ 100 Ø¹Ø§Ù….</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12766</th>\n",
              "      <td>Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯. ÙŠØ¹Ø¯ Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„ØªÙƒÙ„ÙŠÙ Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø·Ø§Ù‚Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù…Ù„ Ø§Ù„ÙÙ‚Ø¯ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ Ù„Ù„ØªØ­Ù…Ù„ ÙˆØ§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø¸Ø±ÙˆÙ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©.</td>\n",
              "      <td>Ù…Ø§ Ù‡ÙŠ Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n</td>\n",
              "      <td>Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯</td>\n",
              "      <td>Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø·Ø§Ù‚Ø© ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ø£Ùˆ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©ØŒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©. ÙŠØ¹ØªØ¨Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù†Ø¸Ø§Ù…Ù‹Ø§ Ù…ØªÙ‚Ø¯Ù…Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø§Ù‚Ø©ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ²ÙŠØ¯ Ù…Ù† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8351</th>\n",
              "      <td>Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ ØªØ®ØµØµ ÙŠÙ‡ØªÙ… Ø¨Ø¯Ø±Ø§Ø³Ø© ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©.\\nØ£ØµØ¨Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø±ÙˆÙØ§Ù‹ ÙÙŠ Ø£ÙˆØ§Ø®Ø± Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø± ÙˆØ°Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙ„ØºØ±Ø§Ù ÙˆÙ…Ø­Ø·Ø§Øª Ø¥Ù…Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù‚Ø©. ÙˆØ§Ù„Ø¢Ù† ÙŠØºØ·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¹Ø¯Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ÙØ±Ø¹ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ†Ø¸Ù… Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù„Ø§Ø³Ù„ÙƒÙŠØ©.\\nÙˆÙ…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£Ù† Ù†Ù‚ÙˆÙ„ Ø£Ù† Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‚Ø¯ ØªØªØ¶Ù…Ù† Ø£ÙŠØ¶Ø§Ù‹ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ‚Ø¯ Ù„Ø§ ØªØªØ¶Ù…Ù†Ù‡Ø§. ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ†Ù‡Ù…Ø§ Ø­ÙŠØ« ØªÙ‡ØªÙ… Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¨Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬Ù‡Ø¯ Ù…Ø«Ù„ Ù†Ù‚Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ¹Ø§Ù…Ù„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù…Ø¹ Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ù†Ø¸Ù… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (ØªÙŠØ§Ø± Ù…Ù†Ø®ÙØ¶ â€“Ø¬Ù‡Ø¯ Ù…Ù†Ø®ÙØ¶)ØŒ ÙˆÙŠØªØ¶Ù…Ù† Ø°Ù„Ùƒ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„Ø©.\\nÙˆØªØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¯Ø±Ø§Ø³Ø© ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø§ØªØŒ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§ØªØŒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù‚Ø¯Ø±Ø© ØºÙŠØ± Ø§Ù„Ù…Ù†Ù‚Ø·Ø¹Ø© UPSØŒ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆÙ…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©.\\nØªØ§Ø±ÙŠØ® ÙˆØ£Ø¹Ù„Ø§Ù… Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©.\\nØ¸Ù‡Ø± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø¨Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…Ù†Ø° Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø¹Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„. ÙÙŠØ¹ØªÙ‚Ø¯ Ø£Ù† Ø£ÙˆÙ„ Ù…Ù‡Ù†Ø¯Ø³ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù‡Ùˆ ÙˆÙ„ÙŠØ§Ù… Ø¬Ù„Ø¨Ø±Øª Ø§Ù„Ø°ÙŠ ØµÙ…Ù… Ø¢Ù„Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø°Ø§Øª Ø§Ù„Ø´Ø­Ù†Ø§Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©. ÙˆÙ‡Ùˆ Ù…Ù† ÙØ±ÙŽÙ‘Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©ØŒ ÙƒÙ…Ø§ ÙŠØ¹ØªÙ‚Ø¯ Ø¨Ø£Ù†Ù‡ Ø£ÙˆÙ„ Ù…Ù† Ø£Ù†Ø´Ø£ Ù…ØµØ·Ù„Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.\\nÙˆÙÙŠ Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ù…Ø± ÙƒØ§Ù†Øª ÙƒÙ„ Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª ÙˆØ§Ù„Ø§Ø®ØªØ±Ø§Ø¹Ø§Øª ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø´Ø­Ù†Ø©. ÙˆØ¨Ø¯Ø£ ÙØµÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¹Ù† Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ ÙÙŠ Ø²Ù…Ù† ØªÙˆÙ…Ø§Ø³ Ø§Ø¯ÙŠØ³ÙˆÙ† ÙˆÙÙŠØ±Ù†Ø± ÙÙˆÙ† Ø³ÙŠÙ…Ù†Ø³. ÙˆÙÙŠ Ø¹Ø§Ù… 1752 Ø§Ø®ØªØ±Ø¹ Ø¨ÙŠÙ†ÙŠØ§Ù…ÙŠÙ† ÙØ±Ø§Ù†ÙƒÙ„ÙŠÙ† Ù…ÙˆØµÙ„Ø© Ø§Ù„ØµÙˆØ§Ø¹Ù‚ ÙˆÙ†Ø´Ø± Ø¨ÙŠÙ† 1751 Ùˆ1753 Ù†ØªØ§Ø¦Ø¬ ØªØ¬Ø§Ø±Ø¨Ù‡ ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† Â«ØªØ¬Ø§Ø±Ø¨ ÙˆÙ…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡Â» (Experiments and Observations on Electricity). ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1800 Ù‚Ø§Ù… Ø£Ù„Ø³Ø§Ù†Ø¯Ø±Ùˆ ÙÙˆÙ„ØªØ§ Ø¨Ø¨Ù†Ø§Ø¡ Ø¨Ø·Ø§Ø±ÙŠØªÙ‡ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù…Ø³Ù…Ø§Ø© Â«Ø¹Ù…ÙˆØ¯ ÙÙˆÙ„ØªØ§Â» Ø¨Ø¹Ø¯ Ø§Ø¹Ø¬Ø§Ø¨Ù‡ Ø¨ØªØ¬Ø±Ø¨Ø© Ø§Ø¬Ø±Ø§Ù‡Ø§ Ù„ÙˆÙŠØ¬ÙŠ Ø¬Ø§Ù„ÙØ§Ù†ÙŠ Ø¹Ø§Ù… 1792. ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1820 Ù‚Ø§Ù… Ù‡Ø§Ù†Ø² ÙƒØ±ÙŠØ³ØªÙŠØ§Ù† Ø§ÙˆØ±Ø³ØªØ¯ Ø¨Ø¹Ù…Ù„ ØªØ¬Ø§Ø±Ø¨ Ø¹Ù† Ø§Ù†Ø­Ù†Ø§Ø¡ Ø§Ø¨Ø±Ø© Ø§Ù„Ø¨ÙˆØµÙ„Ø© Ø¨ØªØ§Ø«ÙŠØ± Ø§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ. ÙˆÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¹Ø§Ù… ÙƒØ±Ø± Ø§Ù†Ø¯Ø±ÙŠÙ‡ Ù…Ø§Ø±ÙŠ Ø§Ù…Ø¨ÙŠØ± ØªÙ„Ùƒ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ø«Ø¨Øª Ø§Ù† Ø³Ù„ÙƒÙŠÙ† ÙŠÙ…Ø± ÙÙŠÙ‡Ù…Ø§ Ø§Ù„ØªÙŠØ§Ø± ÙŠØ¤Ø«Ø±Ø§Ù† Ø¨Ù‚ÙˆÙ‰ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶ ÙˆØ¹Ø±Ù Ø®Ù„Ø§Ù„Ù‡Ø§ Ø§Ù„Ø¬Ù‡Ø¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ.\\nÙ…Ø§ÙŠÙƒÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ (ÙŠÙ†Ø·Ù‚ Ø£ÙŠØ¶Ø§ Ù…ÙŠØ´ÙŠÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ) Ù‚Ø¯Ù… Ø£Ø¹Ù…Ø§Ù„ ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙÙŠØ¶ÙŠÙ† Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØŒ ÙˆØ¹Ø±Ù Ø£ÙŠØ¶Ø§ Ø®Ø·ÙˆØ· Ø§Ù„Ù…Ø¬Ø§Ù„. ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø§Ù„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ Ù‚Ø¯Ù… Ø¬ÙŠÙ…Ø³ ÙƒÙ„ÙŠØ±Ùƒ Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ Ø£Ø¹Ù…Ø§Ù„Ø§ ÙÙŠ Ø¥ÙƒÙ…Ø§Ù„ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆØ¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©Ù€ ÙˆÙ‚Ø¯Ù… Ø¹Ø§Ù… 1864 Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ ÙˆØ§Ù„ØªÙŠ ØªØ¹ØªØ¨Ø± Ø£Ø­Ø¯ Ø£Ù‡Ù… Ø£Ø³Ø³ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ©.\\nØªØ·ÙˆØ± Ù…Ø¬Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¶ÙˆØ¡ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø£Ø¯Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø°ÙˆÙŠ ØªØ¬Ø±Ø¨Ø©. ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙØªØ±Ø© ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ù‡Ù„Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ®ØµØµØ§Øª Ø¨Ø£Ø·Ø± Ø£Ø¹Ù…Ø§Ù„ Ù‡Ù†Ø¯Ø³ÙŠØ©. Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù‡Ù„Øª Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø¨Ù†Ø§Ø¡ ÙˆÙ…Ù‡Ù†Ø¯Ø³Ùˆ Ù…Ø§ÙƒÙŠÙ†Ø§Øª Ù„ÙƒÙ†Ù‡Ø§ ÙØ´Ù„Øª Ø¨Ø§Ù† ØªØ¤Ù‡Ù„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ù„Ù‰ Ø¶ÙˆØ¡ ØªØ³Ø§Ø±Ø¹ Ø§Ù„Ù…Ø¬Ø§Ù„ØŒ ÙˆÙ„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ù‡Ù†Ø¯Ø³Ùˆ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙŠØ¯Ø±Ø³ÙˆÙ† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù„Ø·Ù„Ø§Ø¨ Ù‡Ù†Ø¯Ø³Ù‡ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.Â \\nØ¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ù…Ù‡Ù…Ø© ØªØ¹Ù„ÙŠÙ… ÙˆØªØ£Ù‡ÙŠÙ„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆÙ‚Ø¹Øª Ø¹Ù„Ù‰ Ø¹Ø§ØªÙ‚ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠÙ†ØŒ Ø­ÙŠØ« Ø§Ù†Ù‡ Ù„Ù… ÙŠÙƒÙ† Ù„Ø§Ø­Ø¯ Ø§Ø®Ø± Ø§Ù„Ø¹Ù„Ù… Ø¨Ù…Ø¬Ø§Ù„ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙˆØ§Ù„Ù‚Ø¯Ø±Ø© Ù„ØªÙˆØµÙŠÙ„Ù‡ Ù„Ø§ÙƒØ¨Ø± Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨.\\n</td>\n",
              "      <td>Ù…Ø§ Ù‡Ùˆ Ø¯ÙˆØ± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n</td>\n",
              "      <td>.</td>\n",
              "      <td>Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù‡ÙŠ Ø£Ø¯ÙˆØ§Øª ØªØªØ¨Ø¹ ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØª</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1dd901f9-80ec-49d3-b577-8ae17280a32f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1dd901f9-80ec-49d3-b577-8ae17280a32f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1dd901f9-80ec-49d3-b577-8ae17280a32f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model_answers = []\n",
        "\n",
        "for q in df_sample[\"question\"]:\n",
        "    try:\n",
        "        response = query_engine.query(q)\n",
        "        model_answers.append(response.response)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        model_answers.append(\"\")\n",
        "\n",
        "# Add model answers to dataframe\n",
        "df_sample[\"model_answer\"] = model_answers\n",
        "\n",
        "df_sample.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "258a8a22-62cb-4ace-9b1a-b118dcd2a78d",
      "metadata": {
        "scrolled": true,
        "id": "258a8a22-62cb-4ace-9b1a-b118dcd2a78d",
        "outputId": "ea13ff0d-d9a1-4410-ce6b-848c6367ea87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               context  \\\n",
              "0  Ø§Ù„Ø³ÙŠØ®ÙŠØ© (Ø¨Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ÙŠÙŽÙ‘Ø©: à¨¸à¨¿à©±à¨–à©€) Ù‡ÙŠ Ø¯ÙŠØ§Ù†Ø© ØªÙˆØ­ÙŠØ¯ÙŠØ© Ø¯Ø§Ø±Ù…ÙŠÙŽÙ‘Ø© Ù†Ø´Ø£Øª ÙÙŠ Ø´Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù†Ø¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø±. ÙˆØªØ£ØªÙŠ ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®ÙŠØ©Â» Ù…Ù† ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®Â» ÙˆÙ‡ÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ ØªØ£ØªÙŠ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø³Ù†Ø³ÙƒØ±ÙŠØªÙŠ Ø§Ù„ØªÙŠ ØªØ¹Ù†ÙŠ Ø§Ù„ØªÙ„Ù…ÙŠØ° Ùˆ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¯ Ø£Ùˆ Ø§Ù„ØªØ§Ø¨Ø¹. ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø¯ÙŠØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£ÙƒØ¨Ø± Ø§Ù„Ø¯ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…. ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ØªÙ‚Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙˆØ¶ÙØ­ÙŽØª ÙÙŠ ÙƒØªØ§Ø¨Ù‡Ù… Ø§Ù„Ù…Ù‚Ø¯Ø³ Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ØŒ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† ÙˆØ§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ø®Ø§Ù„Ù‚ Ø§Ù„ÙˆØ§Ø­Ø¯ØŒ ÙˆØ§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙˆØ§Ø© Ù„Ù„Ø¨Ø´Ø±ÙŠØ© Ø¬Ù…Ø¹Ø§Ø¡ØŒ ÙˆØ§Ù„Ø§Ù†Ø®Ø±Ø§Ø· ÙÙŠ Ø®Ø¯Ù…Ø© Ù†ÙƒØ±Ø§Ù† Ø§Ù„Ø°Ø§ØªØŒ ÙˆØ§Ù„Ø³Ø¹ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆØ§Ø²Ø¯Ù‡Ø§Ø±Ù‡Ø§ØŒ ÙˆØ¥ØªØ¨Ø§Ø¹ Ø³Ù„ÙˆÙƒ Ù…Ø¹ÙŠØ´Ø© ØµØ§Ø¯Ù‚. ÙˆÙÙŠ Ø£ÙˆØ§Ø¦Ù„ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ÙˆØ§Ø­Ø¯ ÙˆØ§Ù„Ø¹Ø´Ø±ÙŠÙ† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø­ÙˆØ§Ù„ÙŠ 25 Ù…Ù„ÙŠÙˆÙ† Ø³ÙŠØ®ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆØªØ¹ÙŠØ´ Ø§Ù„ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ø£Ùˆ 76% (20 Ù…Ù„ÙŠÙˆÙ†) Ù…Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ØŒ Ù…ÙˆØ·Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø§Ù„Ù‡Ù†Ø¯ØŒ ÙˆÙŠØ¹ÙŠØ´ Ø­ÙˆØ§Ù„ÙŠ Ù…Ù„ÙŠÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø³Ø§Ø¨Ù‚Ø§Ù‹. ÙˆØ³Ø¨Ø¨ Ø§Ù†ØªØ´Ø§Ø±Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù‡Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ² Ø¹Ù„ÙŠÙ‡Ù… ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆØ¨ ÙˆÙ‡Ø¬Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ® Ø®Ø§Ø±Ø¬ Ø¨Ù„Ø§Ø¯Ù‡Ù…ØŒ Ø­ÙŠØ« Ø¨Ø¯Ø£Øª Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø®Ù„Ø§Ù„ Ø§Ù„Ù†ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø±ØŒ Ø¹Ù†Ø¯Ù…Ø§ Ø£ÙƒÙ…Ù„ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠÙˆÙ† Ø¶Ù…Ù‡Ù… Ù„Ù„Ø¨Ù†Ø¬Ø§Ø¨.\\nØªØ³ØªÙ†Ø¯ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù„ÙŠÙ… Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ù„Ù…Ø¤Ø³Ø³ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© ÙˆÙ‡Ùˆ Ø§Ù„ØºÙˆØ±Ùˆ Ù†Ø§Ù†Ø§ÙƒØŒ ÙˆØ®Ù„ÙØ§Ø¦Ù‡ Ø§Ù„ØªØ³Ø¹Ø© Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø±. Ù„Ù‚Ø¨ ØºÙˆØ±Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù…. Ø£Ù…Ø§ Ø§Ù„ØºÙˆØ±Ùˆ ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ù…Ù„Ù‚Ø¨ Ø¨Ø§Ù„Ø¹Ø§Ø´Ø±ØŒ Ø³Ø§Ù‡Ù… ÙÙŠ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø£Ø¬Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆÙƒØ§Ù† Ø¥Ø³Ù‡Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø¥Ø¶ÙØ§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„ØªÙŠ Ø£Ø³Ø³Ù‡Ø§ Ø£ÙˆÙ„Ù‹Ø§ Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù†Ø§Ù†Ø§Ùƒ Ø¯ÙŠÙ Ø¬ÙŠ ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø± Ø¥Ø³Ù‡Ø§Ù…Ù‹Ø§ Ø¬Ø¯ÙŠØ±Ù‹Ø§ Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©. ÙˆØ³Ù…Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ ÙƒØ®Ù„ÙŠÙØ© Ù„Ù‡ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ø£Ù†Ù‡Ù‰ Ø®Ø· Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø± ÙˆØ¬Ø¹Ù„ Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ­ÙŠ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ†ÙˆÙŠ Ù„Ù„Ø³ÙŠØ®. ÙˆØªØ±ÙØ¶ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø¨Ø£Ù† Ø£ÙŠ ØªÙ‚Ù„ÙŠØ¯ Ø¯ÙŠÙ†ÙŠ Ù…Ø¹ÙŠÙ† Ù„Ù‡ Ø§Ø­ØªÙƒØ§Ø± Ù„Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©. ÙˆØªØ·ÙˆØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø¯ÙŠÙ†ÙŠ. Ø­ÙŠØ« ØªØ¹Ø±Ø¶ Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø£ØªØ¨Ø§Ø¹ Ø§Ù„Ø³ÙŠØ® ÙˆÙ‡Ù… Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ± Ù„Ù„ØªØ¹Ø°ÙŠØ¨ ÙˆØ£Ø¹Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø¨Ø¹Ø¯ Ø±ÙØ¶Ù‡Ù… Ø§Ø¹ØªÙ†Ø§Ù‚ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. ÙˆØ£Ø«Ø§Ø± Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø®Ø§Ù„Ø³Ø§ ÙƒØ·Ù„Ø¨ Ù„Ø­Ù…Ø§ÙŠØ© Ø­Ø±ÙŠØ© Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø¯ÙŠÙ†.\\nØ§Ù„ØªØ§Ø±ÙŠØ®.\\nÙŠØ±ØªØ¨Ø· ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙˆØ«ÙŠÙ‚Ù‹Ø§ Ø¨ØªØ§Ø±ÙŠØ® Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±. Ù…Ù†Ø° Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…ØºÙˆÙ„ÙŠ Ù„Ù„Ù‡Ù†Ø¯ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ± Ø¬Ù‡Ø§Ù†ÙƒÙŠØ± (1605-1707)ØŒ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ ØµØ±Ø§Ø¹ Ù…Ø¹ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ù…ØºÙˆÙ„ØŒ Ù„Ø£Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªØ¤Ø«Ø± ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù‚Ø¨ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ù„Ù„Ù…ØºÙˆÙ„ ÙÙŠ Ø­ÙŠÙ† ØªØ¹ØªØ² Ø¨Ø§Ù„Ø£ÙˆÙ„ÙŠØ§Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. Ù‚ÙØªÙ„ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ø§Ø±Ø²ÙŠÙ† Ø¹Ù„Ù‰ ÙŠØ¯ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø§Ù†ØµÙŠØ§Ø¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ®. Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹ 10 Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®ØŒ Ø¹ÙØ°Ø¨ ÙˆØ£ÙØ¹Ø¯Ù… Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø£Ù†ÙØ³Ù‡Ù… (Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ±)ØŒ ÙˆØ£Ù‚Ø±Ø¨Ø§Ø¡ Ù…Ù‚Ø±Ø¨ÙŠÙ† Ù„Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù‚ÙØªÙ„Ùˆ Ø¨ÙˆØ­Ø´ÙŠØ© Ø¯ÙˆÙ† Ø±Ø­Ù…Ø© (Ù…Ø«Ù„ Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ† Ù…Ù† Ø§Ù„Ø¹Ù…Ø± 6 Ùˆ9 Ø³Ù†ÙˆØ§Øª)ØŒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„ØªÙŠ Ø¹ÙØ°Ø¨Øª ÙˆÙ‚ÙØªÙ„Øª (Ù…Ø«Ù„ Ø¨Ø§Ù†Ø¯Ø§ Ø¨Ù‡Ø§Ø¯ÙˆØ±ØŒ Ø¨Ù‡Ø§ÙŠ Ù…Ø§ØªÙŠ Ø¯Ø§Ø³ØŒ Ø¨Ù‡Ø§ÙŠ Ø³Ø§ØªÙŠ Ø¯Ø§Ø³ ÙˆØ¨Ø§Ù‡Ø§ÙŠ Ø¯ÙŠØ§Ù„Ø§)ØŒ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ø¨Ø±ÙŠÙ† Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø®Ø¶ÙˆØ¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ø³ÙƒØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù„Ù…Ø¹Ø§Ø±Ø¶Ø© Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ù…ØºÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ù….\\nØªÙ…ÙŠÙ‘Ø² Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙˆÙ†ÙØ¯Ø±Ø§Ù„ÙŠØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ø£Ù…Ø±Ø§Ø¡ ÙˆØ§Ù„Ø³ÙŠØ® ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ù…Ù‡Ø±Ø§Ø¬Ø§ Ø±Ø§Ù†Ø¬ÙŠØª Ø³ÙŠÙ†Øº Ø¨Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„ØªØ¹Ø§ÙŠØ´ Ø§Ù„Ø³Ù„Ù…ÙŠ ÙˆØ§Ù„ØªØ¹Ø¯Ø¯ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ù„Ø·Ø©. ÙŠØ¹Ø¯ ØªØ£Ø³ÙŠØ³ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¹Ø§Ø¯Ø©Ù‹ Ø°Ø±ÙˆØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØŒ Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙØªØ±Ø© Ø¬Ø§Ø¡Øª Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù„ØªØ´Ù…Ù„ ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§). Ø§Ø¹ØªÙ†Ù‚ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„ÙÙ„Ø§Ø­ÙŠÙ† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ø§Ù„Ø³ÙŠØ®ÙŠØ©. Ø£Ø®Ø° Ù‡Ø§Ø±ÙŠ Ø³ÙŠÙ†Øº Ù†Ø§Ù„ÙˆØ§ØŒ Ø§Ù„Ù‚Ø§Ø¦Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ø¬ÙŠØ´ Ø§Ù„Ø³ÙŠØ® Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ© Ø§Ù„ØºØ±Ø¨ÙŠØ©ØŒ Ø­Ø¯ÙˆØ¯ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥Ù„Ù‰ Ù…ØµØ¨ Ù…Ù…Ø± Ø®ÙŠØ¨Ø± (Ù‡Ùˆ Ù…Ù…Ø± Ø¬Ø¨Ù„ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø¨Ø§ÙƒØ³ØªØ§Ù†ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ø¹ Ø£ÙØºØ§Ù†Ø³ØªØ§Ù†). Ø¯Ù…Ø¬Øª Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ù„Ù…Ø§Ù†ÙŠØ© Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© ÙˆØ§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆØ­ÙƒÙˆÙ…ÙŠØ© Ù…Ø¨ØªÙƒØ±Ø©.\\nØ´Ù‡Ø¯Øª Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„ØªÙŠ Ø³Ø¨Ù‚Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ (ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø¥Ù„Ù‰ Ø¯ÙˆÙ„ØªÙŠÙ† Ù…Ø³ØªÙ‚Ù„ØªÙŠÙ†ØŒ Ø§Ù„Ù‡Ù†Ø¯ ÙˆØ¨Ø§ÙƒØ³ØªØ§Ù†) Ø³Ù†Ø© 1947ØŒ ØµØ±Ø§Ø¹Ù‹Ø§ Ø­Ø§Ø¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ (Ù…Ù†Ø·Ù‚Ø© Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ© ÙˆØ«Ù‚Ø§ÙÙŠØ© ÙˆØªØ§Ø±ÙŠØ®ÙŠØ© ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø¢Ø³ÙŠØ§ØŒ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©ØŒ ÙˆØªØ¶Ù… Ù…Ù†Ø§Ø·Ù‚ ÙÙŠ Ø´Ø±Ù‚ Ø¨Ø§ÙƒØ³ØªØ§Ù† ÙˆØ´Ù…Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯) Ø¨ÙŠÙ† Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†ØŒ Ø´Ù‡Ø¯ Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ù…Ù† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„ØºØ±Ø¨ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ù‡Ø¬Ø±Ø© Ø¯ÙŠÙ†ÙŠØ© Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø´Ø±Ù‚ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨. ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ø¶Ø±ØŒ ÙŠØ¹ÙŠØ´ ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø³ÙŠØ® ÙÙŠ ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙƒØ§Ø±Ø³ Ø£Ùˆ Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³Ø©.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙØ§Øª Ù‡Ù… ÙŠØµØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„ØªØ²Ø§Ù… Ø¨Ù‡Ø§ ÙˆØ¹Ù†Ø¯Ù‡Ù… Ù…Ù† Ù„Ø§ ÙŠÙ„ØªØ²Ù… Ø¨Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³ ÙŠØµÙÙˆÙ‡ Ø¨ØµÙØ© Ø¨Ø§ØªØª patitØ£ÙŠ Ø§Ù„Ù…Ø±ØªØ¯ØŒ ÙˆÙ…Ù† ÙŠØ¯Ø®Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¬Ø¯ÙŠØ¯ ÙŠØ¬Ø¹Ù„ÙˆÙ‡ ÙŠØªØ¹ÙˆØ¯ Ø¹Ù„ÙŠÙ‡Ø§ ÙˆÙŠØ³Ù…ÙˆÙ‡ Ø§Ù„Ù…ØªÙƒÙŠÙ Ø§Ù„Ø¨Ø·Ø¦.\\nØ§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®.\\nØ§Ù„ØºÙˆØ±Ùˆ ÙƒÙ„Ù…Ø© ØªØ¹Ù†ÙŠ Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆÙ‡Ù… Ø¹Ø´Ø±Ø© ØºÙˆØ±Ùˆ Ù„Ù„Ø³ÙŠØ®ÙŠØ© ÙˆÙ‡Ù…:\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ (Ù‡Ø§Ù†ØºÙ„: ê·¸ í•´ ìš°ë¦¬ëŠ”)Ø› Ù‡Ùˆ Ù…Ø³Ù„Ø³Ù„ ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ ÙƒÙˆÙ…ÙŠØ¯ÙŠ ÙƒÙˆØ±ÙŠ Ø¬Ù†ÙˆØ¨ÙŠØŒ ÙŠØµÙ†Ù Ø¨Ø£Ù†Ù‡ Â«Ø£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø£ØµÙ„ÙŠ Ù„ÙŠ Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†Â». Ù…Ù† Ø¨Ø·ÙˆÙ„Ø© ÙˆÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠØŒ Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ SBS TV Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021 Ø¥Ù„Ù‰ 25 ÙŠÙ†Ø§ÙŠØ± 2022ØŒ Ø¨Ø« ÙƒÙ„ ÙŠÙˆÙ… Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† ÙˆØ§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¹Ø© 22:00 Ø¨ØªÙˆÙ‚ÙŠØª (KST). ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­Ù‹Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ù†ØªÙÙ„ÙŠÙƒØ³. Ø§Ù„Ù‚ØµØ©. Ø§Ù†ÙØµÙ„ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº (ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ) ÙˆÙŠÙˆÙ† Ø³Ùˆ (ÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠ) Ù‚Ø¨Ù„ 10 Ø³Ù†ÙˆØ§ØªØŒ Ù„ÙƒÙ† ÙŠØµØ¨Ø­ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø°ÙŠ ØµÙˆØ±ÙˆÙ‡ Ø®Ù„Ø§Ù„ Ø£ÙŠØ§Ù… Ø¯Ø±Ø§Ø³ØªÙ‡Ù…Ø§ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø´Ø§Ø¦Ø¹Ù‹Ø§ØŒ Ø¨Ø³Ø¨Ø¨Ù‡ Ø¹Ù„ÙŠÙ‡Ù…Ø§ Ø§Ù„ÙˆÙ‚ÙˆÙ Ø£Ù…Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¬Ø¯Ø¯Ø§ØŒ Ø¹Ù„Ù‰ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ø§ÙŠØ±ÙŠØ¯ÙˆÙ† Ø°Ù„Ùƒ. Ø·Ø§Ù‚Ù…. Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ø±Ø³Ø§Ù… ØªØ´ÙƒÙŠÙ„ÙŠ Ø­Ø± Ù…ÙØ¹Ù… Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ©. Ø®Ø¨ÙŠØ±Ø© Ø¹Ù„Ø§Ù‚Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©. Ù…Ø®Ø±Ø¬ ÙˆØ«Ø§Ø¦Ù‚ÙŠØŒ ÙˆÙ‡Ùˆ ÙƒØ§Ù† Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ù„ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº ÙˆØ§ÙŠÙˆÙ† Ø³Ùˆ. Ø£ÙŠØ¯ÙˆÙ„ Ù…Ø´Ù‡ÙˆØ±Ø© ØªØ­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ Ù…Ù†Ø° Ø¸Ù‡ÙˆØ±Ù‡Ø§. Ø§Ù„Ø¥Ù†ØªØ§Ø¬. ØµÙ†Ø§Ø¹Ø©. ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†ØŒ ÙˆÙ‡ÙŠ Ø£ÙˆÙ„ Ø¯Ø±Ø§Ù…Ø§ Ø£ØµÙ„ÙŠØ© Ù„Ù‡Ù…ØŒ Ø³ÙŠØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ© Ø³ÙˆØ¨Ø± Ù…ÙˆÙ† Ø¨ÙŠÙƒØªØ´Ø±Ø²ØŒ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ Ø§Ù„ØªØ§Ø¨Ø¹ Ù„Ø¥Ø³ Ø¨ÙŠ Ø¥Ø³ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø¨Ù€ Â«Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ø³Â». Ø·Ø§Ù‚Ù…. ÙÙŠ Ù…Ø§Ø±Ø³ 2021ØŒ Ø£Ø¹Ù„Ù† ØªØ£ÙƒÙŠØ¯ Ø§Ù†Ø¶Ù…Ø§Ù… ÙƒÙ„ Ù…Ù† ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ ÙˆÙƒÙŠÙ… Ø¯Ø§ Ù…ÙŠ ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØŒ ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨Ù‡Ù… Ø¨Ø¹Ø¯ Ø«Ù„Ø§Ø« Ø³Ù†ÙˆØ§ØªØŒ Ø¢Ø®Ø± Ù…Ø±Ø© Ø¸Ù‡Ø±ÙˆØ§ ÙÙŠÙ‡ ÙÙŠ Ù…Ø¹Ø§ Ø¨ÙÙŠÙ„Ù… Ø§Ù„ØºÙ…ÙˆØ¶ \"\" Ù„Ø¹Ø§Ù… 2018. ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø§Ù‚Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠ ÙÙŠ 8 ÙŠÙˆÙ„ÙŠÙˆ. Ø§Ù„ØªØµÙˆÙŠØ±. Ø¨Ø¯Ø£ ØªØµÙˆÙŠØ± Ø§Ù„Ù…Ø³Ù„Ø³Ù„ ÙÙŠ Ø£ÙˆØ§Ø¦Ù„ ÙŠÙˆÙ„ÙŠÙˆ 2021. Ø·Ø¨Ø¹Ø© Ø£Ø®Ø±Ù‰. Ø³ÙŠØªÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†ØªØ§Ø¬ ÙˆÙŠØ¨ ØªÙˆÙ† Ù…Ù† Â«ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨Â»ØŒ Ø§Ù„ÙˆÙŠØ¨ ØªÙˆÙ† Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ø´Ø®ØµÙŠØªÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØªÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ (ØªØ´ÙˆÙŠ ÙˆØ§Ù†Øº ÙˆØ§ÙŠÙˆÙ† Ø³ÙˆÙˆ)ØŒ ÙˆÙ…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ÙŠØªÙ… Ø¥ØµØ¯Ø§Ø±Ù‡ Ø¹Ù„Ù‰ Â«Ù†Ø§ÙŠÙØ± ÙˆÙŠØ¨ØªÙˆÙˆÙ†Â» Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù…. Ù‡ÙŠ Ù†Ø§Ø´Ø± ÙˆÙ…Ù†ØµØ© Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨ Ø£Ø·Ù„Ù‚ØªÙ‡ Ø´Ø±ÙƒØ© Ù†Ø§ÙÙŠØ± ÙÙŠ ÙƒÙˆØ±ÙŠØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ© ÙÙŠ Ø¹Ø§Ù… 2004.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ù‡Ùˆ Ù…ØµØ·Ù„Ø­ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†ÙŠ ÙˆØ§Ù„Ø¹Ù†ÙŠÙ Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø´Ø£Ù†Ù‡ Ø£Ù† ÙŠØ³Ø¨Ø¨ Ù„Ù‡Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¶Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙŠÙŠÙ† Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ø³Ø¯ÙŠ. Ù‚Ø¯ ÙŠØªÙ… Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†Ù‹ ÙƒØ§Ù† ÙˆØ¹Ù„Ù‰ ÙŠØ¯ÙŠ Ø£ÙŠ Ø´Ø®Øµ ÙÙ‡Ùˆ Ù„Ø§ ÙŠÙ†Ø·ÙˆÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø£Ùˆ Ø³Ø¨Ø¨ ÙˆØ§Ø­Ø¯ ÙˆÙ‡Ùˆ ÙŠØ¹ØªØ¨Ø± Ø¸Ø§Ù‡Ø±Ø© Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± ÙˆÙ…Ù† Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªÙÙ‚Ù„ÙÙ‚ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© ÙˆØªØ¯ÙØ¹Ù‡Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø·Ø±Ù‚ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù…ÙØ³Ù†ÙŠÙ† ÙˆØ­Ù…Ø§ÙŠØªÙ‡Ù… Ù…Ù†Ù‡. ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù… Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø© ÙˆÙ‡Ù… Ø£Ø´Ø®Ø§Øµ ÙŠØ­ØµÙ„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù…ØªÙŠØ§Ø²Ø§Øª ÙˆØ­Ù…Ø§ÙŠØ© Ù…ÙƒØ«ÙØ© ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆØ°Ù„Ùƒ Ø¨Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø±Ù‚Ù… 26 Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆÙ‡Ùˆ Ù‚Ø§Ù†ÙˆÙ† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§ØµØ±ÙŠÙ† ÙˆØ§Ù„Ø¹Ø§Ø¬Ø²ÙŠÙ† ÙƒÙ…Ø§ ÙˆÙŠØ¹ÙˆØ¯ Ø°Ù„Ùƒ Ù„Ø¶Ø¹ÙÙ‡Ù… ÙˆÙ„ÙƒØ«Ø±Ø© Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡Ù… ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ§Ø¬Ù‡Ù‡Ø§ Ù‚Ø³Ù… ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ù…. ÙÙŠ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙŠØªÙ… Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" ÙˆØ¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø¥Ø³Ø§Ø¡Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ø­ÙŠØ« Ø£Ù†Ù‡ ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø³Ø§Ø¡Ø© Ù„ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù‚Ø¯ ØªØ¶Ù…Ù† Ø§Ù„Ø¹Ù†Ù Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªØ­Ø¯Ø« Ù…Ù† Ø´Ø®Øµ Ù…Ù‚Ø±Ø¨ Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø³Ù† Ù…Ø«Ù„ Ø§Ù„Ø£Ù‚Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø®ØµØµÙŠÙ† Ù„Ø±Ø¹Ø§ÙŠØªÙ‡Ù… ÙˆØªØ³ØªÙ…Ø± Ù„ÙØªØ±Ø© ÙˆÙ‡Ùˆ Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø§Ø¯Ø« Ø§Ù„Ø°ÙŠ ÙÙŠÙ‡ ÙŠØªÙ… Ø§Ù„ØªÙ‡Ø¬Ù… Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙƒØ¨ÙŠØ± Ø¨Ø§Ù„Ø³Ù† Ù…Ù† Ù‚Ø¨Ù„ Ø´Ø®Øµ ØºØ±ÙŠØ¨ ÙˆÙŠØ­Ø¯Ø« Ø°Ù„Ùƒ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙŠÙƒÙˆÙ† Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ Ø§Ù„Ø´Ø±Ø·Ø©. Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ± Ø£Ù† Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ØªÙ‚ÙˆÙ… Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ ÙƒÙ„ØªØ§ Ø§Ù„Ø­Ø§Ù„ØªÙŠÙ†ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ø¥Ø³Ø§Ø¡Ø© Ø§Ùˆ Ø¹Ù†ÙØŒ Ø¨Ù‡Ø¯Ù Ø§Ù„Ø­Ø¯ Ù…Ù† Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø­Ø³Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø°ÙŠ Ø£ØµØ¯Ø±ØªÙ‡ Ø§Ù„ÙƒÙ†ÙŠØ³Øª Ø¹Ø§Ù… 2007 ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¹Ø±Ø¶ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙØ£Ù†Ù‡ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø£Ø¬Ø±ÙŠØª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø³Ù†Ø© 2006 Ø¹Ø§Ø´ Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ Ø§Ù„ 70ØŒ000 Ù…ÙØ³Ù† ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ø§Ø¯Ù„ 10% Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³ÙƒØ§Ù†ØŒ 30% Ù…Ù†Ù‡Ù… Ù‡Ù… Ù†Ø§Ø¬ÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ø­Ø±Ù‚Ø© Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ© Ø§Ùˆ Ø§Ù„Ù‡ÙˆÙ„ÙˆÙƒÙˆØ³Øª ÙƒÙ…Ø§ Ø§Ù† Ø­ÙˆØ§Ù„ÙŠ 12ØŒ000 Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙŠØ¹ÙŠØ´ÙˆÙ† Ù…Ø¹ Ø´Ø®Øµ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø§ ØªØ±Ø¨Ø·Ù‡ Ù…Ø¹Ù‡Ù… ØµÙ„Ø© Ø¯Ù… (Ø¹Ø§Ø¯Ø© Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ùˆ Ù…Ø±Ø§ÙÙ‚ Ù„Ù„Ø§Ø¹ØªÙ†Ø§Ø¡ Ø¨Ù‡Ù…). Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø´Ø±Ø·Ø© ØªØªÙ„Ù‚Ù‰ Ø¹Ø¯Ø¯ Ù‡Ø§Ø¦Ù„ Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª ÙˆÙØªØ­Øª Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 12,228 Ù…Ù„Ù ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¹Ù†Ù ÙˆØ§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù…Ù† Ø¹Ø§Ù… 2002 Ø­ØªÙ‰ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù…Ù† Ø³Ø¨ØªÙ…Ø¨Ø± Ø¹Ø§Ù… 2007 (Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ØªØªØ¶Ù…Ù† Ø§Ù„Ù‚ØªÙ„ØŒ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù‚ØªÙ„ØŒ Ø§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ÙˆØ§Ù„Ø§ØºØªØµØ§Ø¨ØŒ Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø³Ø±Ù‚Ø© ÙˆØ§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠÙˆØª ÙƒÙ…Ø§ ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ ÙˆØ§Ù„ØªØ®Ø±ÙŠØ¨) Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙŠ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ù„Ø§ ÙŠØªØ¶Ù…Ù† Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙˆØµÙ„ÙˆØ§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø±Ø³ Ø§Ù„Ù…Ø¯Ù†ÙŠ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ù‚Ø³Ø§Ù…. Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ÙØ¥Ù†: 32% Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ù‡ÙŠ Ø¨Ù„Ø§ØºØ§Øª Ø¹Ù† Ø§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø¨ÙŠÙˆØªÙ‡Ù…ØŒ 17% Ù‡ÙŠ Ø¹Ù† ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ØªÙ„Ù‚ÙˆÙ‡Ø§ØŒ 31% Ø¹Ù† Ø³Ø±Ù‚Ø§Øª ØªØ¹Ø±Ø¶ÙˆØ§ Ù„Ù‡Ø§ Ø¨ÙŠÙ†Ù…Ø§ 11% Ù‡ÙŠ ÙÙ‚Ø· Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ø³Ø¯ÙŠØ©. Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ø³Ù†ÙˆØ§Øª (2002-2007) ÙƒØ§Ù† ÙŠÙ‚Ø¯Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø§Ù„Ù‚Ø§Ø¯Ù…ÙˆÙ† Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø¯ÙˆÙ„Ø© (Ø§Ù„Ù…Ù‡Ø§Ø¬Ø±ÙˆÙ†) Ø­ÙˆØ§Ù„ÙŠ 9% Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ø±ØºÙ… Ø£Ù† Ù†Ø³Ø¨ØªÙ‡Ù… Ù…Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ù„Ø§ ØªØªØ¹Ø¯Ù‰ 25% Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø³Ù†ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙƒØ§Ù†ÙˆØ§ ÙŠÙ‚Ø¯Ù…ÙˆÙ† 5% Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª. ÙˆÙÙ‚Ù‹Ø§ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø¥Ù† Ù„Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ Ø£Ø«Ø±Ù‹Ø§ Ø³Ù„Ø¨ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙ‚Ø¯ Ù„ÙˆØ­Ø¸ Ø§Ø±ØªÙØ§Ø¹ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ÙÙŠÙ‡Ø§ ØªØ¹Ù†ÙŠÙ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ØªÙ…Øª Ù…Ù† Ù‚Ø¨Ù„ Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙˆÙ…Ù‚Ø¯Ù…ÙŠ Ø§Ù„Ø±Ø¹Ø§ÙŠØ©) ÙØ¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø¹Ø§Ù… 2018 ØªÙ… Ø¹Ù„Ø§Ø¬ 6707 Ø­Ø§Ù„Ø© Ø¨ÙŠÙ†Ù…Ø§ Ø­ØªÙ‰ Ù…Ù†ØªØµÙ Ø¹Ø§Ù… 2019 ÙÙ‚Ø· ØªÙ… Ø¹Ù„Ø§Ø¬ Ø£ÙƒØ«Ø± Ù…Ù† 7000 Ø­Ø§Ù„Ø© ÙˆÙ‡Ø°Ø§ Ø¹Ø¯Ø§ Ø¹Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯. ÙŠØ¹Ø¯ Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„ØªÙƒÙ„ÙŠÙ Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø·Ø§Ù‚Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù…Ù„ Ø§Ù„ÙÙ‚Ø¯ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ Ù„Ù„ØªØ­Ù…Ù„ ÙˆØ§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø¸Ø±ÙˆÙ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ ØªØ®ØµØµ ÙŠÙ‡ØªÙ… Ø¨Ø¯Ø±Ø§Ø³Ø© ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©.\\nØ£ØµØ¨Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø±ÙˆÙØ§Ù‹ ÙÙŠ Ø£ÙˆØ§Ø®Ø± Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø± ÙˆØ°Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙ„ØºØ±Ø§Ù ÙˆÙ…Ø­Ø·Ø§Øª Ø¥Ù…Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù‚Ø©. ÙˆØ§Ù„Ø¢Ù† ÙŠØºØ·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¹Ø¯Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ÙØ±Ø¹ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ†Ø¸Ù… Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù„Ø§Ø³Ù„ÙƒÙŠØ©.\\nÙˆÙ…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£Ù† Ù†Ù‚ÙˆÙ„ Ø£Ù† Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‚Ø¯ ØªØªØ¶Ù…Ù† Ø£ÙŠØ¶Ø§Ù‹ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ‚Ø¯ Ù„Ø§ ØªØªØ¶Ù…Ù†Ù‡Ø§. ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ†Ù‡Ù…Ø§ Ø­ÙŠØ« ØªÙ‡ØªÙ… Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¨Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬Ù‡Ø¯ Ù…Ø«Ù„ Ù†Ù‚Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ¹Ø§Ù…Ù„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù…Ø¹ Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ù†Ø¸Ù… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (ØªÙŠØ§Ø± Ù…Ù†Ø®ÙØ¶ â€“Ø¬Ù‡Ø¯ Ù…Ù†Ø®ÙØ¶)ØŒ ÙˆÙŠØªØ¶Ù…Ù† Ø°Ù„Ùƒ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„Ø©.\\nÙˆØªØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¯Ø±Ø§Ø³Ø© ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø§ØªØŒ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§ØªØŒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù‚Ø¯Ø±Ø© ØºÙŠØ± Ø§Ù„Ù…Ù†Ù‚Ø·Ø¹Ø© UPSØŒ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆÙ…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©.\\nØªØ§Ø±ÙŠØ® ÙˆØ£Ø¹Ù„Ø§Ù… Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©.\\nØ¸Ù‡Ø± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø¨Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…Ù†Ø° Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø¹Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„. ÙÙŠØ¹ØªÙ‚Ø¯ Ø£Ù† Ø£ÙˆÙ„ Ù…Ù‡Ù†Ø¯Ø³ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù‡Ùˆ ÙˆÙ„ÙŠØ§Ù… Ø¬Ù„Ø¨Ø±Øª Ø§Ù„Ø°ÙŠ ØµÙ…Ù… Ø¢Ù„Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø°Ø§Øª Ø§Ù„Ø´Ø­Ù†Ø§Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©. ÙˆÙ‡Ùˆ Ù…Ù† ÙØ±ÙŽÙ‘Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©ØŒ ÙƒÙ…Ø§ ÙŠØ¹ØªÙ‚Ø¯ Ø¨Ø£Ù†Ù‡ Ø£ÙˆÙ„ Ù…Ù† Ø£Ù†Ø´Ø£ Ù…ØµØ·Ù„Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.\\nÙˆÙÙŠ Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ù…Ø± ÙƒØ§Ù†Øª ÙƒÙ„ Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª ÙˆØ§Ù„Ø§Ø®ØªØ±Ø§Ø¹Ø§Øª ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø´Ø­Ù†Ø©. ÙˆØ¨Ø¯Ø£ ÙØµÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¹Ù† Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ ÙÙŠ Ø²Ù…Ù† ØªÙˆÙ…Ø§Ø³ Ø§Ø¯ÙŠØ³ÙˆÙ† ÙˆÙÙŠØ±Ù†Ø± ÙÙˆÙ† Ø³ÙŠÙ…Ù†Ø³. ÙˆÙÙŠ Ø¹Ø§Ù… 1752 Ø§Ø®ØªØ±Ø¹ Ø¨ÙŠÙ†ÙŠØ§Ù…ÙŠÙ† ÙØ±Ø§Ù†ÙƒÙ„ÙŠÙ† Ù…ÙˆØµÙ„Ø© Ø§Ù„ØµÙˆØ§Ø¹Ù‚ ÙˆÙ†Ø´Ø± Ø¨ÙŠÙ† 1751 Ùˆ1753 Ù†ØªØ§Ø¦Ø¬ ØªØ¬Ø§Ø±Ø¨Ù‡ ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† Â«ØªØ¬Ø§Ø±Ø¨ ÙˆÙ…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡Â» (Experiments and Observations on Electricity). ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1800 Ù‚Ø§Ù… Ø£Ù„Ø³Ø§Ù†Ø¯Ø±Ùˆ ÙÙˆÙ„ØªØ§ Ø¨Ø¨Ù†Ø§Ø¡ Ø¨Ø·Ø§Ø±ÙŠØªÙ‡ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù…Ø³Ù…Ø§Ø© Â«Ø¹Ù…ÙˆØ¯ ÙÙˆÙ„ØªØ§Â» Ø¨Ø¹Ø¯ Ø§Ø¹Ø¬Ø§Ø¨Ù‡ Ø¨ØªØ¬Ø±Ø¨Ø© Ø§Ø¬Ø±Ø§Ù‡Ø§ Ù„ÙˆÙŠØ¬ÙŠ Ø¬Ø§Ù„ÙØ§Ù†ÙŠ Ø¹Ø§Ù… 1792. ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1820 Ù‚Ø§Ù… Ù‡Ø§Ù†Ø² ÙƒØ±ÙŠØ³ØªÙŠØ§Ù† Ø§ÙˆØ±Ø³ØªØ¯ Ø¨Ø¹Ù…Ù„ ØªØ¬Ø§Ø±Ø¨ Ø¹Ù† Ø§Ù†Ø­Ù†Ø§Ø¡ Ø§Ø¨Ø±Ø© Ø§Ù„Ø¨ÙˆØµÙ„Ø© Ø¨ØªØ§Ø«ÙŠØ± Ø§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ. ÙˆÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¹Ø§Ù… ÙƒØ±Ø± Ø§Ù†Ø¯Ø±ÙŠÙ‡ Ù…Ø§Ø±ÙŠ Ø§Ù…Ø¨ÙŠØ± ØªÙ„Ùƒ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ø«Ø¨Øª Ø§Ù† Ø³Ù„ÙƒÙŠÙ† ÙŠÙ…Ø± ÙÙŠÙ‡Ù…Ø§ Ø§Ù„ØªÙŠØ§Ø± ÙŠØ¤Ø«Ø±Ø§Ù† Ø¨Ù‚ÙˆÙ‰ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶ ÙˆØ¹Ø±Ù Ø®Ù„Ø§Ù„Ù‡Ø§ Ø§Ù„Ø¬Ù‡Ø¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ.\\nÙ…Ø§ÙŠÙƒÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ (ÙŠÙ†Ø·Ù‚ Ø£ÙŠØ¶Ø§ Ù…ÙŠØ´ÙŠÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ) Ù‚Ø¯Ù… Ø£Ø¹Ù…Ø§Ù„ ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙÙŠØ¶ÙŠÙ† Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØŒ ÙˆØ¹Ø±Ù Ø£ÙŠØ¶Ø§ Ø®Ø·ÙˆØ· Ø§Ù„Ù…Ø¬Ø§Ù„. ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø§Ù„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ Ù‚Ø¯Ù… Ø¬ÙŠÙ…Ø³ ÙƒÙ„ÙŠØ±Ùƒ Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ Ø£Ø¹Ù…Ø§Ù„Ø§ ÙÙŠ Ø¥ÙƒÙ…Ø§Ù„ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆØ¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©Ù€ ÙˆÙ‚Ø¯Ù… Ø¹Ø§Ù… 1864 Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ ÙˆØ§Ù„ØªÙŠ ØªØ¹ØªØ¨Ø± Ø£Ø­Ø¯ Ø£Ù‡Ù… Ø£Ø³Ø³ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ©.\\nØªØ·ÙˆØ± Ù…Ø¬Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¶ÙˆØ¡ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø£Ø¯Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø°ÙˆÙŠ ØªØ¬Ø±Ø¨Ø©. ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙØªØ±Ø© ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ù‡Ù„Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ®ØµØµØ§Øª Ø¨Ø£Ø·Ø± Ø£Ø¹Ù…Ø§Ù„ Ù‡Ù†Ø¯Ø³ÙŠØ©. Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù‡Ù„Øª Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø¨Ù†Ø§Ø¡ ÙˆÙ…Ù‡Ù†Ø¯Ø³Ùˆ Ù…Ø§ÙƒÙŠÙ†Ø§Øª Ù„ÙƒÙ†Ù‡Ø§ ÙØ´Ù„Øª Ø¨Ø§Ù† ØªØ¤Ù‡Ù„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ù„Ù‰ Ø¶ÙˆØ¡ ØªØ³Ø§Ø±Ø¹ Ø§Ù„Ù…Ø¬Ø§Ù„ØŒ ÙˆÙ„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ù‡Ù†Ø¯Ø³Ùˆ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙŠØ¯Ø±Ø³ÙˆÙ† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù„Ø·Ù„Ø§Ø¨ Ù‡Ù†Ø¯Ø³Ù‡ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.Â \\nØ¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ù…Ù‡Ù…Ø© ØªØ¹Ù„ÙŠÙ… ÙˆØªØ£Ù‡ÙŠÙ„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆÙ‚Ø¹Øª Ø¹Ù„Ù‰ Ø¹Ø§ØªÙ‚ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠÙ†ØŒ Ø­ÙŠØ« Ø§Ù†Ù‡ Ù„Ù… ÙŠÙƒÙ† Ù„Ø§Ø­Ø¯ Ø§Ø®Ø± Ø§Ù„Ø¹Ù„Ù… Ø¨Ù…Ø¬Ø§Ù„ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙˆØ§Ù„Ù‚Ø¯Ø±Ø© Ù„ØªÙˆØµÙŠÙ„Ù‡ Ù„Ø§ÙƒØ¨Ø± Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨.\\n   \n",
              "\n",
              "                                               question  \\\n",
              "0         \\nÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙŠ Ø´Ù…Ù„ØªÙ‡Ø§ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ®ØŸ   \n",
              "1                 Ù…ØªÙ‰ ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\\n   \n",
              "2           Ù…Ù† Ù‡Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ØŸ\\n\\n   \n",
              "3         Ù…Ø§ Ù‡ÙŠ Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n   \n",
              "4  Ù…Ø§ Ù‡Ùˆ Ø¯ÙˆØ± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n   \n",
              "\n",
              "                                                                                                                              ground_truth  \\\n",
              "0  ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§)   \n",
              "1                                                                                                                         Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021   \n",
              "2                                                                                                              Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø©   \n",
              "3                                            Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯   \n",
              "4                                                                                                                                        .   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            model_answer  \n",
              "0                                                              Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù‡ÙŠ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1479 Ø¥Ù„Ù‰ 1516. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1516 Ø¥Ù„Ù‰ 1568. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1568 Ø¥Ù„Ù‰ 1700. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1700 Ø¥Ù„Ù‰ 1808. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1808 Ø¥Ù„Ù‰ 1833. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1833 Ø¥Ù„Ù‰ 1868. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1868 Ø¥Ù„Ù‰ 1870. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1870 Ø¥Ù„Ù‰ 1873. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1873 Ø¥Ù„Ù‰ 1874. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1874 Ø¥Ù„Ù‰ 1876. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1876 Ø¥Ù„Ù‰ 1877. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1877 Ø¥Ù„Ù‰ 1878. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1878 Ø¥Ù„Ù‰ 1880. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1880 Ø¥Ù„Ù‰ 1881. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1881 Ø¥Ù„Ù‰ 1882. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1882 Ø¥Ù„Ù‰ 1883. Ùˆ  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\" ÙÙŠ  2023Ù….  \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù…Ø§ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ØªØªØ±Ø§ÙˆØ­ Ø£Ø¹Ù…Ø§Ø±Ù‡Ù… Ø¨ÙŠÙ† 65 Ùˆ 100 Ø¹Ø§Ù….  \n",
              "3                                                                                                                                                                                                                                                                                                                        Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø·Ø§Ù‚Ø© ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ø£Ùˆ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©ØŒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©. ÙŠØ¹ØªØ¨Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù†Ø¸Ø§Ù…Ù‹Ø§ Ù…ØªÙ‚Ø¯Ù…Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø§Ù‚Ø©ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ²ÙŠØ¯ Ù…Ù† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©.  \n",
              "4   Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù‡ÙŠ Ø£Ø¯ÙˆØ§Øª ØªØªØ¨Ø¹ ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØª  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3815533-d54b-4ee9-bc33-d0bce89f26eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>model_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ø§Ù„Ø³ÙŠØ®ÙŠØ© (Ø¨Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ÙŠÙŽÙ‘Ø©: à¨¸à¨¿à©±à¨–à©€) Ù‡ÙŠ Ø¯ÙŠØ§Ù†Ø© ØªÙˆØ­ÙŠØ¯ÙŠØ© Ø¯Ø§Ø±Ù…ÙŠÙŽÙ‘Ø© Ù†Ø´Ø£Øª ÙÙŠ Ø´Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù†Ø¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø±. ÙˆØªØ£ØªÙŠ ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®ÙŠØ©Â» Ù…Ù† ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®Â» ÙˆÙ‡ÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ ØªØ£ØªÙŠ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø³Ù†Ø³ÙƒØ±ÙŠØªÙŠ Ø§Ù„ØªÙŠ ØªØ¹Ù†ÙŠ Ø§Ù„ØªÙ„Ù…ÙŠØ° Ùˆ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¯ Ø£Ùˆ Ø§Ù„ØªØ§Ø¨Ø¹. ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø¯ÙŠØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£ÙƒØ¨Ø± Ø§Ù„Ø¯ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…. ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ØªÙ‚Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙˆØ¶ÙØ­ÙŽØª ÙÙŠ ÙƒØªØ§Ø¨Ù‡Ù… Ø§Ù„Ù…Ù‚Ø¯Ø³ Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ØŒ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† ÙˆØ§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ø®Ø§Ù„Ù‚ Ø§Ù„ÙˆØ§Ø­Ø¯ØŒ ÙˆØ§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙˆØ§Ø© Ù„Ù„Ø¨Ø´Ø±ÙŠØ© Ø¬Ù…Ø¹Ø§Ø¡ØŒ ÙˆØ§Ù„Ø§Ù†Ø®Ø±Ø§Ø· ÙÙŠ Ø®Ø¯Ù…Ø© Ù†ÙƒØ±Ø§Ù† Ø§Ù„Ø°Ø§ØªØŒ ÙˆØ§Ù„Ø³Ø¹ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆØ§Ø²Ø¯Ù‡Ø§Ø±Ù‡Ø§ØŒ ÙˆØ¥ØªØ¨Ø§Ø¹ Ø³Ù„ÙˆÙƒ Ù…Ø¹ÙŠØ´Ø© ØµØ§Ø¯Ù‚. ÙˆÙÙŠ Ø£ÙˆØ§Ø¦Ù„ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ÙˆØ§Ø­Ø¯ ÙˆØ§Ù„Ø¹Ø´Ø±ÙŠÙ† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø­ÙˆØ§Ù„ÙŠ 25 Ù…Ù„ÙŠÙˆÙ† Ø³ÙŠØ®ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆØªØ¹ÙŠØ´ Ø§Ù„ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ø£Ùˆ 76% (20 Ù…Ù„ÙŠÙˆÙ†) Ù…Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ØŒ Ù…ÙˆØ·Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø§Ù„Ù‡Ù†Ø¯ØŒ ÙˆÙŠØ¹ÙŠØ´ Ø­ÙˆØ§Ù„ÙŠ Ù…Ù„ÙŠÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø³Ø§Ø¨Ù‚Ø§Ù‹. ÙˆØ³Ø¨Ø¨ Ø§Ù†ØªØ´Ø§Ø±Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù‡Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ² Ø¹Ù„ÙŠÙ‡Ù… ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆØ¨ ÙˆÙ‡Ø¬Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ® Ø®Ø§Ø±Ø¬ Ø¨Ù„Ø§Ø¯Ù‡Ù…ØŒ Ø­ÙŠØ« Ø¨Ø¯Ø£Øª Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø®Ù„Ø§Ù„ Ø§Ù„Ù†ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø±ØŒ Ø¹Ù†Ø¯Ù…Ø§ Ø£ÙƒÙ…Ù„ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠÙˆÙ† Ø¶Ù…Ù‡Ù… Ù„Ù„Ø¨Ù†Ø¬Ø§Ø¨.\\nØªØ³ØªÙ†Ø¯ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù„ÙŠÙ… Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ù„Ù…Ø¤Ø³Ø³ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© ÙˆÙ‡Ùˆ Ø§Ù„ØºÙˆØ±Ùˆ Ù†Ø§Ù†Ø§ÙƒØŒ ÙˆØ®Ù„ÙØ§Ø¦Ù‡ Ø§Ù„ØªØ³Ø¹Ø© Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø±. Ù„Ù‚Ø¨ ØºÙˆØ±Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù…. Ø£Ù…Ø§ Ø§Ù„ØºÙˆØ±Ùˆ ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ù…Ù„Ù‚Ø¨ Ø¨Ø§Ù„Ø¹Ø§Ø´Ø±ØŒ Ø³Ø§Ù‡Ù… ÙÙŠ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø£Ø¬Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆÙƒØ§Ù† Ø¥Ø³Ù‡Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø¥Ø¶ÙØ§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„ØªÙŠ Ø£Ø³Ø³Ù‡Ø§ Ø£ÙˆÙ„Ù‹Ø§ Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù†Ø§Ù†Ø§Ùƒ Ø¯ÙŠÙ Ø¬ÙŠ ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø± Ø¥Ø³Ù‡Ø§Ù…Ù‹Ø§ Ø¬Ø¯ÙŠØ±Ù‹Ø§ Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©. ÙˆØ³Ù…Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ ÙƒØ®Ù„ÙŠÙØ© Ù„Ù‡ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ø£Ù†Ù‡Ù‰ Ø®Ø· Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø± ÙˆØ¬Ø¹Ù„ Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ­ÙŠ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ†ÙˆÙŠ Ù„Ù„Ø³ÙŠØ®. ÙˆØªØ±ÙØ¶ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø¨Ø£Ù† Ø£ÙŠ ØªÙ‚Ù„ÙŠØ¯ Ø¯ÙŠÙ†ÙŠ Ù…Ø¹ÙŠÙ† Ù„Ù‡ Ø§Ø­ØªÙƒØ§Ø± Ù„Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©. ÙˆØªØ·ÙˆØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø¯ÙŠÙ†ÙŠ. Ø­ÙŠØ« ØªØ¹Ø±Ø¶ Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø£ØªØ¨Ø§Ø¹ Ø§Ù„Ø³ÙŠØ® ÙˆÙ‡Ù… Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ± Ù„Ù„ØªØ¹Ø°ÙŠØ¨ ÙˆØ£Ø¹Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø¨Ø¹Ø¯ Ø±ÙØ¶Ù‡Ù… Ø§Ø¹ØªÙ†Ø§Ù‚ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. ÙˆØ£Ø«Ø§Ø± Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø®Ø§Ù„Ø³Ø§ ÙƒØ·Ù„Ø¨ Ù„Ø­Ù…Ø§ÙŠØ© Ø­Ø±ÙŠØ© Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø¯ÙŠÙ†.\\nØ§Ù„ØªØ§Ø±ÙŠØ®.\\nÙŠØ±ØªØ¨Ø· ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙˆØ«ÙŠÙ‚Ù‹Ø§ Ø¨ØªØ§Ø±ÙŠØ® Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±. Ù…Ù†Ø° Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…ØºÙˆÙ„ÙŠ Ù„Ù„Ù‡Ù†Ø¯ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ± Ø¬Ù‡Ø§Ù†ÙƒÙŠØ± (1605-1707)ØŒ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ ØµØ±Ø§Ø¹ Ù…Ø¹ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ù…ØºÙˆÙ„ØŒ Ù„Ø£Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªØ¤Ø«Ø± ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù‚Ø¨ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ù„Ù„Ù…ØºÙˆÙ„ ÙÙŠ Ø­ÙŠÙ† ØªØ¹ØªØ² Ø¨Ø§Ù„Ø£ÙˆÙ„ÙŠØ§Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. Ù‚ÙØªÙ„ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ø§Ø±Ø²ÙŠÙ† Ø¹Ù„Ù‰ ÙŠØ¯ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø§Ù†ØµÙŠØ§Ø¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ®. Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹ 10 Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®ØŒ Ø¹ÙØ°Ø¨ ÙˆØ£ÙØ¹Ø¯Ù… Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø£Ù†ÙØ³Ù‡Ù… (Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ±)ØŒ ÙˆØ£Ù‚Ø±Ø¨Ø§Ø¡ Ù…Ù‚Ø±Ø¨ÙŠÙ† Ù„Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù‚ÙØªÙ„Ùˆ Ø¨ÙˆØ­Ø´ÙŠØ© Ø¯ÙˆÙ† Ø±Ø­Ù…Ø© (Ù…Ø«Ù„ Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ† Ù…Ù† Ø§Ù„Ø¹Ù…Ø± 6 Ùˆ9 Ø³Ù†ÙˆØ§Øª)ØŒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„ØªÙŠ Ø¹ÙØ°Ø¨Øª ÙˆÙ‚ÙØªÙ„Øª (Ù…Ø«Ù„ Ø¨Ø§Ù†Ø¯Ø§ Ø¨Ù‡Ø§Ø¯ÙˆØ±ØŒ Ø¨Ù‡Ø§ÙŠ Ù…Ø§ØªÙŠ Ø¯Ø§Ø³ØŒ Ø¨Ù‡Ø§ÙŠ Ø³Ø§ØªÙŠ Ø¯Ø§Ø³ ÙˆØ¨Ø§Ù‡Ø§ÙŠ Ø¯ÙŠØ§Ù„Ø§)ØŒ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ø¨Ø±ÙŠÙ† Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø®Ø¶ÙˆØ¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ø³ÙƒØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù„Ù…Ø¹Ø§Ø±Ø¶Ø© Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ù…ØºÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ù….\\nØªÙ…ÙŠÙ‘Ø² Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙˆÙ†ÙØ¯Ø±Ø§Ù„ÙŠØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ø£Ù…Ø±Ø§Ø¡ ÙˆØ§Ù„Ø³ÙŠØ® ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ù…Ù‡Ø±Ø§Ø¬Ø§ Ø±Ø§Ù†Ø¬ÙŠØª Ø³ÙŠÙ†Øº Ø¨Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„ØªØ¹Ø§ÙŠØ´ Ø§Ù„Ø³Ù„Ù…ÙŠ ÙˆØ§Ù„ØªØ¹Ø¯Ø¯ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ù„Ø·Ø©. ÙŠØ¹Ø¯ ØªØ£Ø³ÙŠØ³ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¹Ø§Ø¯Ø©Ù‹ Ø°Ø±ÙˆØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØŒ Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙØªØ±Ø© Ø¬Ø§Ø¡Øª Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù„ØªØ´Ù…Ù„ ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§). Ø§Ø¹ØªÙ†Ù‚ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„ÙÙ„Ø§Ø­ÙŠÙ† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ø§Ù„Ø³ÙŠØ®ÙŠØ©. Ø£Ø®Ø° Ù‡Ø§Ø±ÙŠ Ø³ÙŠÙ†Øº Ù†Ø§Ù„ÙˆØ§ØŒ Ø§Ù„Ù‚Ø§Ø¦Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ø¬ÙŠØ´ Ø§Ù„Ø³ÙŠØ® Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ© Ø§Ù„ØºØ±Ø¨ÙŠØ©ØŒ Ø­Ø¯ÙˆØ¯ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥Ù„Ù‰ Ù…ØµØ¨ Ù…Ù…Ø± Ø®ÙŠØ¨Ø± (Ù‡Ùˆ Ù…Ù…Ø± Ø¬Ø¨Ù„ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø¨Ø§ÙƒØ³ØªØ§Ù†ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ø¹ Ø£ÙØºØ§Ù†Ø³ØªØ§Ù†). Ø¯Ù…Ø¬Øª Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ù„Ù…Ø§Ù†ÙŠØ© Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© ÙˆØ§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆØ­ÙƒÙˆÙ…ÙŠØ© Ù…Ø¨ØªÙƒØ±Ø©.\\nØ´Ù‡Ø¯Øª Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„ØªÙŠ Ø³Ø¨Ù‚Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ (ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø¥Ù„Ù‰ Ø¯ÙˆÙ„ØªÙŠÙ† Ù…Ø³ØªÙ‚Ù„ØªÙŠÙ†ØŒ Ø§Ù„Ù‡Ù†Ø¯ ÙˆØ¨Ø§ÙƒØ³ØªØ§Ù†) Ø³Ù†Ø© 1947ØŒ ØµØ±Ø§Ø¹Ù‹Ø§ Ø­Ø§Ø¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ (Ù…Ù†Ø·Ù‚Ø© Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ© ÙˆØ«Ù‚Ø§ÙÙŠØ© ÙˆØªØ§Ø±ÙŠØ®ÙŠØ© ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø¢Ø³ÙŠØ§ØŒ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©ØŒ ÙˆØªØ¶Ù… Ù…Ù†Ø§Ø·Ù‚ ÙÙŠ Ø´Ø±Ù‚ Ø¨Ø§ÙƒØ³ØªØ§Ù† ÙˆØ´Ù…Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯) Ø¨ÙŠÙ† Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†ØŒ Ø´Ù‡Ø¯ Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ù…Ù† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„ØºØ±Ø¨ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ù‡Ø¬Ø±Ø© Ø¯ÙŠÙ†ÙŠØ© Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø´Ø±Ù‚ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨. ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ø¶Ø±ØŒ ÙŠØ¹ÙŠØ´ ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø³ÙŠØ® ÙÙŠ ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙƒØ§Ø±Ø³ Ø£Ùˆ Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³Ø©.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙØ§Øª Ù‡Ù… ÙŠØµØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„ØªØ²Ø§Ù… Ø¨Ù‡Ø§ ÙˆØ¹Ù†Ø¯Ù‡Ù… Ù…Ù† Ù„Ø§ ÙŠÙ„ØªØ²Ù… Ø¨Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³ ÙŠØµÙÙˆÙ‡ Ø¨ØµÙØ© Ø¨Ø§ØªØª patitØ£ÙŠ Ø§Ù„Ù…Ø±ØªØ¯ØŒ ÙˆÙ…Ù† ÙŠØ¯Ø®Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¬Ø¯ÙŠØ¯ ÙŠØ¬Ø¹Ù„ÙˆÙ‡ ÙŠØªØ¹ÙˆØ¯ Ø¹Ù„ÙŠÙ‡Ø§ ÙˆÙŠØ³Ù…ÙˆÙ‡ Ø§Ù„Ù…ØªÙƒÙŠÙ Ø§Ù„Ø¨Ø·Ø¦.\\nØ§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®.\\nØ§Ù„ØºÙˆØ±Ùˆ ÙƒÙ„Ù…Ø© ØªØ¹Ù†ÙŠ Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆÙ‡Ù… Ø¹Ø´Ø±Ø© ØºÙˆØ±Ùˆ Ù„Ù„Ø³ÙŠØ®ÙŠØ© ÙˆÙ‡Ù…:\\n</td>\n",
              "      <td>\\nÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙŠ Ø´Ù…Ù„ØªÙ‡Ø§ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ®ØŸ</td>\n",
              "      <td>ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§)</td>\n",
              "      <td>Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù‡ÙŠ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1479 Ø¥Ù„Ù‰ 1516. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1516 Ø¥Ù„Ù‰ 1568. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1568 Ø¥Ù„Ù‰ 1700. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1700 Ø¥Ù„Ù‰ 1808. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1808 Ø¥Ù„Ù‰ 1833. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1833 Ø¥Ù„Ù‰ 1868. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1868 Ø¥Ù„Ù‰ 1870. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1870 Ø¥Ù„Ù‰ 1873. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1873 Ø¥Ù„Ù‰ 1874. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1874 Ø¥Ù„Ù‰ 1876. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1876 Ø¥Ù„Ù‰ 1877. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1877 Ø¥Ù„Ù‰ 1878. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1878 Ø¥Ù„Ù‰ 1880. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1880 Ø¥Ù„Ù‰ 1881. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1881 Ø¥Ù„Ù‰ 1882. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1882 Ø¥Ù„Ù‰ 1883. Ùˆ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ (Ù‡Ø§Ù†ØºÙ„: ê·¸ í•´ ìš°ë¦¬ëŠ”)Ø› Ù‡Ùˆ Ù…Ø³Ù„Ø³Ù„ ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ ÙƒÙˆÙ…ÙŠØ¯ÙŠ ÙƒÙˆØ±ÙŠ Ø¬Ù†ÙˆØ¨ÙŠØŒ ÙŠØµÙ†Ù Ø¨Ø£Ù†Ù‡ Â«Ø£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø£ØµÙ„ÙŠ Ù„ÙŠ Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†Â». Ù…Ù† Ø¨Ø·ÙˆÙ„Ø© ÙˆÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠØŒ Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ SBS TV Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021 Ø¥Ù„Ù‰ 25 ÙŠÙ†Ø§ÙŠØ± 2022ØŒ Ø¨Ø« ÙƒÙ„ ÙŠÙˆÙ… Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† ÙˆØ§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¹Ø© 22:00 Ø¨ØªÙˆÙ‚ÙŠØª (KST). ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­Ù‹Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ù†ØªÙÙ„ÙŠÙƒØ³. Ø§Ù„Ù‚ØµØ©. Ø§Ù†ÙØµÙ„ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº (ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ) ÙˆÙŠÙˆÙ† Ø³Ùˆ (ÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠ) Ù‚Ø¨Ù„ 10 Ø³Ù†ÙˆØ§ØªØŒ Ù„ÙƒÙ† ÙŠØµØ¨Ø­ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø°ÙŠ ØµÙˆØ±ÙˆÙ‡ Ø®Ù„Ø§Ù„ Ø£ÙŠØ§Ù… Ø¯Ø±Ø§Ø³ØªÙ‡Ù…Ø§ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø´Ø§Ø¦Ø¹Ù‹Ø§ØŒ Ø¨Ø³Ø¨Ø¨Ù‡ Ø¹Ù„ÙŠÙ‡Ù…Ø§ Ø§Ù„ÙˆÙ‚ÙˆÙ Ø£Ù…Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¬Ø¯Ø¯Ø§ØŒ Ø¹Ù„Ù‰ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ø§ÙŠØ±ÙŠØ¯ÙˆÙ† Ø°Ù„Ùƒ. Ø·Ø§Ù‚Ù…. Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ø±Ø³Ø§Ù… ØªØ´ÙƒÙŠÙ„ÙŠ Ø­Ø± Ù…ÙØ¹Ù… Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ©. Ø®Ø¨ÙŠØ±Ø© Ø¹Ù„Ø§Ù‚Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©. Ù…Ø®Ø±Ø¬ ÙˆØ«Ø§Ø¦Ù‚ÙŠØŒ ÙˆÙ‡Ùˆ ÙƒØ§Ù† Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ù„ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº ÙˆØ§ÙŠÙˆÙ† Ø³Ùˆ. Ø£ÙŠØ¯ÙˆÙ„ Ù…Ø´Ù‡ÙˆØ±Ø© ØªØ­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ Ù…Ù†Ø° Ø¸Ù‡ÙˆØ±Ù‡Ø§. Ø§Ù„Ø¥Ù†ØªØ§Ø¬. ØµÙ†Ø§Ø¹Ø©. ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†ØŒ ÙˆÙ‡ÙŠ Ø£ÙˆÙ„ Ø¯Ø±Ø§Ù…Ø§ Ø£ØµÙ„ÙŠØ© Ù„Ù‡Ù…ØŒ Ø³ÙŠØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ© Ø³ÙˆØ¨Ø± Ù…ÙˆÙ† Ø¨ÙŠÙƒØªØ´Ø±Ø²ØŒ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ Ø§Ù„ØªØ§Ø¨Ø¹ Ù„Ø¥Ø³ Ø¨ÙŠ Ø¥Ø³ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø¨Ù€ Â«Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ø³Â». Ø·Ø§Ù‚Ù…. ÙÙŠ Ù…Ø§Ø±Ø³ 2021ØŒ Ø£Ø¹Ù„Ù† ØªØ£ÙƒÙŠØ¯ Ø§Ù†Ø¶Ù…Ø§Ù… ÙƒÙ„ Ù…Ù† ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ ÙˆÙƒÙŠÙ… Ø¯Ø§ Ù…ÙŠ ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØŒ ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨Ù‡Ù… Ø¨Ø¹Ø¯ Ø«Ù„Ø§Ø« Ø³Ù†ÙˆØ§ØªØŒ Ø¢Ø®Ø± Ù…Ø±Ø© Ø¸Ù‡Ø±ÙˆØ§ ÙÙŠÙ‡ ÙÙŠ Ù…Ø¹Ø§ Ø¨ÙÙŠÙ„Ù… Ø§Ù„ØºÙ…ÙˆØ¶ \"\" Ù„Ø¹Ø§Ù… 2018. ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø§Ù‚Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠ ÙÙŠ 8 ÙŠÙˆÙ„ÙŠÙˆ. Ø§Ù„ØªØµÙˆÙŠØ±. Ø¨Ø¯Ø£ ØªØµÙˆÙŠØ± Ø§Ù„Ù…Ø³Ù„Ø³Ù„ ÙÙŠ Ø£ÙˆØ§Ø¦Ù„ ÙŠÙˆÙ„ÙŠÙˆ 2021. Ø·Ø¨Ø¹Ø© Ø£Ø®Ø±Ù‰. Ø³ÙŠØªÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†ØªØ§Ø¬ ÙˆÙŠØ¨ ØªÙˆÙ† Ù…Ù† Â«ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨Â»ØŒ Ø§Ù„ÙˆÙŠØ¨ ØªÙˆÙ† Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ø´Ø®ØµÙŠØªÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØªÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ (ØªØ´ÙˆÙŠ ÙˆØ§Ù†Øº ÙˆØ§ÙŠÙˆÙ† Ø³ÙˆÙˆ)ØŒ ÙˆÙ…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ÙŠØªÙ… Ø¥ØµØ¯Ø§Ø±Ù‡ Ø¹Ù„Ù‰ Â«Ù†Ø§ÙŠÙØ± ÙˆÙŠØ¨ØªÙˆÙˆÙ†Â» Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù…. Ù‡ÙŠ Ù†Ø§Ø´Ø± ÙˆÙ…Ù†ØµØ© Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨ Ø£Ø·Ù„Ù‚ØªÙ‡ Ø´Ø±ÙƒØ© Ù†Ø§ÙÙŠØ± ÙÙŠ ÙƒÙˆØ±ÙŠØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ© ÙÙŠ Ø¹Ø§Ù… 2004.</td>\n",
              "      <td>Ù…ØªÙ‰ ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\\n</td>\n",
              "      <td>Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021</td>\n",
              "      <td>ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\" ÙÙŠ  2023Ù….</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ù‡Ùˆ Ù…ØµØ·Ù„Ø­ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†ÙŠ ÙˆØ§Ù„Ø¹Ù†ÙŠÙ Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø´Ø£Ù†Ù‡ Ø£Ù† ÙŠØ³Ø¨Ø¨ Ù„Ù‡Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¶Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙŠÙŠÙ† Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ø³Ø¯ÙŠ. Ù‚Ø¯ ÙŠØªÙ… Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†Ù‹ ÙƒØ§Ù† ÙˆØ¹Ù„Ù‰ ÙŠØ¯ÙŠ Ø£ÙŠ Ø´Ø®Øµ ÙÙ‡Ùˆ Ù„Ø§ ÙŠÙ†Ø·ÙˆÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø£Ùˆ Ø³Ø¨Ø¨ ÙˆØ§Ø­Ø¯ ÙˆÙ‡Ùˆ ÙŠØ¹ØªØ¨Ø± Ø¸Ø§Ù‡Ø±Ø© Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± ÙˆÙ…Ù† Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªÙÙ‚Ù„ÙÙ‚ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© ÙˆØªØ¯ÙØ¹Ù‡Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø·Ø±Ù‚ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù…ÙØ³Ù†ÙŠÙ† ÙˆØ­Ù…Ø§ÙŠØªÙ‡Ù… Ù…Ù†Ù‡. ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù… Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø© ÙˆÙ‡Ù… Ø£Ø´Ø®Ø§Øµ ÙŠØ­ØµÙ„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù…ØªÙŠØ§Ø²Ø§Øª ÙˆØ­Ù…Ø§ÙŠØ© Ù…ÙƒØ«ÙØ© ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆØ°Ù„Ùƒ Ø¨Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø±Ù‚Ù… 26 Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆÙ‡Ùˆ Ù‚Ø§Ù†ÙˆÙ† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§ØµØ±ÙŠÙ† ÙˆØ§Ù„Ø¹Ø§Ø¬Ø²ÙŠÙ† ÙƒÙ…Ø§ ÙˆÙŠØ¹ÙˆØ¯ Ø°Ù„Ùƒ Ù„Ø¶Ø¹ÙÙ‡Ù… ÙˆÙ„ÙƒØ«Ø±Ø© Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡Ù… ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ§Ø¬Ù‡Ù‡Ø§ Ù‚Ø³Ù… ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ù…. ÙÙŠ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙŠØªÙ… Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" ÙˆØ¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø¥Ø³Ø§Ø¡Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ø­ÙŠØ« Ø£Ù†Ù‡ ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø³Ø§Ø¡Ø© Ù„ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù‚Ø¯ ØªØ¶Ù…Ù† Ø§Ù„Ø¹Ù†Ù Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªØ­Ø¯Ø« Ù…Ù† Ø´Ø®Øµ Ù…Ù‚Ø±Ø¨ Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø³Ù† Ù…Ø«Ù„ Ø§Ù„Ø£Ù‚Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø®ØµØµÙŠÙ† Ù„Ø±Ø¹Ø§ÙŠØªÙ‡Ù… ÙˆØªØ³ØªÙ…Ø± Ù„ÙØªØ±Ø© ÙˆÙ‡Ùˆ Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø§Ø¯Ø« Ø§Ù„Ø°ÙŠ ÙÙŠÙ‡ ÙŠØªÙ… Ø§Ù„ØªÙ‡Ø¬Ù… Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙƒØ¨ÙŠØ± Ø¨Ø§Ù„Ø³Ù† Ù…Ù† Ù‚Ø¨Ù„ Ø´Ø®Øµ ØºØ±ÙŠØ¨ ÙˆÙŠØ­Ø¯Ø« Ø°Ù„Ùƒ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙŠÙƒÙˆÙ† Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ Ø§Ù„Ø´Ø±Ø·Ø©. Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ± Ø£Ù† Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ØªÙ‚ÙˆÙ… Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ ÙƒÙ„ØªØ§ Ø§Ù„Ø­Ø§Ù„ØªÙŠÙ†ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ø¥Ø³Ø§Ø¡Ø© Ø§Ùˆ Ø¹Ù†ÙØŒ Ø¨Ù‡Ø¯Ù Ø§Ù„Ø­Ø¯ Ù…Ù† Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø­Ø³Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø°ÙŠ Ø£ØµØ¯Ø±ØªÙ‡ Ø§Ù„ÙƒÙ†ÙŠØ³Øª Ø¹Ø§Ù… 2007 ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¹Ø±Ø¶ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙØ£Ù†Ù‡ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø£Ø¬Ø±ÙŠØª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø³Ù†Ø© 2006 Ø¹Ø§Ø´ Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ Ø§Ù„ 70ØŒ000 Ù…ÙØ³Ù† ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ø§Ø¯Ù„ 10% Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³ÙƒØ§Ù†ØŒ 30% Ù…Ù†Ù‡Ù… Ù‡Ù… Ù†Ø§Ø¬ÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ø­Ø±Ù‚Ø© Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ© Ø§Ùˆ Ø§Ù„Ù‡ÙˆÙ„ÙˆÙƒÙˆØ³Øª ÙƒÙ…Ø§ Ø§Ù† Ø­ÙˆØ§Ù„ÙŠ 12ØŒ000 Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙŠØ¹ÙŠØ´ÙˆÙ† Ù…Ø¹ Ø´Ø®Øµ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø§ ØªØ±Ø¨Ø·Ù‡ Ù…Ø¹Ù‡Ù… ØµÙ„Ø© Ø¯Ù… (Ø¹Ø§Ø¯Ø© Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ùˆ Ù…Ø±Ø§ÙÙ‚ Ù„Ù„Ø§Ø¹ØªÙ†Ø§Ø¡ Ø¨Ù‡Ù…). Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø´Ø±Ø·Ø© ØªØªÙ„Ù‚Ù‰ Ø¹Ø¯Ø¯ Ù‡Ø§Ø¦Ù„ Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª ÙˆÙØªØ­Øª Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 12,228 Ù…Ù„Ù ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¹Ù†Ù ÙˆØ§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù…Ù† Ø¹Ø§Ù… 2002 Ø­ØªÙ‰ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù…Ù† Ø³Ø¨ØªÙ…Ø¨Ø± Ø¹Ø§Ù… 2007 (Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ØªØªØ¶Ù…Ù† Ø§Ù„Ù‚ØªÙ„ØŒ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù‚ØªÙ„ØŒ Ø§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ÙˆØ§Ù„Ø§ØºØªØµØ§Ø¨ØŒ Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø³Ø±Ù‚Ø© ÙˆØ§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠÙˆØª ÙƒÙ…Ø§ ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ ÙˆØ§Ù„ØªØ®Ø±ÙŠØ¨) Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙŠ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ù„Ø§ ÙŠØªØ¶Ù…Ù† Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙˆØµÙ„ÙˆØ§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø±Ø³ Ø§Ù„Ù…Ø¯Ù†ÙŠ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ù‚Ø³Ø§Ù…. Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ÙØ¥Ù†: 32% Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ù‡ÙŠ Ø¨Ù„Ø§ØºØ§Øª Ø¹Ù† Ø§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø¨ÙŠÙˆØªÙ‡Ù…ØŒ 17% Ù‡ÙŠ Ø¹Ù† ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ØªÙ„Ù‚ÙˆÙ‡Ø§ØŒ 31% Ø¹Ù† Ø³Ø±Ù‚Ø§Øª ØªØ¹Ø±Ø¶ÙˆØ§ Ù„Ù‡Ø§ Ø¨ÙŠÙ†Ù…Ø§ 11% Ù‡ÙŠ ÙÙ‚Ø· Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ø³Ø¯ÙŠØ©. Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ø³Ù†ÙˆØ§Øª (2002-2007) ÙƒØ§Ù† ÙŠÙ‚Ø¯Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø§Ù„Ù‚Ø§Ø¯Ù…ÙˆÙ† Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø¯ÙˆÙ„Ø© (Ø§Ù„Ù…Ù‡Ø§Ø¬Ø±ÙˆÙ†) Ø­ÙˆØ§Ù„ÙŠ 9% Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ø±ØºÙ… Ø£Ù† Ù†Ø³Ø¨ØªÙ‡Ù… Ù…Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ù„Ø§ ØªØªØ¹Ø¯Ù‰ 25% Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø³Ù†ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙƒØ§Ù†ÙˆØ§ ÙŠÙ‚Ø¯Ù…ÙˆÙ† 5% Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª. ÙˆÙÙ‚Ù‹Ø§ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø¥Ù† Ù„Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ Ø£Ø«Ø±Ù‹Ø§ Ø³Ù„Ø¨ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙ‚Ø¯ Ù„ÙˆØ­Ø¸ Ø§Ø±ØªÙØ§Ø¹ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ÙÙŠÙ‡Ø§ ØªØ¹Ù†ÙŠÙ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ØªÙ…Øª Ù…Ù† Ù‚Ø¨Ù„ Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙˆÙ…Ù‚Ø¯Ù…ÙŠ Ø§Ù„Ø±Ø¹Ø§ÙŠØ©) ÙØ¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø¹Ø§Ù… 2018 ØªÙ… Ø¹Ù„Ø§Ø¬ 6707 Ø­Ø§Ù„Ø© Ø¨ÙŠÙ†Ù…Ø§ Ø­ØªÙ‰ Ù…Ù†ØªØµÙ Ø¹Ø§Ù… 2019 ÙÙ‚Ø· ØªÙ… Ø¹Ù„Ø§Ø¬ Ø£ÙƒØ«Ø± Ù…Ù† 7000 Ø­Ø§Ù„Ø© ÙˆÙ‡Ø°Ø§ Ø¹Ø¯Ø§ Ø¹Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ</td>\n",
              "      <td>Ù…Ù† Ù‡Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ØŸ\\n\\n</td>\n",
              "      <td>Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø©</td>\n",
              "      <td>ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù…Ø§ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ØªØªØ±Ø§ÙˆØ­ Ø£Ø¹Ù…Ø§Ø±Ù‡Ù… Ø¨ÙŠÙ† 65 Ùˆ 100 Ø¹Ø§Ù….</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯. ÙŠØ¹Ø¯ Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„ØªÙƒÙ„ÙŠÙ Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø·Ø§Ù‚Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù…Ù„ Ø§Ù„ÙÙ‚Ø¯ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ Ù„Ù„ØªØ­Ù…Ù„ ÙˆØ§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø¸Ø±ÙˆÙ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©.</td>\n",
              "      <td>Ù…Ø§ Ù‡ÙŠ Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n</td>\n",
              "      <td>Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯</td>\n",
              "      <td>Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø·Ø§Ù‚Ø© ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ø£Ùˆ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©ØŒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©. ÙŠØ¹ØªØ¨Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù†Ø¸Ø§Ù…Ù‹Ø§ Ù…ØªÙ‚Ø¯Ù…Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø§Ù‚Ø©ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ²ÙŠØ¯ Ù…Ù† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ ØªØ®ØµØµ ÙŠÙ‡ØªÙ… Ø¨Ø¯Ø±Ø§Ø³Ø© ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©.\\nØ£ØµØ¨Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø±ÙˆÙØ§Ù‹ ÙÙŠ Ø£ÙˆØ§Ø®Ø± Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø± ÙˆØ°Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙ„ØºØ±Ø§Ù ÙˆÙ…Ø­Ø·Ø§Øª Ø¥Ù…Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù‚Ø©. ÙˆØ§Ù„Ø¢Ù† ÙŠØºØ·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¹Ø¯Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ÙØ±Ø¹ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ†Ø¸Ù… Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù„Ø§Ø³Ù„ÙƒÙŠØ©.\\nÙˆÙ…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£Ù† Ù†Ù‚ÙˆÙ„ Ø£Ù† Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‚Ø¯ ØªØªØ¶Ù…Ù† Ø£ÙŠØ¶Ø§Ù‹ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ‚Ø¯ Ù„Ø§ ØªØªØ¶Ù…Ù†Ù‡Ø§. ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ†Ù‡Ù…Ø§ Ø­ÙŠØ« ØªÙ‡ØªÙ… Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¨Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬Ù‡Ø¯ Ù…Ø«Ù„ Ù†Ù‚Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ¹Ø§Ù…Ù„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù…Ø¹ Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ù†Ø¸Ù… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (ØªÙŠØ§Ø± Ù…Ù†Ø®ÙØ¶ â€“Ø¬Ù‡Ø¯ Ù…Ù†Ø®ÙØ¶)ØŒ ÙˆÙŠØªØ¶Ù…Ù† Ø°Ù„Ùƒ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„Ø©.\\nÙˆØªØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¯Ø±Ø§Ø³Ø© ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø§ØªØŒ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§ØªØŒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù‚Ø¯Ø±Ø© ØºÙŠØ± Ø§Ù„Ù…Ù†Ù‚Ø·Ø¹Ø© UPSØŒ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆÙ…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©.\\nØªØ§Ø±ÙŠØ® ÙˆØ£Ø¹Ù„Ø§Ù… Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©.\\nØ¸Ù‡Ø± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø¨Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…Ù†Ø° Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø¹Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„. ÙÙŠØ¹ØªÙ‚Ø¯ Ø£Ù† Ø£ÙˆÙ„ Ù…Ù‡Ù†Ø¯Ø³ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù‡Ùˆ ÙˆÙ„ÙŠØ§Ù… Ø¬Ù„Ø¨Ø±Øª Ø§Ù„Ø°ÙŠ ØµÙ…Ù… Ø¢Ù„Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø°Ø§Øª Ø§Ù„Ø´Ø­Ù†Ø§Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©. ÙˆÙ‡Ùˆ Ù…Ù† ÙØ±ÙŽÙ‘Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©ØŒ ÙƒÙ…Ø§ ÙŠØ¹ØªÙ‚Ø¯ Ø¨Ø£Ù†Ù‡ Ø£ÙˆÙ„ Ù…Ù† Ø£Ù†Ø´Ø£ Ù…ØµØ·Ù„Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.\\nÙˆÙÙŠ Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ù…Ø± ÙƒØ§Ù†Øª ÙƒÙ„ Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª ÙˆØ§Ù„Ø§Ø®ØªØ±Ø§Ø¹Ø§Øª ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø´Ø­Ù†Ø©. ÙˆØ¨Ø¯Ø£ ÙØµÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¹Ù† Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ ÙÙŠ Ø²Ù…Ù† ØªÙˆÙ…Ø§Ø³ Ø§Ø¯ÙŠØ³ÙˆÙ† ÙˆÙÙŠØ±Ù†Ø± ÙÙˆÙ† Ø³ÙŠÙ…Ù†Ø³. ÙˆÙÙŠ Ø¹Ø§Ù… 1752 Ø§Ø®ØªØ±Ø¹ Ø¨ÙŠÙ†ÙŠØ§Ù…ÙŠÙ† ÙØ±Ø§Ù†ÙƒÙ„ÙŠÙ† Ù…ÙˆØµÙ„Ø© Ø§Ù„ØµÙˆØ§Ø¹Ù‚ ÙˆÙ†Ø´Ø± Ø¨ÙŠÙ† 1751 Ùˆ1753 Ù†ØªØ§Ø¦Ø¬ ØªØ¬Ø§Ø±Ø¨Ù‡ ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† Â«ØªØ¬Ø§Ø±Ø¨ ÙˆÙ…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡Â» (Experiments and Observations on Electricity). ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1800 Ù‚Ø§Ù… Ø£Ù„Ø³Ø§Ù†Ø¯Ø±Ùˆ ÙÙˆÙ„ØªØ§ Ø¨Ø¨Ù†Ø§Ø¡ Ø¨Ø·Ø§Ø±ÙŠØªÙ‡ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù…Ø³Ù…Ø§Ø© Â«Ø¹Ù…ÙˆØ¯ ÙÙˆÙ„ØªØ§Â» Ø¨Ø¹Ø¯ Ø§Ø¹Ø¬Ø§Ø¨Ù‡ Ø¨ØªØ¬Ø±Ø¨Ø© Ø§Ø¬Ø±Ø§Ù‡Ø§ Ù„ÙˆÙŠØ¬ÙŠ Ø¬Ø§Ù„ÙØ§Ù†ÙŠ Ø¹Ø§Ù… 1792. ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1820 Ù‚Ø§Ù… Ù‡Ø§Ù†Ø² ÙƒØ±ÙŠØ³ØªÙŠØ§Ù† Ø§ÙˆØ±Ø³ØªØ¯ Ø¨Ø¹Ù…Ù„ ØªØ¬Ø§Ø±Ø¨ Ø¹Ù† Ø§Ù†Ø­Ù†Ø§Ø¡ Ø§Ø¨Ø±Ø© Ø§Ù„Ø¨ÙˆØµÙ„Ø© Ø¨ØªØ§Ø«ÙŠØ± Ø§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ. ÙˆÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¹Ø§Ù… ÙƒØ±Ø± Ø§Ù†Ø¯Ø±ÙŠÙ‡ Ù…Ø§Ø±ÙŠ Ø§Ù…Ø¨ÙŠØ± ØªÙ„Ùƒ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ø«Ø¨Øª Ø§Ù† Ø³Ù„ÙƒÙŠÙ† ÙŠÙ…Ø± ÙÙŠÙ‡Ù…Ø§ Ø§Ù„ØªÙŠØ§Ø± ÙŠØ¤Ø«Ø±Ø§Ù† Ø¨Ù‚ÙˆÙ‰ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶ ÙˆØ¹Ø±Ù Ø®Ù„Ø§Ù„Ù‡Ø§ Ø§Ù„Ø¬Ù‡Ø¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ.\\nÙ…Ø§ÙŠÙƒÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ (ÙŠÙ†Ø·Ù‚ Ø£ÙŠØ¶Ø§ Ù…ÙŠØ´ÙŠÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ) Ù‚Ø¯Ù… Ø£Ø¹Ù…Ø§Ù„ ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙÙŠØ¶ÙŠÙ† Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØŒ ÙˆØ¹Ø±Ù Ø£ÙŠØ¶Ø§ Ø®Ø·ÙˆØ· Ø§Ù„Ù…Ø¬Ø§Ù„. ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø§Ù„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ Ù‚Ø¯Ù… Ø¬ÙŠÙ…Ø³ ÙƒÙ„ÙŠØ±Ùƒ Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ Ø£Ø¹Ù…Ø§Ù„Ø§ ÙÙŠ Ø¥ÙƒÙ…Ø§Ù„ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆØ¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©Ù€ ÙˆÙ‚Ø¯Ù… Ø¹Ø§Ù… 1864 Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ ÙˆØ§Ù„ØªÙŠ ØªØ¹ØªØ¨Ø± Ø£Ø­Ø¯ Ø£Ù‡Ù… Ø£Ø³Ø³ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ©.\\nØªØ·ÙˆØ± Ù…Ø¬Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¶ÙˆØ¡ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø£Ø¯Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø°ÙˆÙŠ ØªØ¬Ø±Ø¨Ø©. ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙØªØ±Ø© ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ù‡Ù„Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ®ØµØµØ§Øª Ø¨Ø£Ø·Ø± Ø£Ø¹Ù…Ø§Ù„ Ù‡Ù†Ø¯Ø³ÙŠØ©. Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù‡Ù„Øª Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø¨Ù†Ø§Ø¡ ÙˆÙ…Ù‡Ù†Ø¯Ø³Ùˆ Ù…Ø§ÙƒÙŠÙ†Ø§Øª Ù„ÙƒÙ†Ù‡Ø§ ÙØ´Ù„Øª Ø¨Ø§Ù† ØªØ¤Ù‡Ù„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ù„Ù‰ Ø¶ÙˆØ¡ ØªØ³Ø§Ø±Ø¹ Ø§Ù„Ù…Ø¬Ø§Ù„ØŒ ÙˆÙ„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ù‡Ù†Ø¯Ø³Ùˆ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙŠØ¯Ø±Ø³ÙˆÙ† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù„Ø·Ù„Ø§Ø¨ Ù‡Ù†Ø¯Ø³Ù‡ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.Â \\nØ¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ù…Ù‡Ù…Ø© ØªØ¹Ù„ÙŠÙ… ÙˆØªØ£Ù‡ÙŠÙ„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆÙ‚Ø¹Øª Ø¹Ù„Ù‰ Ø¹Ø§ØªÙ‚ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠÙ†ØŒ Ø­ÙŠØ« Ø§Ù†Ù‡ Ù„Ù… ÙŠÙƒÙ† Ù„Ø§Ø­Ø¯ Ø§Ø®Ø± Ø§Ù„Ø¹Ù„Ù… Ø¨Ù…Ø¬Ø§Ù„ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙˆØ§Ù„Ù‚Ø¯Ø±Ø© Ù„ØªÙˆØµÙŠÙ„Ù‡ Ù„Ø§ÙƒØ¨Ø± Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨.\\n</td>\n",
              "      <td>Ù…Ø§ Ù‡Ùˆ Ø¯ÙˆØ± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n</td>\n",
              "      <td>.</td>\n",
              "      <td>Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù‡ÙŠ Ø£Ø¯ÙˆØ§Øª ØªØªØ¨Ø¹ ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØª</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3815533-d54b-4ee9-bc33-d0bce89f26eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3815533-d54b-4ee9-bc33-d0bce89f26eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3815533-d54b-4ee9-bc33-d0bce89f26eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "completed_n = len(model_answers)  # should be 328\n",
        "df_sample = df_sample.reset_index(drop=True)\n",
        "\n",
        "# Attach the answers ONLY to the first completed_n rows\n",
        "df_sample.loc[:completed_n-1, \"model_answer\"] = model_answers\n",
        "\n",
        "# Keep only rows that actually have a model answer (not NaN / empty)\n",
        "df_done = df_sample.iloc[:completed_n].copy()\n",
        "df_done = df_done[df_done[\"model_answer\"] != \"\"]\n",
        "\n",
        "print(df_done.shape)\n",
        "df_done.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7acc35e0-59ee-4816-98d5-390375af2e14",
      "metadata": {
        "id": "7acc35e0-59ee-4816-98d5-390375af2e14"
      },
      "outputs": [],
      "source": [
        "df_done.to_csv(\"df_sample_with_answers_328.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5037fbbc-59b2-4168-829d-e790bb7b72ee",
      "metadata": {
        "id": "5037fbbc-59b2-4168-829d-e790bb7b72ee",
        "outputId": "91c3a0b6-7c13-47e0-bff4-28e135baf7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://x3td0ccbc9n6-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n"
          ]
        }
      ],
      "source": [
        "print(session.url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7753a1e4-ede6-4c45-952b-fce34e2d53b6",
      "metadata": {
        "id": "7753a1e4-ede6-4c45-952b-fce34e2d53b6"
      },
      "source": [
        "## RAGAS Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "00d9090e-5d13-4c84-ba6a-898ec1c4465a",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "00d9090e-5d13-4c84-ba6a-898ec1c4465a",
        "outputId": "3b666e4a-7df4-4af5-baaf-181e92047bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.5)\n",
            "Requirement already satisfied: ragas[langchain] in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "\u001b[33mWARNING: ragas 0.4.0 does not provide the extra 'langchain'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy<3.0.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (2.0.2)\n",
            "Requirement already satisfied: datasets>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (4.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.12.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.4.4)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (5.6.3)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.20.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (13.9.4)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (4.67.1)\n",
            "Requirement already satisfied: instructor in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.13.0)\n",
            "Requirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (11.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (3.6)\n",
            "Requirement already satisfied: scikit-network in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.33.5)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (0.4.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (from ragas[langchain]) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.47)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (2.32.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->ragas[langchain]) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->ragas[langchain]) (0.36.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.27.2)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas[langchain]) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas[langchain]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas[langchain]) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas[langchain]) (1.3.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (3.13.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (0.17.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (3.1.6)\n",
            "Requirement already satisfied: pre-commit>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (4.5.0)\n",
            "Requirement already satisfied: ty>=0.0.1a23 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas[langchain]) (0.0.1a31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas[langchain]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas[langchain]) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas[langchain]) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas[langchain]) (1.5.4)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (2.0.44)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas[langchain]) (0.4.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->ragas[langchain]) (2025.11.3)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from scikit-network->ragas[langchain]) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas[langchain]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas[langchain]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas[langchain]) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas[langchain]) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas[langchain]) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas[langchain]) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas[langchain]) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->ragas[langchain]) (3.11)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas[langchain]) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas[langchain]) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=4.0.0->ragas[langchain]) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas[langchain]) (3.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community->ragas[langchain]) (1.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ragas[langchain]) (0.1.2)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (3.5.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (2.6.15)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (1.9.1)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor->ragas[langchain]) (20.35.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->ragas[langchain]) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->ragas[langchain]) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->ragas[langchain]) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->ragas[langchain]) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->ragas[langchain]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->ragas[langchain]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->ragas[langchain]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0.0->ragas[langchain]) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas[langchain]) (1.1.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas[langchain]) (0.4.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas[langchain]) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"ragas[langchain]\" langchain-core langchain pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2099e725-0fd5-47cc-8a3b-f786a49c74f3",
      "metadata": {
        "id": "2099e725-0fd5-47cc-8a3b-f786a49c74f3",
        "outputId": "5fa515d6-7849-48d3-83a4-166265b3cd77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-843810481.py:5: DeprecationWarning: LlamaIndexLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
            "  evaluator_llm = LlamaIndexLLMWrapper(llm)\n"
          ]
        }
      ],
      "source": [
        "from ragas.llms import LlamaIndexLLMWrapper\n",
        "\n",
        "# you already defined `llm = HuggingFaceLLM(...)` and `Settings.llm = llm`\n",
        "\n",
        "evaluator_llm = LlamaIndexLLMWrapper(llm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "97ef2602-0f8b-48e3-8f29-0e205124811d",
      "metadata": {
        "id": "97ef2602-0f8b-48e3-8f29-0e205124811d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "451bf55ca4f44f14a63b4c5c380388e7",
            "8a7787eb416846eda265d65e60fae4c7",
            "f94900f5bcbf486b802473c833732036",
            "ba96c6d95ef34a359fe3f4ca254a4c58",
            "2c0861cfc19d40649dbb4ff7cb39fd57",
            "bed7628b710f4bb2a65c1548c603ffb8",
            "c3215918da924f3f993cf85d79242df4",
            "640531c22fac4d3ca23785a8933ed972",
            "5dc80644302747238e95fb80548f0443",
            "e68aba7ae201405195ae2f01d6ad3ced",
            "29698dba001342ea801ef886ada98be3",
            "d930f62003d6487682c461d0b17a587c",
            "b3fc01e468b743929f3478989a2de785",
            "15cb215c5ed04759aedfe7b5f1246e95",
            "0244a5508c62460fb4ba80f304e0ab2f",
            "b710d46644df4b4e8aaf3850564739a2",
            "02d5ec36e08743868a287c90a98233dd",
            "ca3288b1a2a5486a8970cb21d6e484ab",
            "96615fe238ce48188bd773f7f6382ffb",
            "e65329ca0f39480f9968a1517fd6b5bc",
            "825d0691c1b34a1f9ec83977b1bcdd19",
            "3045e6c39d264ecabf79ca21814604ff",
            "5940eadc90194d59aa81c1d18a1c75f7",
            "f66647483db843bb889e9ed37b164325",
            "7da20044020c4b79bc6e6c137aae7733",
            "b74ea58c2b294e8096758210aa785210",
            "145b8909aad44d1ea182fbc8cd870c72",
            "e273f91657a845749101f79b392925d4",
            "f1e903ef0fb94642bac05c907995f9ce",
            "082d211f3fc247debd7b5bbfd37a38ba",
            "74890f11b62a4b8c91fabda11179cfe5",
            "68c91b454b164fd79dd468376bdc2e74",
            "5748d35243164012aad722b758d3f5e0",
            "6e7e0ce1f2d9474990e9e79d8e6e0599",
            "e452e8bae2a6401f9461bfd28443b59f",
            "b9184984b61943739daacef2f5f66e09",
            "9e854199f12946339cf5c63860b0c32c",
            "60ff22f8c6c84adcb3a2ca65e4fc9c64",
            "5f4baeade8c843089039332c2e1b9871",
            "47db75e67b054a7c9c6b0be57d5d4646",
            "68887d8ea6e7460c88beb7573d993685",
            "99f206bf17ea4c8984cec076a2c48f7c",
            "414739cb67e9428da3e1881d6d5890c8",
            "943774a0d58449edaf1caa5f020614a8",
            "027b6c839fad460289acfcbf18a1fc2d",
            "8647fb0b68f343cbb2a1edbe756e8842",
            "a8b841315bca4cb5aa502f8159e9291b",
            "56ded534d55f41db8489304da45f0f73",
            "18c9ade6853e4a198dc9da7632c2ef02",
            "f336339c465947ffb1c5606ca3298dc0",
            "8f10c786e9b444ce8b3e595558d02654",
            "9bac8823a55447dda45efc2d7b505b0c",
            "275dc9e9f2b04bcfa58ac10ad489b021",
            "21332b1b2b4a4bceb1402324b7d3db8c",
            "341491e5ccb54632aa59a24f88e65ccb",
            "956eb04c5dc14961a70d4c771433b7a0",
            "7c0f4031a2134fa5b17381abe7920ddc",
            "e7c7b6d02a2e434aa461f296c6aa8fba",
            "2728eda4af5f4e50bdf8f05c7cbcbc46",
            "ea26d1fe3c3143f98c3b5e4ff5f9875a",
            "ac07ac748e6d46f781f3c6bb0a0d047c",
            "196680bca63945dc9b1cfb1cfd37f8fa",
            "74bb9607d5aa45259a98a29cc7bf486a",
            "c222916d93bd48b6a1ce12a4c425be74",
            "0fe84526557f4a7bb863b912e183d0ab",
            "d8ba76c6050a4dbeb3809eda7e1a94f3",
            "e8d99e8bd21f4ef4800db523f81e4035",
            "dd4700a901184174801d6d3df88fbaab",
            "c0d198a7bfba466097bf5c9b0049036f",
            "210468fd35e142a38cf6eb384bba2f04",
            "9b6f07d3b7ce48428083fc0cfef6b03d",
            "27abb6beddcc41be9bb3b945e4cb033a",
            "2ec626a94ae3481ea9daaea00ef9d01c",
            "1f2a90e6f8fd42ce9b85e9bc87c5a85d",
            "480082e3c2a74b999649bd41c5fc8391",
            "83927c966882465791371d6cee8701cc",
            "058ff631ef3c4b52935aa32043896547",
            "443755b2a613411b9aa12ece661866e7",
            "6f705aca58f044ceaf665ef4c912f45f",
            "fbeda64cce684010ae720e14ca69db45",
            "4e8683bf43ab41d2a6da69fe48ae4c1b",
            "5b9eacd80d2347198bc31650456b0aa1",
            "5935a527a79d4c24989e2a66e4087928",
            "a79eaf0b74be48c597951668d3de1b7b",
            "8c16edec7a564af6922a3167acfd961c",
            "3bf826bc07d94ee0a7ac7164ede650b4",
            "9d1a112c75424537bd0da8d101013c88",
            "adfe21709c5b45ed8db53278429ea85d",
            "a8805b42db7846619c955aefe9ccdb4b",
            "1e12df703270461ab5a06b347750ac94",
            "09a8be294d1741a6a0a5b67c48e2f55d",
            "eb5c56b0969547198b91c13dc80a44cc",
            "4d48470bbc394fd6b68d0854153de703",
            "b20918ee48c44daea1d6dc7cf4daccef",
            "387298101df14924a5c926a9431b995e",
            "1500d7cff8ed4a6681e66a982cc39082",
            "d585a9e8211e40d98e6e5fdcc08664c2",
            "6a1c7cc2d28b480bbe5732bc2af705b0",
            "6fc9ba7171a741c8ac57c19fb64b0edd",
            "b2a810cacbe646d988592242c1fce30c",
            "a5e4ca98b7d94e9c89e4e7027adb57aa",
            "f4f955e6dd8c42c28e7de7f1a1b441ba",
            "2f45bfcbddc3415596f31d39f8455fe9",
            "9592ad33bca44acf80ff32a8687fe52f",
            "e1841a07954c4bcca4f3e843112bc39a",
            "89ec0ae7b78d4d6c80be8d11169041ec",
            "99d0b465e79c4ead80e46aacbb1c0dd0",
            "313d20956cfd4cbebba59ecf49736d56",
            "0eb8768695b54a6d881f675948cb212d",
            "03b2d41ba8c640e993675b3359afc0e9",
            "8fd0d87b14334731828df95932b2655b",
            "8bf7de3e296642e0a132a2bea24d3aa9",
            "ab0041c1a9d14d28ac90b705405e743f",
            "46590fd4bc384d10a28b68ef2bf24427",
            "dee3374f15744cf39e2d8c5357b9cc2f",
            "e808543f907c427a9dd66780057f7a07",
            "65f7d3b800114779a5cb0a87ede886ae",
            "ac4eb90b009742068bd3109c406a2a3b",
            "37188d5bbd6548eabdd8de9447c27300",
            "e2ed533a0c5e47c2bd3583423be8634d",
            "03408157efe045b88e4501bd1b5656bb",
            "5d8134afd6bd4dc49fdeb6b479af6e40",
            "e06599aebcde428ea19fa7a06a8b2bd3",
            "d915397cbc1f4a8385c963fa93b86d51",
            "46870b53139b44ccb2468d76bb137ebe",
            "6a809be34f5a435dbc44daeed22c0f96",
            "675040c1ecf14c7ab5810abf56fd8487",
            "26ac0b1c5d7a46f5bd9d78d122cadcb6",
            "8ccda064d07d4a49b7bea0d3545afcfa",
            "b5d81c15e1db4d2c873658499ee05ea6",
            "5f2501083cbf456683d3493b0823157c",
            "41a2a02d63eb402c960b663365f60997",
            "04343964cec64660aee32ecae9dd0629",
            "87e1d29a34bb46a8968304b2429fb50f",
            "f9290de12fd24320bda378b0ac970165",
            "9e996fd531684e61aeb6162a9c27b9ed",
            "a88d13392508486b8248db630819d55d",
            "928354ac84b24198af3db60c68cc5e90",
            "14672e3c8b2c4e40ab2ee009739bb5e2",
            "b8521950be3c4a2b96cbe9522bc21f34",
            "215d91dbf7c941a1940d1c9d65b9d724",
            "779b943511c0486fa82f1bf54b7cdc9d",
            "704acf7381fe4ba4ab791a684d140c3c"
          ]
        },
        "outputId": "a8c362d2-fff7-4bad-b425-607aa7abfcdf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "451bf55ca4f44f14a63b4c5c380388e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d930f62003d6487682c461d0b17a587c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5940eadc90194d59aa81c1d18a1c75f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e7e0ce1f2d9474990e9e79d8e6e0599"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "027b6c839fad460289acfcbf18a1fc2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "956eb04c5dc14961a70d4c771433b7a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8d99e8bd21f4ef4800db523f81e4035"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "443755b2a613411b9aa12ece661866e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8805b42db7846619c955aefe9ccdb4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2a810cacbe646d988592242c1fce30c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fd0d87b14334731828df95932b2655b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d8134afd6bd4dc49fdeb6b479af6e40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2_Dense/model.safetensors:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04343964cec64660aee32ecae9dd0629"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\n",
        "    \"sentence-transformers/LaBSE\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e02bb965-1393-44f6-a7ca-128d21ba4b90",
      "metadata": {
        "id": "e02bb965-1393-44f6-a7ca-128d21ba4b90",
        "outputId": "0cb422d5-80d7-4d1b-c2e3-51ec9c7c29f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3569151.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/LaBSE\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3096901c-2f88-4723-9a45-209339839aa3",
      "metadata": {
        "id": "3096901c-2f88-4723-9a45-209339839aa3",
        "outputId": "87b84476-2638-41e3-aacc-4fa843d51eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['context', 'question', 'ground_truth', 'model_answer',\n",
            "       'retrieved_contexts', 'response', 'user_input', 'reference'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Starting from df_sample with: context, question, ground_truth, model_answer\n",
        "df_ragas = df_done.copy()\n",
        "\n",
        "# Rename columns to match Ragas expectations\n",
        "df_ragas[\"retrieved_contexts\"] = df_ragas[\"context\"].apply(lambda x: [x])  # must be list of strings\n",
        "df_ragas[\"response\"] = df_ragas[\"model_answer\"]\n",
        "df_ragas[\"user_input\"] = df_ragas[\"question\"]\n",
        "df_ragas[\"reference\"] = df_ragas[\"ground_truth\"]  # rename ground_truth to reference\n",
        "\n",
        "print(df_ragas.columns)\n",
        "# Should include: 'user_input', 'response', 'retrieved_contexts', 'reference'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "09c68957-b6ce-4636-990d-6fe84acb1102",
      "metadata": {
        "id": "09c68957-b6ce-4636-990d-6fe84acb1102"
      },
      "outputs": [],
      "source": [
        "\n",
        "from ragas import EvaluationDataset\n",
        "ragas_dataset = EvaluationDataset.from_pandas(df_ragas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "21d9be93-97e7-46cc-a21c-1d54dccad37a",
      "metadata": {
        "scrolled": true,
        "id": "21d9be93-97e7-46cc-a21c-1d54dccad37a",
        "outputId": "a13c5177-2f3d-4480-dd56-d8dbe7eae620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b0c50c07d0249dc81d40df2c04785f9",
            "750713b5acab407eb1e83ea7500740ed",
            "bbfccdef77614bffb5caf16eda556fc1",
            "daa085de0a2e4991a866bbded3956880",
            "7965815e6ec94afab00f887e6bcbeb29",
            "586e84451055479cb7b88e7705d13f03",
            "710acdc9ae3a4e958dbee4b8b5a0e3d6",
            "2a5e5667057e46ed842736508ebb4bd9",
            "b4e4e2091a3946788a81b028eda7d1da",
            "e05ed0c372a24e45add72e3aab66b4a2",
            "65e5cac368eb4798aa81e452770e5ded"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b0c50c07d0249dc81d40df2c04785f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[696]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[697]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[701]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[664]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[669]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[702]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[705]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[706]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[707]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[708]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[710]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[711]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[674]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[679]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[712]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[716]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[718]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[721]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[684]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[689]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[722]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[727]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[728]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[730]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[731]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[694]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[699]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[732]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[733]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[735]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[736]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[704]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[709]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[748]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[750]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[714]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[719]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[753]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[756]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[758]: OutputParserException(Invalid json output: What can you tell me about albert Albert Einstein? Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass-energy equivalence formula E = mc2, which arises from relativity theory, has been called 'the world's most famous equation'. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius. Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[760]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[724]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[729]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[762]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[763]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[764]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[765]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[766]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[770]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[771]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[734]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[772]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[773]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[739]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[776]: OutputParserException(Invalid json output: Where was Albert Einstein born?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[777]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[778]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.\",\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[780]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[744]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[749]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[782]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[783]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[787]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[790]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[791]: OutputParserException(Failed to parse StringIO from completion 26. Got: 1 validation error for StringIO\n",
            "  Input should be a valid dictionary or instance of StringIO [type=model_type, input_value=26, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/model_type\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[754]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[759]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[795]: OutputParserException(Invalid json output: Sheikh Imam was educated by his father. Sheikh Imam was educated in mathematics by his father. Sheikh Imam was educated in languages by his father. Sheikh Imam was educated in arts by his father. Sheikh Imam was educated in education by his father. Sheikh Imam was educated in the arts by his father. Sheikh Imam was educated in the field of education by his father.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[797]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[798]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\\\\\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\\n\\\"\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[799]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This accurate description of the sunâ€™s power source is not included in the answer. The sunâ€™s light plays a critical role in Earthâ€™s climate system. This broader impact of the sunâ€™s light on Earthâ€™s climate system is not addressed in the answer. The effect of sunlight on weather patterns and ocean currents is omitted in the answer. The energy from the sun provides heat and light, which are essential for life on Earth. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This accurate description of the sunâ€™s power source is not included in the answer. The sunâ€™s light plays a critical role in Earthâ€™s climate system. This broader impact of the sunâ€™s light on Earthâ€™s climate system is not addressed in the answer. The effect of sunlight on weather patterns and ocean currents is omitted in the answer. The energy from the sun provides heat and light, which are essential for life on Earth. This statement is somewhat supported by the ground truth\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[800]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[801]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[802]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[769]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[805]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[806]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[807]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[808]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[810]: OutputParserException(Invalid json output: Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… ØªÙˆØ­ÙŠØ¯ Ø£Ø±Ø§Ø¶ÙŠÙ‡Ø§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ØªÙ… Ø¥Ù‚Ø§Ù…Ø© Ù„ÙŠØªÙˆØ§Ù†ÙŠØ§ ÙÙŠ Ø¹Ø§Ù… 1918.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[812]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[813]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[774]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[779]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[815]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[816]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[817]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[821]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[822]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[823]: OutputParserException(Failed to parse StringIO from completion null. Got: 1 validation error for StringIO\n",
            "  Input should be a valid dictionary or instance of StringIO [type=model_type, input_value=None, input_type=NoneType]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/model_type\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[784]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[789]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[804]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[825]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[826]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[827]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[830]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[831]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[832]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[833]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[794]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[814]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[835]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[836]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[837]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[842]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[809]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[845]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[824]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[848]: OutputParserException(Invalid json output: Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹Ø© Ø¨Ø§Ù„ØªÙŠÙ…ÙˆØ± ÙˆØ¬Ø§ÙŠ ÙˆÙŠØ¨Ø±\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[850]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[851]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[819]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[855]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. Fix the output string and return it.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[857]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[860]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[862]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[863]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[829]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[868]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[871]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[872]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[834]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[839]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[876]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[878]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[880]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[882]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[883]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[844]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[849]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[886]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[887]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[888]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[890]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[891]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[893]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[854]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[859]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[895]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[897]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[899]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[902]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[903]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[864]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[869]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[905]: OutputParserException(Invalid json output: Mozila is a company that produces and is highly respected in the field of pharmaceuticals and biotechnology. Mozila was founded in 1977. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is one of the largest companies in the field of pharmaceuticals and biotechnology. Mozila is\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[906]: OutputParserException(Invalid json output: Where was Albert Einstein born?\\n\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[908]: OutputParserException(Invalid json output: What can you tell me about albert Albert Einstein?\\\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[910]: OutputParserException(Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"Eritrea is bordered by Ethiopia to the north and south, the Western Sahara to the northwest, the Sudan to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the northwest, the Sudan South to the northwest, the Sudan Central to the northwest, the Sudan North to the\"}]}. Got: 1 validation error for StringIO\n",
            "text\n",
            "  Field required [type=missing, input_value={'statements': [{'stateme...e Sudan North to the'}]}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[912]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[874]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[879]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[915]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[917]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[918]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[920]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[921]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[922]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[923]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[924]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[884]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[889]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[925]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[926]: OutputParserException(Invalid json output: Where was Albert Einstein born?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[927]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[930]: OutputParserException(Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"Cyrus the Great defeated the following countries: \", \"reason\": \"The context mentions Cyrus the Great defeating several countries including Espana, Italia, Francia, Ingleterra, Irlanda, Ingleterra, Italia, Espagna, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia, Italia,\"}]}. Got: 1 validation error for StringIO\n",
            "text\n",
            "  Field required [type=missing, input_value={'statements': [{'stateme...lia, Italia, Italia,'}]}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[932]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[933]: OutputParserException(Invalid json output: Ù‡Ø²Ù… Ø§Ù„Ù…ÙŠØ¯ÙŠÙŠÙ†ØŒ Ù„ÙŠØ¯ÙŠØ§ØŒ ÙˆØ§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø¨Ø§Ø¨Ù„ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[894]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[914]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[937]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[941]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[942]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[943]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[945]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[904]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[909]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[946]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[947]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[950]: OutputParserException(Invalid json output: Albert Einstein was a German-born theoretical physicist. Albert Einstein is recognized as one of the greatest and most influential physicists of all time. Albert Einstein was best known for developing the theory of relativity. Albert Einstein also made important contributions to the development of the theory of quantum mechanics. Albert Einstein was a German-born theoretical physicist. Albert Einstein is recognized as one of the greatest and most influential physicists of all time. Albert Einstein was best known for developing the theory of relativity. Albert Einstein also made important contributions to the development of the theory of quantum mechanics. Albert Einstein was a German-born theoretical physicist. Albert Einstein is recognized as one of the greatest and most influential physicists of all time. Albert Einstein was best known for developing the theory of relativity. Albert Einstein also made important contributions to the development of the theory of quantum mechanics. Albert Einstein was a German-born theoretical physicist. Albert Einstein is recognized as one of the greatest and most influential physicists of all time. Albert Einstein was best known for developing the theory of relativity. Albert Einstein also made important contributions to the development of the theory of quantum mechanics. Albert Einstein was a German-born theoretical physicist. Albert Einstein is recognized as one of the greatest and most influential physicists of all time. Albert Einstein was best\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[952]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[953]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.\\\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[954]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[919]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[958]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[960]: OutputParserException(Invalid json output: Ù„ÙˆÙ†Ø§ 26 Ù‡Ùˆ Ù„Ø§Ø¹Ø¨ Ù‚ÙØ² Ø·ÙˆÙŠÙ„ ØµÙŠÙ†ÙŠ\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[962]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[963]: OutputParserException(Invalid json output: Ù‡Ù… Ø¬Ù…Ø§Ø¹Ø© Ø¹Ø±Ù‚ÙŠØ© Ø¯ÙŠÙ†ÙŠØ© Ù…Ø³ÙŠØ­ÙŠØ© Ø£ØµÙ„Ù‡Ø§ Ù…Ù† Ø´Ø±Ù‚ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙˆÙ…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø´Ø±Ù‚ ÙÙŠ Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[967]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[929]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[971]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[972]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[973]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[934]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[975]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[976]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[977]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[939]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[944]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[980]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. Fix the output string and return it.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[981]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[982]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[983]: OutputParserException(Invalid json output: The mention of her title is present in the given context.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[985]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[986]: OutputParserException(Invalid json output: What is the population of Saint Eizidour according to the 2021 census?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[949]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[990]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. Fix the output string and return it.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[991]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[992]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[995]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[996]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[998]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[959]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[964]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1001]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1005]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1006]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1007]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1008]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[969]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[974]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1011]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1013]: OutputParserException(Invalid json output: Ø§Ù„Ø¬Ø²Ø± Ø§Ù„Ø¨Ù„ÙŠØ§Ø±ØŒ ÙˆÙ…Ù† Ø«Ù…Ù‘ Ø®Ø·Ù‘ÙˆØ§ Ù†Ø­Ùˆ Â«ØªØ±Ø´ÙŠØ´Â» ÙÙŠ Ø¥Ø³Ø¨Ø§Ù†ÙŠØ§ Ø­ÙŠØ« Ø­ÙˆÙ‘Ù„ÙˆØ§ Ø¥Ø­Ø¯Ù‰ Ù…Ø­Ø·Ù‘Ø§ØªÙ‡Ù… Ø§Ù„ØªÙ‘Ø¬Ø§Ø±ÙŠÙ‘Ø© Ø¥Ù„Ù‰ Ù…Ø³ØªØ¹Ù…Ø±Ø©. \\\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1016]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1017]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[979]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[984]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1020]: OutputParserException(Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"BSD is a free operating system.\", \"reason\": \"The context mentions that BSD is a free operating system. There is no information suggesting that BSD is not free.\"}]}. Got: 1 validation error for StringIO\n",
            "text\n",
            "  Field required [type=missing, input_value={'statements': [{'stateme...hat BSD is not free.'}]}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1021]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1022]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1023]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1024]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth which mentions the sun providing light and its roles, though it focuses more broadly on the sun's energy. This accurate description of the sunâ€™s power source is not included in the answer. The sunâ€™s light plays a critical role in Earthâ€™s climate system. This broader impact of the sunâ€™s light on Earthâ€™s climate system is not addressed in the answer. The effect of sunlight on weather patterns and ocean currents is omitted in the answer. The boiling point of water is 100 degrees Celsius at sea level. This statement is directly supported by the ground truth which specifies the boiling point of water as 100 degrees Celsius at sea level.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1025]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1027]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1028]: OutputParserException(Invalid json output: Ù…Ù†Ø·Ù‚Ø© Ø²Ø±Ø§Ø¹ÙŠØ© ÙƒØ¨ÙŠØ±Ø© ØªØ³Ù…Ù‘Ù‰ Ø§Ù„Ø¹Ø¨Ø¯Ù„ÙŠ\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[989]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[994]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1030]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1031]: OutputParserException(Invalid json output: Where was Hussein Doogan born?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1032]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1035]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1036]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1037]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[999]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1004]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1019]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1040]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1041]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1042]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1043]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1045]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1046]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1048]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1009]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1014]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1051]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1053]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1055]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1056]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1057]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1058]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1061]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1039]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1063]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1065]: OutputParserException(Invalid json output: The university's primary goal is to be an academic institution. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best universities in the country. The university is considered one of the most important academic goals in Spain, where it is considered one of the best\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1067]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1068]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1029]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1034]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1049]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1070]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1072]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1073]: OutputParserException(Invalid json output: The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kastani zemeq. The statement mentions the country of origin of the kast\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1075]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1076]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1077]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1044]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1080]: OutputParserException(Invalid json output: statements: [ { \"statement\": \"Lietuvos valstybes turizmo daliniai yra ikiomai ikiomai Lietuvos teritoriumas, ikiomai Lietuvos valstybes turizmo daliniai yra ikiomai ikiomai Lietuvos teritoriumas, ikiomai Lietuvos valstybes turizmo daliniai yra ikiomai ikiomai Lietuvos teritoriumas, ikiomai Lietuvos valstybes turizmo daliniai yra ikiomai ikiomai Lietuvos teritoriumas, ikiomai Lietuvos valstybes turizmo daliniai yra ikiomai ikiomai Lietuvos teritoriumas, ikiomai Lietuvos valstybes turizmo daliniai yra ikiomai ikiomai Lietuvos teritoriumas, ikiomai Lietuvos valstybes turizmo daliniai yra ikiomai ikiomai Lietuvos teritoriumas, ikiomai Lietuvos valstybes turizmo dal\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1082]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1083]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1085]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1087]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1088]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1089]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1054]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1090]: OutputParserException(Invalid json output: statements\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1069]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1093]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1096]: OutputParserException(Invalid json output: rome Ù‡ÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø¥ÙŠØ·Ø§Ù„ÙŠØ©ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø§ ØªØ³ØªØ¶ÙŠÙ Ø§Ù„Ø³ÙØ§Ø±Ø§Øª. 0\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1059]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1098]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1064]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1100]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1102]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1103]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1104]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. The reason for this is that the ground truth states that the sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium. This accurate description of the sunâ€™s power source is not included in the answer. The statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion. The reason for this is that the ground truth is incomplete and does not mention the sunâ€™s energy. The statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. The reason for this is that the ground truth states that the sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium. This accurate description of the sunâ€™s power source is not included in the answer. The statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion. The reason for this is that the ground truth is incomplete and does not mention the sunâ€™s energy. The statement is somewhat supported by the ground truth\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1105]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1106]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1107]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1108]: OutputParserException(Invalid json output: The exact sentence is present in the given context.\\\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1074]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1111]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1112]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1113]: OutputParserException(Invalid json output: Ø§Ù„Ø­Ø¯Ø±ÙŠØŒ Ù‡Ùˆ ÙÙ† Ù…Ù† Ø§Ù„ÙÙ†ÙˆÙ† Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ…Ø§Ø±Ø³ ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¬Ø¨Ù„ÙŠØ© ÙˆØ§Ù„Ø³Ø§Ø­Ù„ÙŠØ© Ø¨Ù…Ù†Ø·Ù‚Ø© Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø© (ØºØ±Ø¨ )ØŒ Ø§Ø´ØªÙ‡Ø± Ø£ÙƒØ«Ø± ÙÙŠ Ù…Ø­Ø§ÙØ¸Ø© Ø§Ù„Ø·Ø§Ø¦Ù Ø§Ù„Ø¬Ø¨Ù„ÙŠØ©ØŒ ÙŠØ¹Ø¯ ÙˆØ§Ø­Ø¯Ø§Ù‹ Ù…Ù† Ø«Ù„Ø§Ø«Ø© ÙÙ†ÙˆÙ† ØªØªØ¨Ø¹ Ù„ÙÙ† Ø§Ù„Ù…Ø¬Ø±ÙˆØ± (Ø§Ù„Ù…Ø¬Ø§Ù„Ø³ÙŠØŒ Ø§Ù„Ø­Ø¯Ø±ÙŠØŒ Ø§Ù„Ù‚ØµÙŠÙ…ÙŠ).Ù†Ø¨Ø°Ø©. ÙŠÙ…Ø§Ø±Ø³ ÙÙ† Ø§Ù„Ø­Ø¯Ø±ÙŠ Ø­ØµØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ù…Ø¬Ø§Ù„Ø³ Ø§Ù„Ø±Ø¬Ø§Ù„ Ù…Ø³Ø§Ø¡Ù‹ØŒ ÙˆÙŠØ­ÙŠØ§ Ø§Ù„ÙÙ† Ø¨ØªØ­Ù„Ù‚ Ø§Ù„Ø±Ø¬Ø§Ù„ Ø¬Ù„ÙˆØ³Ø§Ù‹ØŒ ÙÙŠÙ…Ø§ ÙŠØªÙˆØ³Ø·Ù‡Ù… Ø§Ù„Ù…ØºÙ†ÙŠ Ø§Ù„Ø°ÙŠ ÙŠÙ†Ø´Ø¯ Ø£Ø¨ÙŠØ§Øª Ù…Ù† Ø§Ù„Ø´Ø¹Ø± Ø¨Ù„Ø­Ù† Ø§Ù„Ø­Ø¯Ø±ÙŠ. Ø£Ø­ÙŠØ§ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ÙÙ†Ø§Ù†ÙŠÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙŠÙ† ÙÙ† Ø§Ù„Ø­Ø¯Ø±ÙŠ ÙÙŠ Ø£Ø¹Ù…Ø§Ù„Ù‡Ù… Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚ÙŠØ©ØŒ ÙƒØ§Ù„ÙÙ†Ø§Ù† Ø§Ù„Ø±Ø§Ø­Ù„ØŒ Ø·Ø§Ø±Ù‚ Ø¹Ø¨Ø¯Ø§Ù„Ø­ÙƒÙŠÙ… ÙÙŠ Ø£ØºÙ†ÙŠØªÙ‡ \\\\\\\"ÙŠØ§ Ù„ÙŠØª Ù…Ø§ Ø¨ÙŠÙ†Ù†Ø§ Ø±ÙŠØ¹Ø§Ù†\\\\\\\" ÙˆÙ…Ù† Ø£Ø¨ÙŠØ§Øª Ø§Ù„Ø­Ø¯Ø±ÙŠ: Ø´Ø±ÙŠØª Ù„ÙŠ Ø¨Ù†Ø¯Ù‚Ø© Ù…Ø´Ø­Ø§Ù†Ù‡Ø§ Ø±Ø³Ù… Ø§Ù„Ø°Ù‡Ø¨ ÙÙŠÙ‡ Ø°Ø®ÙŠØ±ØªÙ‡Ø§ Ù…Ø¯ Ù…ÙƒÙŠ ØºÙŠØ± Ø²Ù‡Ø¨ØªÙ‡Ø§ ØºØ±Ø§Ø±Ø©ÙˆØ£ÙŠØ¶Ø§Ù‹: Ø«Ù„Ø§Ø« ÙŠØ§ Ù‚Ù„Ø¨ ØµØ§Ø¨ØªÙƒ Ø­Ø±Ù‚Ù‡ ÙˆÙØ±Ù‚Ù‡ ÙˆÙ„Ùƒ Ù…Ø³ÙƒØ§Ù†ÙŠ\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1116]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1117]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1118]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[1079]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1084]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1099]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1122]: OutputParserException(Invalid json output: The provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain. The context does not provide any information about the specific feature of the newspaper's editorial line. The answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context. The context does not provide any information about the newspaper's editorial line, and the answer is not related to the context\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1123]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1125]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1126]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1127]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1131]: OutputParserException(Invalid json output: What is the date?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1132]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1094]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1133]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1136]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1137]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1138]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\\\\\\\"\\n\\nAlbert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\\\\\\\"\\n\\nAlbert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\\\\\\\"\\n\\nAlbert Einstein, born on 14 March 1879, was a German-born\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1141]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1142]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1109]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1145]: OutputParserException(Invalid json output: University of the Arab States covers the following geographic regions: Egypt, Iran, Israel, Palestine, Lebanon, Syria, Yemen, Kuwait, Bahrain, United Arab Emirates, Qatar, Saudi Arabia, Oman, Jordan, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon, Iran, Israel, Egypt, Syria, Palestine, Lebanon,\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1146]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1147]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1148]: OutputParserException(Failed to parse StringIO from completion {\"classifications\": [{\"statement\": \"\\u062c\\u0627\\u0645\\u0639\\u0629 \\u0627\\u0644\\u062f\\u0648\\u0644 \\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629 \\u0647\\u064a \\u0645\\u0646\\u0638\\u0645\\u0629 \\u062f\\u0648\\u0644\\u064a\\u0629 \\u0642\\u0627\\u0645\\u062a \\u0628\\u0639\\u062f \\u0627\\u0644\\u062d\\u0631\\u0628 \\u0627\\u0644\\u0639\\u0627\\u0644\\u0645\\u064a\\u0629 \\u0627\\u0644\\u062b\\u0627\\u0646\\u064a\\u0629\\u060c \\u0648\\u0642\\u062f \\u062a\\u0643\\u0648\\u0646\\u062a \\u0641\\u064a 22 \\u0645\\u0627\\u0631\\u0633 1945\\u0645 \\u0623\\u064a \\u0642\\u0628\\u0644 \\u0645\\u0646\\u0638\\u0645\\u0629 \\u0627\\u0644\\u0623\\u0645\\u0645 \\u0627\\u0644\\u0645\\u062a\\u062d\\u062f\\u0629 \\u0628\\u0634\\u0647\\u0648\\u0631\\u060c \\u0648\\u062a\\u0623\\u0644\\u0641\\u062a \\u0641\\u064a \\u0623\\u0648\\u0644 \\u0648\\u0642\\u062a\\u0647\\u0627 \\u0645\\u0646 \\u0633\\u0628\\u0639 \\u062f\\u0648\\u0644 \\u0639\\u0631\\u0628\\u064a\\u0629 \\u0643\\u0627\\u0646\\u062a \\u062a\\u062a\\u0645\\u062a\\u0639 \\u0628\\u0627\\u0644\\u0627\\u0633\\u062a\\u0642\\u0644\\u0627\\u0644 \\u0627\\u0644\\u0633\\u064a\\u0627\\u0633\\u064a \\u0648\\u0642\\u062a\\u0630\\u0627\\u0643\\u060c \\u0647\\u064a: \\u0645\\u0635\\u0631\\u060c \\u0633\\u0648\\u0631\\u064a\\u0627\\u060c \\u0627\\u0644\\u0645\\u0645\\u0644\\u0643\\u0629 \\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629 \\u0627\\u0644\\u0633\\u0639\\u0648\\u062f\\u064a\\u0629\\u060c \\u0634\\u0631\\u0642 \\u0627\\u0644\\u0623\\u0631\\u062f\\u0646\\u060c \\u0644\\u0628\\u0646\\u0627\\u0646\\u060c \\u0627\\u0644\\u0639\\u0631\\u0627\\u0642\\u060c \\u0627\\u0644\\u064a\\u0645\\u0646.\\\",\\n            \"}]}. Got: 1 validation error for StringIO\n",
            "text\n",
            "  Field required [type=missing, input_value={'classifications': [{'st...Ù†.\",\\n            '}]}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1149]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion. This statement is not included in the answer. The statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion. This statement is not included in the answer. The statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion. This statement is not included in the answer. The statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion. This statement is not included in the answer\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1150]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1151]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1152]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1114]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1119]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1155]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1157]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1161]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1162]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1163]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1124]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1129]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1166]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1167]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1168]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1170]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1171]: OutputParserException(Invalid json output: Where was Albert Einstein born?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1173]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1174]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This accurate description of the reasons for the decline of the Sudanese pound is not included in the answer. The statement is incorrect and contradicts the ground truth which states that the decline of the Sudanese pound is due to some failed actions by the Sudanese Central Bank. The energy from the sun provides heat and light, which are essential for life on Earth. The sun's light plays a critical role in Earth's climate system. Sunlight helps to drive the weather and ocean currents. The effect of sunlight on weather patterns and ocean currents is omitted in the answer. The boiling point of water is 100 degrees Celsius at sea level. The boiling point of water can change with altitude. The statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This accurate description of the reasons for the decline of the Sudanese pound is not included in the answer. The statement is incorrect and contradicts the ground truth which states that the decline of the Sudanese\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1134]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1139]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1177]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1178]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1180]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1182]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1183]: OutputParserException(Invalid json output: What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert Einstein? What can you tell me about albert Albert\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1144]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1164]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1186]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1187]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1188]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1190]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1191]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1193]: OutputParserException(Invalid json output: ÙŠÙˆÙ… ØªÙ‡Ø§Ù…Ø© Ø§Ù„Ø§ÙˆÙ„ Ù‡Ùˆ Ù…Ù† Ø§Ù‚Ø¯Ù… Ø§ÙŠØ§Ù… Ø§Ù„Ø¹Ø±Ø¨ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø³Ù„Ø£Ù… ÙÙ‚Ø¯ ÙˆÙ‚Ø¹Øª Ø§Ù„Ø­Ø±Ø¨ Ø¨ÙŠÙ† Ù‚Ø¨Ø§Ø¦Ù„ Ø¨Ù†ÙŠ Ø§Ù„Ø­ÙƒÙ… ÙˆÙ‚Ø­Ø·Ø§Ù† Ùˆ Ù‚Ø¨Ø§Ø¦Ù„ Ø¨ÙƒØ± Ø¨Ù† ÙˆØ§Ø¦Ù„ ÙˆØªØºÙ„Ø¨ ÙˆÙƒØ§Ù† Ø§Ù„Ù†ØµØ± Ø­Ù„ÙŠÙØ§Ù‹ Ù„Ù‚Ø­Ø·Ø§Ù† ÙˆØ¨Ù†ÙŠ Ø§Ù„Ø­ÙƒÙ… ÙˆØ£Ø¬Ù„ÙŠØª Ø¨ÙƒØ± ÙˆØªØºÙ„Ø¨ Ø§Ù„Ù‰ Ø§Ù„Ø­Ø¬Ø§Ø² Ø§Ù„Ù…Ø¹Ø±ÙƒØ© Ø¨ÙƒØ± Ùˆ ØªØºÙ„Ø¨ ÙƒØ§Ù†Øª Ø¨ØªÙ‡Ø§Ù…Ø©ØŒ ÙÙ„Ù…Ø§ Ù‚Ø§Ø±Ø¨Øª Ø¨Ù„Ø¯ÙŽ Ø­ÙƒÙ… Ø¨Ù† Ø³Ø¹Ø¯ Ø¨Ù† Ù…Ø°Ø­Ø¬ØŒ Ø­Ø§Ø±Ø¨ØªÙ‡Ø§ Ø¨Ù†ÙŠ Ø§Ù„Ø­ÙƒÙ… Ø¨Ù† Ø³Ø¹Ø¯ Ø§Ù„Ø¹Ø´ÙŠØ±Ø©ØŒ ÙØ£Ø®Ø±Ø¬ØªÙ‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¬Ø§Ø².[18] Ø°ÙƒØ± Ù…Ø­Ù…Ø¯ Ø§Ø¨Ù† Ø§Ø³Ø­Ø§Ù‚ØŒ Ø¥Ø¬Ù„Ø§Ø¡ Ø¨ÙƒØ± ÙˆØªØºÙ„Ø¨ Ù…Ù† ØªÙ‡Ø§Ù…Ø© Ø£Ù† Ù‚Ø­Ø·Ø§Ù† Ù‚ØµØ¯Øª Ù„Ù‡Ø§ ÙØ§Ø¬Ù„ØªÙ‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø§Ù‚ ÙˆØ³ÙƒÙ†Øª Ù…ÙƒØ§Ù†Ù‡Ø§ Ø­ÙƒÙ… Ø§Ø¨Ù† Ø³Ø¹Ø¯ Ø§Ù„Ø¹Ø´ÙŠØ±Ø© Ø§Ø¨Ù† Ù…Ø°Ø­Ø¬ ÙˆÙ„Ù‡Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ø´Ø¹Ø§Ø± ÙˆØ±ÙˆØ§ÙŠØ§Øª.[19] Ù…ØµØ§Ø¯Ø±\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1195]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1154]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1159]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1196]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1197]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1198]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1201]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1202]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1205]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1206]: OutputParserException(Invalid json output: What is the name of the person?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1169]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1207]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1208]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1211]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1212]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1215]: OutputParserException(Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"\\u062f\\u0648\\u0644\\u0629 \\u0627\\u0644\\u0632\\u0646\\u0643\\u064a\\u0629 \\u0647\\u064a \\u062f\\u0648\\u0644\\u0629 \\u0641\\u064a \\u0634\\u0631\\u0642 \\u0622\\u0633\\u064a\\u0627.\", \"reason\": \"\\u062f\\u0648\\u0644\\u0629 \\u0627\\u0644\\u0632\\u0646\\u0643\\u064a\\u0629 \\u0647\\u064a \\u062f\\u0648\\u0644\\u0629 \\u0641\\u064a \\u0634\\u0631\\u0642 \\u0622\\u0633\\u064a\\u0627\\u060c \\u0648\\u0628\\u0627\\u0644\\u062a\\u0627\\u0644\\u064a\\u060c \\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u062a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0647 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\\u0636\\u0648\\u0639\\u060c \\u0648\\u064a\\u0645\\u0643\\u0646 \\u0623\\u0646 \\u064a\\u0643\\u0648\\u0646 \\u0647\\u0630\\u0627 \\u0627\\u0644\\u0642\\u0648\\u0644 \\u0645\\u064f\\u0634\\u0627\\u0628\\u0647\\u064b\\u0627 \\u0644\\u0650\\u0644\\u0645\\u0648\"}]}. Got: 1 validation error for StringIO\n",
            "text\n",
            "  Field required [type=missing, input_value={'statements': [{'stateme...Ø¨Ù‡Ù‹Ø§ Ù„ÙÙ„Ù…Ùˆ'}]}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[1179]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1184]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1217]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1220]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1222]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1223]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1225]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[1189]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1194]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1227]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1228]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1229]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. The energy from the sun provides heat and light, which are essential for life on Earth. The sun's light plays a critical role in Earth's climate system. Sunlight helps to drive the weather and ocean currents. The sun's light plays a critical role in Earth's climate system. The energy from the sun provides heat and light, which are essential for life on Earth. The sun's light plays a critical role in Earth's climate system. The energy from the sun provides heat and light, which are essential for life on Earth. The sun's light plays a critical role in Earth's climate system. The energy from the sun provides heat and light, which are essential for life on Earth. The sun's light plays a critical role in Earth's climate system. The energy from the sun provides heat and light, which are essential for life on Earth. The sun's light plays a critical role in Earth's climate system. The energy from the sun provides heat and light, which are essential for life on Earth. The sun's light plays a critical role in Earth's climate system. The energy from the sun provides heat and light, which are essential for life on Earth.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1231]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1232]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1233]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1235]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1236]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1199]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1204]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1237]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1238]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
            "He received the 1921 Nobel Prize in Physics for his services to theoretical physics.\n",
            "He published 4 papers in 1905.\n",
            "Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1240]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1241]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1242]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1245]: OutputParserException(Invalid json output: 1\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Now perform the same with the following input\n",
            "input: {\n",
            "    \"context\": \"Ø¹ØµØ± Ø§Ù„Ù†Ù‡Ø¶Ø© Ù‡ÙŠ Ø­Ø±ÙƒØ© Ø«Ù‚Ø§ÙÙŠØ© Ø§Ø³ØªÙ…Ø±Øª ØªÙ‚Ø±ÙŠØ¨Ø§ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø¹Ø´Ø± Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø¹Ø´Ø±. ÙˆÙƒØ§Ù†Øª Ø¨Ø¯Ø§ÙŠØªÙ‡Ø§ ÙÙŠ Ø£ÙˆØ§Ø®Ø± Ø§Ù„Ø¹ØµÙˆØ± Ø§Ù„ÙˆØ³Ø·Ù‰ Ù…Ù† Ø¥ÙŠØ·Ø§Ù„ÙŠØ§ Ø«Ù… Ø£Ø®Ø°Øª ÙÙŠ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø¥Ù„Ù‰ Ø¨Ù‚ÙŠØ© Ø£Ø±Ø¬Ø§Ø¡ Ø£ÙˆØ±ÙˆØ¨Ø§. Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† ØªÙˆÙØ± Ø§Ù„ÙˆØ±Ù‚ ÙˆØ§Ø®ØªØ±Ø§Ø¹ Ø­Ø±ÙˆÙ Ø§Ù„Ù…ÙˆÙ†ÙˆØªÙŠØ¨ Ø§Ù„ØªÙŠ Ø³Ø§Ù‡Ù…Øª ÙÙŠ Ø³Ø±Ø¹Ø© Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£ÙÙƒØ§Ø± Ø£ÙˆØ§Ø®Ø± Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø±ØŒ Ø¥Ù„Ø§ Ø£Ù† ØªØºÙŠÙŠØ±Ø§Øª Ø¹ØµØ± Ø§Ù„Ù†Ù‡Ø¶Ø© Ù„Ù… ØªÙ†ØªØ´Ø± Ø¨Ø´ÙƒÙ„ Ù…ÙˆØ­Ø¯ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø£ÙˆØ±ÙˆØ¨Ø§. Ø´Ù‡Ø¯ Ø¹ØµØ± Ø§Ù„Ù†Ù‡Ø¶Ø© Ø¨ÙˆØµÙÙ‡ Ø­Ø±ÙƒØ© Ø«Ù‚Ø§ÙÙŠØ© Ø§Ø²Ø¯Ù‡Ø§Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø£Ø¯Ø¨ Ø¨Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ¥Ø¨Ø¯Ø§Ø¹Ø§ ÙÙŠ Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ù„Ø§ØªÙŠÙ†ÙŠ Ø¨Ø¯Ø¡Ø§Ù‹ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø¹Ø´Ø±ØŒ ÙˆÙ†Ù‡Ø¶Ø©Ù‹ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠØ¹Ø²Ùˆ Ø§Ù„Ù…Ø¹Ø§Øµ\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1246]: OutputParserException(Invalid json output: Ù„Ø§ØŒ ÙƒØ§Ù† Ø¹ØµØ± Ø§Ù„Ù†Ù‡Ø¶Ø© ÙØªØ±Ø© ØªÙ‚Ø¯Ù… Ø«Ù‚Ø§ÙÙŠ Ø¹Ù† Ø§Ù„Ø¹ØµÙˆØ± Ø§Ù„ÙˆØ³Ø·Ù‰.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1247]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1209]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1214]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1248]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.\\\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1250]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1251]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1252]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1253]: OutputParserException(Invalid json output: The exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\nThe exact sentence is present in the given context.\\nThe date of birth of Einstein is mentioned clearly in the context.\\\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1256]: OutputParserException(Invalid json output: What is the relevance of the input to the question?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1257]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1258]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1219]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1224]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1261]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1262]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1263]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1265]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1266]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1234]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1249]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1271]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1275]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1276]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. Fix the output string and return it.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"text\": {\"title\": \"Text\", \"type\": \"string\"}}, \"required\": [\"text\"], \"title\": \"StringIO\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1277]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1239]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1244]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1280]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1282]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1283]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1285]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1288]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. Fix the output string and return it.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1254]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1290]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1292]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1293]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1294]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1295]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1296]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1297]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1259]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1264]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1300]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1301]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1302]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1305]: OutputParserException(Invalid json output: statements: [ { \"statement\": \"Ø³Ø§Ø¯Ùˆ Ø¹Ù„ÙŠ ÙˆØ§Ø±Ø³AMI Ù‡Ùˆ Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ÙŠ.\", \"reason\": \"Ø§Ù„statement Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©\", \"verdict\": 0 }, { \"statement\": \"Ø³Ø§Ø¯Ùˆ Ø¹Ù„ÙŠ ÙˆØ§Ø±Ø³AMI Ù‡Ùˆ Ù…Ù† Ø£Ø¨Ø±Ø² Ù„Ø§Ø¹Ø¨ÙŠÙ† ÙÙŠ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ÙŠØ©.\", \"reason\": \"Ø§Ù„statement Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©\", \"verdict\": 0 } ]\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1306]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1269]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1274]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1289]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1313]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1315]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1317]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1318]: OutputParserException(Invalid json output: ÙƒØ§Ø³Ù„ÙÙŠÙ†ÙŠØ§ Ù‡Ùˆ Ù…Ø³Ù„Ø³Ù„ Ø±Ø³ÙˆÙ… Ù…ØªØ­Ø±ÙƒØ© Ø£Ù…Ø±ÙŠÙƒÙŠ Ù…ÙˆØ¬Ù‡ Ù„Ù„Ø¨Ø§Ù„ØºÙŠÙ† ÙŠØªÙ… Ø¨Ø«Ù‡ Ø¹Ù„Ù‰ Ù†ØªÙÙ„ÙŠÙƒØ³ Ø¨Ø§Ù„Ø®Ø¯Ù…Ø© ØªÙ„ÙØ²Ø© Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„ØªÙŠ ØªÙ†ØªØ¬Ù‡ ÙØ±ÙŠØ¯ÙŠØ±Ø§ØªÙˆØ± Ø³ØªÙˆØ¯ÙŠÙˆØ² Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ù„Ø¹Ø¨Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ÙŠØ§Ø¨Ø§Ù†ÙŠØ© ÙˆØ³Ù„Ø³Ù„Ø© ØªØ­Ù…Ù„ Ù†ÙØ³ Ø§Ù„Ø§Ø³Ù… Ù…Ù† Ù‚Ø¨Ù„ ÙƒÙˆÙ†AMIØŒ Ø£ÙˆÙ„ Ù…ÙˆØ³Ù…ÙŠÙ† ØªÙƒÙŠÙ Ø¹Ø§Ù… 1989 Ø¯Ø®ÙˆÙ„ ÙˆØªØ§Ø¨Ø¹ ØªØ±ÙŠÙÙˆØ± Ø¨Ù„Ù…onteØŒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø£Ù„ÙˆÙƒØ§Ø±Ø¯ (ÙƒØ§Ø³Ù„ÙÙŠÙ†ÙŠØ§) Ù„Ø£Ù†Ù‡Ø§ ØªØ¯Ø§ÙØ¹ Ø¹Ù† Ø§Ù„Ø£Ù…Ø© Ù…Ù† Ø§Ù„Ø£ÙÙ„Ø§Ù‚ Ù…Ù† Ø¯Ø±Ø§ÙƒÙˆÙ„Ø§ ÙˆØ£ØªØ¨Ø§Ø¹Ù‡ØŒ ÙŠØ³ØªØ¹ÙŠØ± Ø§Ù„Ù…ÙˆØ³Ù…Ø§Ù† Ø§Ù„Ø«Ø§Ù†ÙŠ ÙˆØ§Ù„Ø«Ø§Ù„Ø« Ø´Ø®ØµÙŠØ§Øª ÙˆØ¹Ù†Ø§ØµØ± Ù…Ù† Ø¥Ø¯Ø®Ø§Ù„ Ù„Ø¹Ø¨Ø© Ø¹Ø§Ù… 2005\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1279]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1284]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1321]: OutputParserException(Invalid json output: What is the unit of electrical resistance?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1323]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1325]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1326]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1327]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1330]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1332]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1333]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1335]: OutputParserException(Invalid json output: statements\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1336]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1337]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1338]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1299]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1304]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1319]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1341]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1342]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1343]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1346]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1347]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1348]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1309]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1314]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1352]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1356]: OutputParserException(Invalid json output: \n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1357]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1358]: OutputParserException(Invalid json output: ÙŠØ­Ø¯Ø« ÙƒØ³ÙˆÙ Ø§Ù„Ø´Ù…Ø³ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ…Ø± Ø§Ù„Ù‚Ù…Ø± Ø¨ÙŠÙ† Ø§Ù„Ø£Ø±Ø¶ ÙˆØ§Ù„Ø´Ù…Ø³ ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ ÙŠØ­Ø¬Ø¨ ØµÙˆØ±Ø© Ø§Ù„Ø´Ù…Ø³ ÙƒÙ„ÙŠÙ‹Ø§ Ø£Ùˆ Ø¬Ø²Ø¦ÙŠÙ‹Ø§ Ø¹Ù† Ù…Ø´Ø§Ù‡Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¶.\\\\\\\"\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1359]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1324]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1329]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1360]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1361]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1363]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1366]: OutputParserException(Invalid json output: Where was Albert Einstein born?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1368]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1334]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1371]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1373]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1375]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1377]: OutputParserException(Invalid json output: The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1378]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1380]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1339]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1344]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1381]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1382]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1383]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1385]: OutputParserException(Invalid json output: He was a German-born theoretical physicist. He is recognized as one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity. He also made important contributions to the development of the theory of quantum mechanics.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1386]: OutputParserException(Invalid json output: Where was Albert Einstein born?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1387]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1388]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1390]: OutputParserException(Invalid json output: statements\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "ERROR:ragas.executor:Exception raised in Job[1349]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1354]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1391]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1392]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1393]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1396]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1398]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1400]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1364]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1369]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1401]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1402]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1403]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1405]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1407]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1410]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1374]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1379]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1412]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1415]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1418]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1420]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1384]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1389]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1421]: OutputParserException(Invalid json output: Where was Albert Einstein born?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1422]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1425]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1428]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1430]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1394]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1399]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1432]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1434]: OutputParserException(Invalid json output: The primary function of the sun is to provide light to the solar system. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy. This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1435]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1437]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1438]: OutputParserException(Invalid json output: Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ®Ù„ÙŠ Ø¹Ù† Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø¯Ù†ÙŠØ©ØŒ Ø­ØµØ± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¨Ø§Ø¨ÙˆÙŠ Ø¨Ø£Ù…ÙˆØ± Ø§Ù„Ø¯ÙŠÙ† ÙˆØ§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…ØªØµÙ„Ø© Ø¨Ù‡\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1440]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1404]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1445]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1409]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1446]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1447]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1450]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1414]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1419]: TimeoutError()\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1451]: OutputParserException(Invalid json output: What is the profession of the person described in the given text?\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1453]: OutputParserException(Failed to parse StringIO from completion 0. Got: 1 validation error for StringIO\n",
            "  Input should be a valid dictionary or instance of StringIO [type=model_type, input_value=0, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/model_type\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1455]: OutputParserException(Invalid json output: statements\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1456]: OutputParserException(Invalid json output: The final answer is: $\\boxed{0}$\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1458]: OutputParserException(Invalid json output: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø°Ù‡Ø§ Ø§Ù„Ù„Ø¬Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1460]: OutputParserException(Invalid json output: Ø§Ù„Ø£Ø³Ø·ÙˆØ±Ø© Ù‡ÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø´ÙˆÙ„ÙŠÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ ÙÙŠ Ø¨Ù„Ø´ Ù…Ø§Ù„Ù‚Ø© ÙÙŠ Ø¥Ø³Ø¨Ø§Ù†ÙŠØ§. \\\n",
            "Ø§Ù„Ø£Ø³Ø·ÙˆØ±Ø© Ù‡ÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø´ÙˆÙ„ÙŠÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø«Ø±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ø¨Ù„Ø´ Ù…Ø§Ù„Ù‚Ø©. \\\n",
            "Ø§Ù„Ø£Ø³Ø·ÙˆØ±Ø© Ù‡ÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø´ÙˆÙ„ÙŠÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø«Ø±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ø¨Ù„Ø´ Ù…Ø§Ù„Ù‚Ø©. \\\n",
            "Ø§Ù„Ø£Ø³Ø·ÙˆØ±Ø© Ù‡ÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø´ÙˆÙ„ÙŠÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø«Ø±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ø¨Ù„Ø´ Ù…Ø§Ù„Ù‚Ø©. \\\n",
            "Ø§Ù„Ø£Ø³Ø·ÙˆØ±Ø© Ù‡ÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø´ÙˆÙ„ÙŠÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø«Ø±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ø¨Ù„Ø´ Ù…Ø§Ù„Ù‚Ø©. \\\n",
            "Ø§Ù„Ø£Ø³Ø·ÙˆØ±Ø© Ù‡ÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø´ÙˆÙ„ÙŠÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø«Ø±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ø¨Ù„Ø´ Ù…Ø§Ù„Ù‚Ø©. \\\n",
            "Ø§Ù„Ø£Ø³Ø·ÙˆØ±Ø© Ù‡ÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø´ÙˆÙ„ÙŠÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø«Ø±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠ\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "ERROR:ragas.executor:Exception raised in Job[1424]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1429]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1462]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1463]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1466]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1470]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1471]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1472]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1439]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1473]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1475]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1477]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1480]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1444]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1449]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1483]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1485]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1486]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1487]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1488]: OutputParserException(Invalid json output: Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1490]: OutputParserException(Invalid json output: statements\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1491]: OutputParserException(Failed to parse StringIO from completion {\"question\": \"Where was Albert Einstein born?\", \"noncommittal\": 0}. Got: 1 validation error for StringIO\n",
            "text\n",
            "  Field required [type=missing, input_value={'question': 'Where was A...rn?', 'noncommittal': 0}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1492]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "ERROR:ragas.executor:Exception raised in Job[1454]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1459]: TimeoutError()\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1493]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1495]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "WARNING:ragas.prompt.pydantic_prompt:LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.prompt.pydantic_prompt:Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "ERROR:ragas.executor:Exception raised in Job[1496]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1498]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. Fix the output string and return it.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"$defs\": {\\\"ContextRecallClassification\\\": {\\\"properties\\\": {\\\"statement\\\": {\\\"title\\\": \"Statement\", \\\"type\\\": \"string\"}, \\\"reason\\\": {\\\"title\\\": \"Reason\", \\\"type\\\": \"string\"}, \\\"attributed\\\": {\\\"title\\\": \"Attributed\", \\\"type\\\": \"integer\"}}}, \\\"required\\\": [\\\"statement\\\", \\\"reason\\\", \\\"attributed\\\"], \\\"title\\\": \"ContextRecallClassification\\\", \\\"type\\\": \"object\"},\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\n",
            "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "ERROR:ragas.executor:Exception raised in Job[1464]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1469]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1474]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1479]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1484]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[1489]: TimeoutError()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faithfulness': 0.2083, 'answer_relevancy': 0.5207, 'context_precision': 0.5732, 'context_recall': 0.8268, 'answer_correctness': 0.4131}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_correctness,\n",
        ")\n",
        "\n",
        "from ragas import evaluate\n",
        "\n",
        "result = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=[\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        answer_correctness,\n",
        "    ],\n",
        "    llm=evaluator_llm,\n",
        "    embeddings=embeddings,\n",
        ")\n",
        "result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ebf1cec-0e46-4184-9f46-0d2c304ee11b",
      "metadata": {
        "id": "8ebf1cec-0e46-4184-9f46-0d2c304ee11b"
      },
      "source": [
        "### Overall RAGAS Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b88c8a32-5452-4e53-844f-a779c1d78855",
      "metadata": {
        "id": "b88c8a32-5452-4e53-844f-a779c1d78855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9abcdd-1d42-444e-95e7-b808e6500cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ragas_overall_scores.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "overall_df = result.to_pandas()\n",
        "overall_df.to_csv(\"ragas_overall_scores.csv\", index=False)\n",
        "\n",
        "print(\"Saved: ragas_overall_scores.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303dec01-52f0-4ab3-8030-1575d109cdbd",
      "metadata": {
        "id": "303dec01-52f0-4ab3-8030-1575d109cdbd"
      },
      "source": [
        "### Per-Question RAGAS Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "76dab4e5-e824-4cd8-a8be-7053492a73a3",
      "metadata": {
        "id": "76dab4e5-e824-4cd8-a8be-7053492a73a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "efd68945-ff19-468e-906e-42b4b595eb07"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "evaluate() got an unexpected keyword argument 'return_multi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3037561254.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m per_question = evaluate(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     metrics=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/_analytics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: evaluate() got an unexpected keyword argument 'return_multi'"
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "\n",
        "per_question = evaluate(\n",
        "    df_sample,\n",
        "    metrics=[\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        answer_correctness,\n",
        "    ],\n",
        "    return_multi=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "1ec8d57f-254e-4982-8151-0d0386621101",
      "metadata": {
        "id": "1ec8d57f-254e-4982-8151-0d0386621101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19adf2eb-73b5-4f66-d0ce-502109004cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ragas_per_question_scores.csv (This file contains per-question scores)\n"
          ]
        }
      ],
      "source": [
        "# per_question_df = per_question.to_pandas()\n",
        "# per_question_df.to_csv(\"ragas_per_question_scores.csv\", index=False)\n",
        "\n",
        "# print(\"Saved: ragas_per_question_scores.csv\")\n",
        "# Renaming overall_df to per_question_df for clarity and saving it\n",
        "overall_df.to_csv(\"ragas_per_question_scores.csv\", index=False)\n",
        "print(\"Saved: ragas_per_question_scores.csv (This file contains per-question scores)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "34277cf0-c532-434b-b462-5fbde0b2653a",
      "metadata": {
        "id": "34277cf0-c532-434b-b462-5fbde0b2653a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e5e655-02ef-4530-877c-0f7c34867863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             user_input  \\\n",
            "0         \\nÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙŠ Ø´Ù…Ù„ØªÙ‡Ø§ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ®ØŸ   \n",
            "1                 Ù…ØªÙ‰ ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\\n   \n",
            "2           Ù…Ù† Ù‡Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ØŸ\\n\\n   \n",
            "3         Ù…Ø§ Ù‡ÙŠ Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n   \n",
            "4  Ù…Ø§ Ù‡Ùˆ Ø¯ÙˆØ± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©ØŸ\\n   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      retrieved_contexts  \\\n",
            "0  [Ø§Ù„Ø³ÙŠØ®ÙŠØ© (Ø¨Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ÙŠÙŽÙ‘Ø©: à¨¸à¨¿à©±à¨–à©€) Ù‡ÙŠ Ø¯ÙŠØ§Ù†Ø© ØªÙˆØ­ÙŠØ¯ÙŠØ© Ø¯Ø§Ø±Ù…ÙŠÙŽÙ‘Ø© Ù†Ø´Ø£Øª ÙÙŠ Ø´Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù†Ø¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø±. ÙˆØªØ£ØªÙŠ ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®ÙŠØ©Â» Ù…Ù† ÙƒÙ„Ù…Ø© Â«Ø³ÙŠØ®Â» ÙˆÙ‡ÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ ØªØ£ØªÙŠ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø³Ù†Ø³ÙƒØ±ÙŠØªÙŠ Ø§Ù„ØªÙŠ ØªØ¹Ù†ÙŠ Ø§Ù„ØªÙ„Ù…ÙŠØ° Ùˆ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¯ Ø£Ùˆ Ø§Ù„ØªØ§Ø¨Ø¹. ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø¯ÙŠØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆÙ‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£ÙƒØ¨Ø± Ø§Ù„Ø¯ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…. ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ØªÙ‚Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙˆØ¶ÙØ­ÙŽØª ÙÙŠ ÙƒØªØ§Ø¨Ù‡Ù… Ø§Ù„Ù…Ù‚Ø¯Ø³ Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ØŒ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† ÙˆØ§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ø®Ø§Ù„Ù‚ Ø§Ù„ÙˆØ§Ø­Ø¯ØŒ ÙˆØ§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙˆØ§Ø© Ù„Ù„Ø¨Ø´Ø±ÙŠØ© Ø¬Ù…Ø¹Ø§Ø¡ØŒ ÙˆØ§Ù„Ø§Ù†Ø®Ø±Ø§Ø· ÙÙŠ Ø®Ø¯Ù…Ø© Ù†ÙƒØ±Ø§Ù† Ø§Ù„Ø°Ø§ØªØŒ ÙˆØ§Ù„Ø³Ø¹ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙˆØ§Ø²Ø¯Ù‡Ø§Ø±Ù‡Ø§ØŒ ÙˆØ¥ØªØ¨Ø§Ø¹ Ø³Ù„ÙˆÙƒ Ù…Ø¹ÙŠØ´Ø© ØµØ§Ø¯Ù‚. ÙˆÙÙŠ Ø£ÙˆØ§Ø¦Ù„ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ÙˆØ§Ø­Ø¯ ÙˆØ§Ù„Ø¹Ø´Ø±ÙŠÙ† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø­ÙˆØ§Ù„ÙŠ 25 Ù…Ù„ÙŠÙˆÙ† Ø³ÙŠØ®ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆØªØ¹ÙŠØ´ Ø§Ù„ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ø£Ùˆ 76% (20 Ù…Ù„ÙŠÙˆÙ†) Ù…Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ØŒ Ù…ÙˆØ·Ù† Ø§Ù„Ø³ÙŠØ® ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø§Ù„Ù‡Ù†Ø¯ØŒ ÙˆÙŠØ¹ÙŠØ´ Ø­ÙˆØ§Ù„ÙŠ Ù…Ù„ÙŠÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø³Ø§Ø¨Ù‚Ø§Ù‹. ÙˆØ³Ø¨Ø¨ Ø§Ù†ØªØ´Ø§Ø±Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù‡Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ² Ø¹Ù„ÙŠÙ‡Ù… ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆØ¨ ÙˆÙ‡Ø¬Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ® Ø®Ø§Ø±Ø¬ Ø¨Ù„Ø§Ø¯Ù‡Ù…ØŒ Ø­ÙŠØ« Ø¨Ø¯Ø£Øª Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø®Ù„Ø§Ù„ Ø§Ù„Ù†ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø±ØŒ Ø¹Ù†Ø¯Ù…Ø§ Ø£ÙƒÙ…Ù„ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠÙˆÙ† Ø¶Ù…Ù‡Ù… Ù„Ù„Ø¨Ù†Ø¬Ø§Ø¨.\\nØªØ³ØªÙ†Ø¯ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù„ÙŠÙ… Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ù„Ù…Ø¤Ø³Ø³ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© ÙˆÙ‡Ùˆ Ø§Ù„ØºÙˆØ±Ùˆ Ù†Ø§Ù†Ø§ÙƒØŒ ÙˆØ®Ù„ÙØ§Ø¦Ù‡ Ø§Ù„ØªØ³Ø¹Ø© Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø±. Ù„Ù‚Ø¨ ØºÙˆØ±Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù…. Ø£Ù…Ø§ Ø§Ù„ØºÙˆØ±Ùˆ ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ù…Ù„Ù‚Ø¨ Ø¨Ø§Ù„Ø¹Ø§Ø´Ø±ØŒ Ø³Ø§Ù‡Ù… ÙÙŠ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø£Ø¬Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ©ØŒ ÙˆÙƒØ§Ù† Ø¥Ø³Ù‡Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø¥Ø¶ÙØ§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„ØªÙŠ Ø£Ø³Ø³Ù‡Ø§ Ø£ÙˆÙ„Ù‹Ø§ Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù†Ø§Ù†Ø§Ùƒ Ø¯ÙŠÙ Ø¬ÙŠ ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø®Ø§Ù…Ø³ Ø¹Ø´Ø± Ø¥Ø³Ù‡Ø§Ù…Ù‹Ø§ Ø¬Ø¯ÙŠØ±Ù‹Ø§ Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©. ÙˆØ³Ù…Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ ÙƒØ®Ù„ÙŠÙØ© Ù„Ù‡ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ø£Ù†Ù‡Ù‰ Ø®Ø· Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø¨Ø´Ø± ÙˆØ¬Ø¹Ù„ Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ø³ Ù„Ù„Ø³ÙŠØ® Ø¬ÙˆØ±Ùˆ Ø¬Ø±Ø§Ù†Ø« ØµØ§Ø­Ø¨ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ­ÙŠ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ†ÙˆÙŠ Ù„Ù„Ø³ÙŠØ®. ÙˆØªØ±ÙØ¶ Ø§Ù„Ø¯ÙŠØ§Ù†Ø© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø¨Ø£Ù† Ø£ÙŠ ØªÙ‚Ù„ÙŠØ¯ Ø¯ÙŠÙ†ÙŠ Ù…Ø¹ÙŠÙ† Ù„Ù‡ Ø§Ø­ØªÙƒØ§Ø± Ù„Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©. ÙˆØªØ·ÙˆØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø¯ÙŠÙ†ÙŠ. Ø­ÙŠØ« ØªØ¹Ø±Ø¶ Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø£ØªØ¨Ø§Ø¹ Ø§Ù„Ø³ÙŠØ® ÙˆÙ‡Ù… Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ± Ù„Ù„ØªØ¹Ø°ÙŠØ¨ ÙˆØ£Ø¹Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø¨Ø¹Ø¯ Ø±ÙØ¶Ù‡Ù… Ø§Ø¹ØªÙ†Ø§Ù‚ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. ÙˆØ£Ø«Ø§Ø± Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø®Ø§Ù„Ø³Ø§ ÙƒØ·Ù„Ø¨ Ù„Ø­Ù…Ø§ÙŠØ© Ø­Ø±ÙŠØ© Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø¯ÙŠÙ†.\\nØ§Ù„ØªØ§Ø±ÙŠØ®.\\nÙŠØ±ØªØ¨Ø· ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙˆØ«ÙŠÙ‚Ù‹Ø§ Ø¨ØªØ§Ø±ÙŠØ® Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±. Ù…Ù†Ø° Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…ØºÙˆÙ„ÙŠ Ù„Ù„Ù‡Ù†Ø¯ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ± Ø¬Ù‡Ø§Ù†ÙƒÙŠØ± (1605-1707)ØŒ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© ÙÙŠ ØµØ±Ø§Ø¹ Ù…Ø¹ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ù…ØºÙˆÙ„ØŒ Ù„Ø£Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªØ¤Ø«Ø± ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù‚Ø¨ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ù„Ù„Ù…ØºÙˆÙ„ ÙÙŠ Ø­ÙŠÙ† ØªØ¹ØªØ² Ø¨Ø§Ù„Ø£ÙˆÙ„ÙŠØ§Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…. Ù‚ÙØªÙ„ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ø§Ø±Ø²ÙŠÙ† Ø¹Ù„Ù‰ ÙŠØ¯ Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø§Ù†ØµÙŠØ§Ø¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ®. Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹ 10 Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®ØŒ Ø¹ÙØ°Ø¨ ÙˆØ£ÙØ¹Ø¯Ù… Ø§Ø«Ù†Ø§Ù† Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø£Ù†ÙØ³Ù‡Ù… (Ø§Ù„ØºÙˆØ±Ùˆ Ø£Ø±Ø¬Ø§Ù† ÙˆØ§Ù„ØºÙˆØ±Ùˆ ØªÙŠØ¬ Ø¨Ù‡Ø§Ø¯ÙˆØ±)ØŒ ÙˆØ£Ù‚Ø±Ø¨Ø§Ø¡ Ù…Ù‚Ø±Ø¨ÙŠÙ† Ù„Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ® Ù‚ÙØªÙ„Ùˆ Ø¨ÙˆØ­Ø´ÙŠØ© Ø¯ÙˆÙ† Ø±Ø­Ù…Ø© (Ù…Ø«Ù„ Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„ØºÙˆØ±Ùˆ Ø¬ÙˆØ¨ÙŠÙ†Ø¯ Ø³ÙŠÙ†Øº Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ† Ù…Ù† Ø§Ù„Ø¹Ù…Ø± 6 Ùˆ9 Ø³Ù†ÙˆØ§Øª)ØŒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø§Ù„ØªÙŠ Ø¹ÙØ°Ø¨Øª ÙˆÙ‚ÙØªÙ„Øª (Ù…Ø«Ù„ Ø¨Ø§Ù†Ø¯Ø§ Ø¨Ù‡Ø§Ø¯ÙˆØ±ØŒ Ø¨Ù‡Ø§ÙŠ Ù…Ø§ØªÙŠ Ø¯Ø§Ø³ØŒ Ø¨Ù‡Ø§ÙŠ Ø³Ø§ØªÙŠ Ø¯Ø§Ø³ ÙˆØ¨Ø§Ù‡Ø§ÙŠ Ø¯ÙŠØ§Ù„Ø§)ØŒ Ø¹Ù„Ù‰ ÙŠØ¯ Ø§Ù„Ø­ÙƒØ§Ù… Ø§Ù„Ù…ØºÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ø¨Ø±ÙŠÙ† Ù„Ø±ÙØ¶Ù‡Ù… Ø§Ù„Ø®Ø¶ÙˆØ¹ Ù„Ø£ÙˆØ§Ù…Ø±Ù‡Ù…ØŒ ÙˆÙ…Ø¹Ø§Ø±Ø¶ØªÙ‡Ù… Ù„Ø§Ø¶Ø·Ù‡Ø§Ø¯ Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ø³ÙƒØ±Øª Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ù„Ù…Ø¹Ø§Ø±Ø¶Ø© Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ù…ØºÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ù….\\nØªÙ…ÙŠÙ‘Ø² Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙˆÙ†ÙØ¯Ø±Ø§Ù„ÙŠØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ø£Ù…Ø±Ø§Ø¡ ÙˆØ§Ù„Ø³ÙŠØ® ØªØ­Øª Ø­ÙƒÙ… Ø§Ù„Ù…Ù‡Ø±Ø§Ø¬Ø§ Ø±Ø§Ù†Ø¬ÙŠØª Ø³ÙŠÙ†Øº Ø¨Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø§Ù„Ø¯ÙŠÙ†ÙŠ ÙˆØ§Ù„ØªØ¹Ø§ÙŠØ´ Ø§Ù„Ø³Ù„Ù…ÙŠ ÙˆØ§Ù„ØªØ¹Ø¯Ø¯ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ù„Ø·Ø©. ÙŠØ¹Ø¯ ØªØ£Ø³ÙŠØ³ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¹Ø§Ø¯Ø©Ù‹ Ø°Ø±ÙˆØ© Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØŒ Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙØªØ±Ø© Ø¬Ø§Ø¡Øª Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù„ØªØ´Ù…Ù„ ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§). Ø§Ø¹ØªÙ†Ù‚ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„ÙÙ„Ø§Ø­ÙŠÙ† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ø§Ù„Ø³ÙŠØ®ÙŠØ©. Ø£Ø®Ø° Ù‡Ø§Ø±ÙŠ Ø³ÙŠÙ†Øº Ù†Ø§Ù„ÙˆØ§ØŒ Ø§Ù„Ù‚Ø§Ø¦Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ø¬ÙŠØ´ Ø§Ù„Ø³ÙŠØ® Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ© Ø§Ù„ØºØ±Ø¨ÙŠØ©ØŒ Ø­Ø¯ÙˆØ¯ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥Ù„Ù‰ Ù…ØµØ¨ Ù…Ù…Ø± Ø®ÙŠØ¨Ø± (Ù‡Ùˆ Ù…Ù…Ø± Ø¬Ø¨Ù„ÙŠ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø¨Ø§ÙƒØ³ØªØ§Ù†ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ø¹ Ø£ÙØºØ§Ù†Ø³ØªØ§Ù†). Ø¯Ù…Ø¬Øª Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ù„Ù…Ø§Ù†ÙŠØ© Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© ÙˆØ§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆØ­ÙƒÙˆÙ…ÙŠØ© Ù…Ø¨ØªÙƒØ±Ø©.\\nØ´Ù‡Ø¯Øª Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„ØªÙŠ Ø³Ø¨Ù‚Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ (ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‡Ù†Ø¯ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ø¥Ù„Ù‰ Ø¯ÙˆÙ„ØªÙŠÙ† Ù…Ø³ØªÙ‚Ù„ØªÙŠÙ†ØŒ Ø§Ù„Ù‡Ù†Ø¯ ÙˆØ¨Ø§ÙƒØ³ØªØ§Ù†) Ø³Ù†Ø© 1947ØŒ ØµØ±Ø§Ø¹Ù‹Ø§ Ø­Ø§Ø¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ (Ù…Ù†Ø·Ù‚Ø© Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ© ÙˆØ«Ù‚Ø§ÙÙŠØ© ÙˆØªØ§Ø±ÙŠØ®ÙŠØ© ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø¢Ø³ÙŠØ§ØŒ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©ØŒ ÙˆØªØ¶Ù… Ù…Ù†Ø§Ø·Ù‚ ÙÙŠ Ø´Ø±Ù‚ Ø¨Ø§ÙƒØ³ØªØ§Ù† ÙˆØ´Ù…Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯) Ø¨ÙŠÙ† Ø§Ù„Ø³ÙŠØ® ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†ØŒ Ø´Ù‡Ø¯ Ø§Ù„Ù‡Ø¬Ø±Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø³ÙŠØ® Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙˆØ§Ù„Ù‡Ù†Ø¯ÙˆØ³ Ù…Ù† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ Ø§Ù„ØºØ±Ø¨ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ù‡Ø¬Ø±Ø© Ø¯ÙŠÙ†ÙŠØ© Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø´Ø±Ù‚ Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨. ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ø¶Ø±ØŒ ÙŠØ¹ÙŠØ´ ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø³ÙŠØ® ÙÙŠ ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø¨Ù†Ø¬Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙƒØ§Ø±Ø³ Ø£Ùˆ Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³Ø©.\\nØ§Ù„Ø®Ù…Ø³Ø© ÙƒØ§ÙØ§Øª Ù‡Ù… ÙŠØµØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„ØªØ²Ø§Ù… Ø¨Ù‡Ø§ ÙˆØ¹Ù†Ø¯Ù‡Ù… Ù…Ù† Ù„Ø§ ÙŠÙ„ØªØ²Ù… Ø¨Ø§Ù„ÙƒØ§ÙØ§Øª Ø§Ù„Ø®Ù…Ø³ ÙŠØµÙÙˆÙ‡ Ø¨ØµÙØ© Ø¨Ø§ØªØª patitØ£ÙŠ Ø§Ù„Ù…Ø±ØªØ¯ØŒ ÙˆÙ…Ù† ÙŠØ¯Ø®Ù„ Ø§Ù„Ø³ÙŠØ®ÙŠØ© Ø¬Ø¯ÙŠØ¯ ÙŠØ¬Ø¹Ù„ÙˆÙ‡ ÙŠØªØ¹ÙˆØ¯ Ø¹Ù„ÙŠÙ‡Ø§ ÙˆÙŠØ³Ù…ÙˆÙ‡ Ø§Ù„Ù…ØªÙƒÙŠÙ Ø§Ù„Ø¨Ø·Ø¦.\\nØ§Ù„ØºÙˆØ±Ùˆ Ø§Ù„Ø³ÙŠØ®.\\nØ§Ù„ØºÙˆØ±Ùˆ ÙƒÙ„Ù…Ø© ØªØ¹Ù†ÙŠ Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆÙ‡Ù… Ø¹Ø´Ø±Ø© ØºÙˆØ±Ùˆ Ù„Ù„Ø³ÙŠØ®ÙŠØ© ÙˆÙ‡Ù…:\\n]   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ (Ù‡Ø§Ù†ØºÙ„: ê·¸ í•´ ìš°ë¦¬ëŠ”)Ø› Ù‡Ùˆ Ù…Ø³Ù„Ø³Ù„ ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ ÙƒÙˆÙ…ÙŠØ¯ÙŠ ÙƒÙˆØ±ÙŠ Ø¬Ù†ÙˆØ¨ÙŠØŒ ÙŠØµÙ†Ù Ø¨Ø£Ù†Ù‡ Â«Ø£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø£ØµÙ„ÙŠ Ù„ÙŠ Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†Â». Ù…Ù† Ø¨Ø·ÙˆÙ„Ø© ÙˆÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠØŒ Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ SBS TV Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021 Ø¥Ù„Ù‰ 25 ÙŠÙ†Ø§ÙŠØ± 2022ØŒ Ø¨Ø« ÙƒÙ„ ÙŠÙˆÙ… Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† ÙˆØ§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¹Ø© 22:00 Ø¨ØªÙˆÙ‚ÙŠØª (KST). ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­Ù‹Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ù†ØªÙÙ„ÙŠÙƒØ³. Ø§Ù„Ù‚ØµØ©. Ø§Ù†ÙØµÙ„ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº (ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ) ÙˆÙŠÙˆÙ† Ø³Ùˆ (ÙƒÙŠÙ… Ø¯Ø§Ù…ÙŠ) Ù‚Ø¨Ù„ 10 Ø³Ù†ÙˆØ§ØªØŒ Ù„ÙƒÙ† ÙŠØµØ¨Ø­ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø°ÙŠ ØµÙˆØ±ÙˆÙ‡ Ø®Ù„Ø§Ù„ Ø£ÙŠØ§Ù… Ø¯Ø±Ø§Ø³ØªÙ‡Ù…Ø§ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø´Ø§Ø¦Ø¹Ù‹Ø§ØŒ Ø¨Ø³Ø¨Ø¨Ù‡ Ø¹Ù„ÙŠÙ‡Ù…Ø§ Ø§Ù„ÙˆÙ‚ÙˆÙ Ø£Ù…Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¬Ø¯Ø¯Ø§ØŒ Ø¹Ù„Ù‰ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ø§ÙŠØ±ÙŠØ¯ÙˆÙ† Ø°Ù„Ùƒ. Ø·Ø§Ù‚Ù…. Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ø±Ø³Ø§Ù… ØªØ´ÙƒÙŠÙ„ÙŠ Ø­Ø± Ù…ÙØ¹Ù… Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ©. Ø®Ø¨ÙŠØ±Ø© Ø¹Ù„Ø§Ù‚Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©. Ù…Ø®Ø±Ø¬ ÙˆØ«Ø§Ø¦Ù‚ÙŠØŒ ÙˆÙ‡Ùˆ ÙƒØ§Ù† Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ù„ÙŠ ØªØ´ÙˆÙŠ ÙˆÙˆÙ†Øº ÙˆØ§ÙŠÙˆÙ† Ø³Ùˆ. Ø£ÙŠØ¯ÙˆÙ„ Ù…Ø´Ù‡ÙˆØ±Ø© ØªØ­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ Ù…Ù†Ø° Ø¸Ù‡ÙˆØ±Ù‡Ø§. Ø§Ù„Ø¥Ù†ØªØ§Ø¬. ØµÙ†Ø§Ø¹Ø©. ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ù†ØŒ ÙˆÙ‡ÙŠ Ø£ÙˆÙ„ Ø¯Ø±Ø§Ù…Ø§ Ø£ØµÙ„ÙŠØ© Ù„Ù‡Ù…ØŒ Ø³ÙŠØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ© Ø³ÙˆØ¨Ø± Ù…ÙˆÙ† Ø¨ÙŠÙƒØªØ´Ø±Ø²ØŒ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ Ø§Ù„ØªØ§Ø¨Ø¹ Ù„Ø¥Ø³ Ø¨ÙŠ Ø¥Ø³ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø¨Ù€ Â«Ø¥Ø³ØªÙˆØ¯ÙŠÙˆ Ø¥Ø³Â». Ø·Ø§Ù‚Ù…. ÙÙŠ Ù…Ø§Ø±Ø³ 2021ØŒ Ø£Ø¹Ù„Ù† ØªØ£ÙƒÙŠØ¯ Ø§Ù†Ø¶Ù…Ø§Ù… ÙƒÙ„ Ù…Ù† ØªØ´ÙˆÙŠ ÙˆÙˆ Ø´ÙŠÙƒ ÙˆÙƒÙŠÙ… Ø¯Ø§ Ù…ÙŠ ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØŒ ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø¨Ù‡Ù… Ø¨Ø¹Ø¯ Ø«Ù„Ø§Ø« Ø³Ù†ÙˆØ§ØªØŒ Ø¢Ø®Ø± Ù…Ø±Ø© Ø¸Ù‡Ø±ÙˆØ§ ÙÙŠÙ‡ ÙÙŠ Ù…Ø¹Ø§ Ø¨ÙÙŠÙ„Ù… Ø§Ù„ØºÙ…ÙˆØ¶ \"\" Ù„Ø¹Ø§Ù… 2018. ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø§Ù‚Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠ ÙÙŠ 8 ÙŠÙˆÙ„ÙŠÙˆ. Ø§Ù„ØªØµÙˆÙŠØ±. Ø¨Ø¯Ø£ ØªØµÙˆÙŠØ± Ø§Ù„Ù…Ø³Ù„Ø³Ù„ ÙÙŠ Ø£ÙˆØ§Ø¦Ù„ ÙŠÙˆÙ„ÙŠÙˆ 2021. Ø·Ø¨Ø¹Ø© Ø£Ø®Ø±Ù‰. Ø³ÙŠØªÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†ØªØ§Ø¬ ÙˆÙŠØ¨ ØªÙˆÙ† Ù…Ù† Â«ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨Â»ØŒ Ø§Ù„ÙˆÙŠØ¨ ØªÙˆÙ† Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ø´Ø®ØµÙŠØªÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØªÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠ (ØªØ´ÙˆÙŠ ÙˆØ§Ù†Øº ÙˆØ§ÙŠÙˆÙ† Ø³ÙˆÙˆ)ØŒ ÙˆÙ…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ÙŠØªÙ… Ø¥ØµØ¯Ø§Ø±Ù‡ Ø¹Ù„Ù‰ Â«Ù†Ø§ÙŠÙØ± ÙˆÙŠØ¨ØªÙˆÙˆÙ†Â» Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù…. Ù‡ÙŠ Ù†Ø§Ø´Ø± ÙˆÙ…Ù†ØµØ© Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨ Ø£Ø·Ù„Ù‚ØªÙ‡ Ø´Ø±ÙƒØ© Ù†Ø§ÙÙŠØ± ÙÙŠ ÙƒÙˆØ±ÙŠØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ© ÙÙŠ Ø¹Ø§Ù… 2004.]   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [\"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ù‡Ùˆ Ù…ØµØ·Ù„Ø­ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†ÙŠ ÙˆØ§Ù„Ø¹Ù†ÙŠÙ Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø´Ø£Ù†Ù‡ Ø£Ù† ÙŠØ³Ø¨Ø¨ Ù„Ù‡Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¶Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙŠÙŠÙ† Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ø³Ø¯ÙŠ. Ù‚Ø¯ ÙŠØªÙ… Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†Ù‹ ÙƒØ§Ù† ÙˆØ¹Ù„Ù‰ ÙŠØ¯ÙŠ Ø£ÙŠ Ø´Ø®Øµ ÙÙ‡Ùˆ Ù„Ø§ ÙŠÙ†Ø·ÙˆÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø£Ùˆ Ø³Ø¨Ø¨ ÙˆØ§Ø­Ø¯ ÙˆÙ‡Ùˆ ÙŠØ¹ØªØ¨Ø± Ø¸Ø§Ù‡Ø±Ø© Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± ÙˆÙ…Ù† Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªÙÙ‚Ù„ÙÙ‚ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© ÙˆØªØ¯ÙØ¹Ù‡Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø·Ø±Ù‚ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù…ÙØ³Ù†ÙŠÙ† ÙˆØ­Ù…Ø§ÙŠØªÙ‡Ù… Ù…Ù†Ù‡. ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù… Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø© ÙˆÙ‡Ù… Ø£Ø´Ø®Ø§Øµ ÙŠØ­ØµÙ„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù…ØªÙŠØ§Ø²Ø§Øª ÙˆØ­Ù…Ø§ÙŠØ© Ù…ÙƒØ«ÙØ© ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆØ°Ù„Ùƒ Ø¨Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø±Ù‚Ù… 26 Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆÙ‡Ùˆ Ù‚Ø§Ù†ÙˆÙ† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§ØµØ±ÙŠÙ† ÙˆØ§Ù„Ø¹Ø§Ø¬Ø²ÙŠÙ† ÙƒÙ…Ø§ ÙˆÙŠØ¹ÙˆØ¯ Ø°Ù„Ùƒ Ù„Ø¶Ø¹ÙÙ‡Ù… ÙˆÙ„ÙƒØ«Ø±Ø© Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡Ù… ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ§Ø¬Ù‡Ù‡Ø§ Ù‚Ø³Ù… ÙƒØ¨ÙŠØ± Ù…Ù†Ù‡Ù…. ÙÙŠ Ø¯ÙˆÙ„Ø© Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙŠØªÙ… Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" ÙˆØ¨ÙŠÙ† Ù…ØµØ·Ù„Ø­ \"Ø¥Ø³Ø§Ø¡Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†\" Ø­ÙŠØ« Ø£Ù†Ù‡ ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø³Ø§Ø¡Ø© Ù„ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù‚Ø¯ ØªØ¶Ù…Ù† Ø§Ù„Ø¹Ù†Ù Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªØ­Ø¯Ø« Ù…Ù† Ø´Ø®Øµ Ù…Ù‚Ø±Ø¨ Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø³Ù† Ù…Ø«Ù„ Ø§Ù„Ø£Ù‚Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø®ØµØµÙŠÙ† Ù„Ø±Ø¹Ø§ÙŠØªÙ‡Ù… ÙˆØªØ³ØªÙ…Ø± Ù„ÙØªØ±Ø© ÙˆÙ‡Ùˆ Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø§Ø¯Ø« Ø§Ù„Ø°ÙŠ ÙÙŠÙ‡ ÙŠØªÙ… Ø§Ù„ØªÙ‡Ø¬Ù… Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙƒØ¨ÙŠØ± Ø¨Ø§Ù„Ø³Ù† Ù…Ù† Ù‚Ø¨Ù„ Ø´Ø®Øµ ØºØ±ÙŠØ¨ ÙˆÙŠØ­Ø¯Ø« Ø°Ù„Ùƒ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙŠÙƒÙˆÙ† Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ø¹Ù…Ù„ Ø§Ù„Ø´Ø±Ø·Ø©. Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ± Ø£Ù† Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ØªÙ‚ÙˆÙ… Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ ÙƒÙ„ØªØ§ Ø§Ù„Ø­Ø§Ù„ØªÙŠÙ†ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ø¥Ø³Ø§Ø¡Ø© Ø§Ùˆ Ø¹Ù†ÙØŒ Ø¨Ù‡Ø¯Ù Ø§Ù„Ø­Ø¯ Ù…Ù† Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù†. Ø­Ø³Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø°ÙŠ Ø£ØµØ¯Ø±ØªÙ‡ Ø§Ù„ÙƒÙ†ÙŠØ³Øª Ø¹Ø§Ù… 2007 ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¹Ø±Ø¶ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙØ£Ù†Ù‡ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø£Ø¬Ø±ÙŠØª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø³Ù†Ø© 2006 Ø¹Ø§Ø´ Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ Ø§Ù„ 70ØŒ000 Ù…ÙØ³Ù† ÙÙŠ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ø§Ø¯Ù„ 10% Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³ÙƒØ§Ù†ØŒ 30% Ù…Ù†Ù‡Ù… Ù‡Ù… Ù†Ø§Ø¬ÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ø­Ø±Ù‚Ø© Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ© Ø§Ùˆ Ø§Ù„Ù‡ÙˆÙ„ÙˆÙƒÙˆØ³Øª ÙƒÙ…Ø§ Ø§Ù† Ø­ÙˆØ§Ù„ÙŠ 12ØŒ000 Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙŠØ¹ÙŠØ´ÙˆÙ† Ù…Ø¹ Ø´Ø®Øµ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø§ ØªØ±Ø¨Ø·Ù‡ Ù…Ø¹Ù‡Ù… ØµÙ„Ø© Ø¯Ù… (Ø¹Ø§Ø¯Ø© Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ùˆ Ù…Ø±Ø§ÙÙ‚ Ù„Ù„Ø§Ø¹ØªÙ†Ø§Ø¡ Ø¨Ù‡Ù…). Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø´Ø±Ø·Ø© ØªØªÙ„Ù‚Ù‰ Ø¹Ø¯Ø¯ Ù‡Ø§Ø¦Ù„ Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª ÙˆÙØªØ­Øª Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 12,228 Ù…Ù„Ù ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¹Ù†Ù ÙˆØ§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ù…Ù† Ø¹Ø§Ù… 2002 Ø­ØªÙ‰ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù…Ù† Ø³Ø¨ØªÙ…Ø¨Ø± Ø¹Ø§Ù… 2007 (Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ØªØªØ¶Ù…Ù† Ø§Ù„Ù‚ØªÙ„ØŒ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù‚ØªÙ„ØŒ Ø§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ÙˆØ§Ù„Ø§ØºØªØµØ§Ø¨ØŒ Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø³Ø±Ù‚Ø© ÙˆØ§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠÙˆØª ÙƒÙ…Ø§ ÙˆØªØªØ¶Ù…Ù† Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ ÙˆØ§Ù„ØªØ®Ø±ÙŠØ¨) Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙŠ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ù„Ø§ ÙŠØªØ¶Ù…Ù† Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙˆØµÙ„ÙˆØ§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø±Ø³ Ø§Ù„Ù…Ø¯Ù†ÙŠ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ù‚Ø³Ø§Ù…. Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ÙØ¥Ù†: 32% Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ù‡ÙŠ Ø¨Ù„Ø§ØºØ§Øª Ø¹Ù† Ø§Ù„Ø³Ø·Ùˆ Ø¹Ù„Ù‰ Ø¨ÙŠÙˆØªÙ‡Ù…ØŒ 17% Ù‡ÙŠ Ø¹Ù† ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ØªÙ„Ù‚ÙˆÙ‡Ø§ØŒ 31% Ø¹Ù† Ø³Ø±Ù‚Ø§Øª ØªØ¹Ø±Ø¶ÙˆØ§ Ù„Ù‡Ø§ Ø¨ÙŠÙ†Ù…Ø§ 11% Ù‡ÙŠ ÙÙ‚Ø· Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ø³Ø¯ÙŠØ©. Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ø³Ù†ÙˆØ§Øª (2002-2007) ÙƒØ§Ù† ÙŠÙ‚Ø¯Ù… ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø§Ù„Ù‚Ø§Ø¯Ù…ÙˆÙ† Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø¯ÙˆÙ„Ø© (Ø§Ù„Ù…Ù‡Ø§Ø¬Ø±ÙˆÙ†) Ø­ÙˆØ§Ù„ÙŠ 9% Ù…Ù† Ø§Ù„Ø´ÙƒØ§ÙˆÙ‰ Ø±ØºÙ… Ø£Ù† Ù†Ø³Ø¨ØªÙ‡Ù… Ù…Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ù„Ø§ ØªØªØ¹Ø¯Ù‰ 25% Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø³Ù†ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙƒØ§Ù†ÙˆØ§ ÙŠÙ‚Ø¯Ù…ÙˆÙ† 5% Ù…Ù† Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª. ÙˆÙÙ‚Ù‹Ø§ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø±ÙØ§Ù‡ ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø¥Ù† Ù„Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ Ø£Ø«Ø±Ù‹Ø§ Ø³Ù„Ø¨ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¸Ø§Ù‡Ø±Ø© Ø§Ù„Ø¹Ù†Ù Ø¶Ø¯ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† ÙÙ‚Ø¯ Ù„ÙˆØ­Ø¸ Ø§Ø±ØªÙØ§Ø¹ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ÙÙŠÙ‡Ø§ ØªØ¹Ù†ÙŠÙ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ØªÙ…Øª Ù…Ù† Ù‚Ø¨Ù„ Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙˆÙ…Ù‚Ø¯Ù…ÙŠ Ø§Ù„Ø±Ø¹Ø§ÙŠØ©) ÙØ¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø¹Ø§Ù… 2018 ØªÙ… Ø¹Ù„Ø§Ø¬ 6707 Ø­Ø§Ù„Ø© Ø¨ÙŠÙ†Ù…Ø§ Ø­ØªÙ‰ Ù…Ù†ØªØµÙ Ø¹Ø§Ù… 2019 ÙÙ‚Ø· ØªÙ… Ø¹Ù„Ø§Ø¬ Ø£ÙƒØ«Ø± Ù…Ù† 7000 Ø­Ø§Ù„Ø© ÙˆÙ‡Ø°Ø§ Ø¹Ø¯Ø§ Ø¹Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ]   \n",
            "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯. ÙŠØ¹Ø¯ Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„ØªÙƒÙ„ÙŠÙ Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø·Ø§Ù‚Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù…Ù„ Ø§Ù„ÙÙ‚Ø¯ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ Ù„Ù„ØªØ­Ù…Ù„ ÙˆØ§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø¸Ø±ÙˆÙ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©.]   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‡ÙŠ ØªØ®ØµØµ ÙŠÙ‡ØªÙ… Ø¨Ø¯Ø±Ø§Ø³Ø© ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©.\\nØ£ØµØ¨Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø±ÙˆÙØ§Ù‹ ÙÙŠ Ø£ÙˆØ§Ø®Ø± Ø§Ù„Ù‚Ø±Ù† Ø§Ù„ØªØ§Ø³Ø¹ Ø¹Ø´Ø± ÙˆØ°Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙ„ØºØ±Ø§Ù ÙˆÙ…Ø­Ø·Ø§Øª Ø¥Ù…Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù‚Ø©. ÙˆØ§Ù„Ø¢Ù† ÙŠØºØ·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¹Ø¯Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ÙØ±Ø¹ÙŠØ© ÙˆØ§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ†Ø¸Ù… Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù„Ø§Ø³Ù„ÙƒÙŠØ©.\\nÙˆÙ…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£Ù† Ù†Ù‚ÙˆÙ„ Ø£Ù† Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù‚Ø¯ ØªØªØ¶Ù…Ù† Ø£ÙŠØ¶Ø§Ù‹ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª ÙˆÙ‚Ø¯ Ù„Ø§ ØªØªØ¶Ù…Ù†Ù‡Ø§. ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ†Ù‡Ù…Ø§ Ø­ÙŠØ« ØªÙ‡ØªÙ… Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¨Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬Ù‡Ø¯ Ù…Ø«Ù„ Ù†Ù‚Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ¹Ø§Ù…Ù„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù…Ø¹ Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ù†Ø¸Ù… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (ØªÙŠØ§Ø± Ù…Ù†Ø®ÙØ¶ â€“Ø¬Ù‡Ø¯ Ù…Ù†Ø®ÙØ¶)ØŒ ÙˆÙŠØªØ¶Ù…Ù† Ø°Ù„Ùƒ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„Ø©.\\nÙˆØªØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¯Ø±Ø§Ø³Ø© ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ø¸Ù… Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø¯ÙˆØ§Ø¦Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø§ØªØŒ Ø§Ù„Ù…Ø­Ø±ÙƒØ§ØªØŒ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§ØªØŒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù‚Ø¯Ø±Ø© ØºÙŠØ± Ø§Ù„Ù…Ù†Ù‚Ø·Ø¹Ø© UPSØŒ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆÙ…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©.\\nØªØ§Ø±ÙŠØ® ÙˆØ£Ø¹Ù„Ø§Ù… Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©.\\nØ¸Ù‡Ø± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø¨Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…Ù†Ø° Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø¹Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„. ÙÙŠØ¹ØªÙ‚Ø¯ Ø£Ù† Ø£ÙˆÙ„ Ù…Ù‡Ù†Ø¯Ø³ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù‡Ùˆ ÙˆÙ„ÙŠØ§Ù… Ø¬Ù„Ø¨Ø±Øª Ø§Ù„Ø°ÙŠ ØµÙ…Ù… Ø¢Ù„Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø°Ø§Øª Ø§Ù„Ø´Ø­Ù†Ø§Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©. ÙˆÙ‡Ùˆ Ù…Ù† ÙØ±ÙŽÙ‘Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø§ÙƒÙ†Ø©ØŒ ÙƒÙ…Ø§ ÙŠØ¹ØªÙ‚Ø¯ Ø¨Ø£Ù†Ù‡ Ø£ÙˆÙ„ Ù…Ù† Ø£Ù†Ø´Ø£ Ù…ØµØ·Ù„Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.\\nÙˆÙÙŠ Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ù…Ø± ÙƒØ§Ù†Øª ÙƒÙ„ Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª ÙˆØ§Ù„Ø§Ø®ØªØ±Ø§Ø¹Ø§Øª ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø´Ø­Ù†Ø©. ÙˆØ¨Ø¯Ø£ ÙØµÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø¹Ù† Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ ÙÙŠ Ø²Ù…Ù† ØªÙˆÙ…Ø§Ø³ Ø§Ø¯ÙŠØ³ÙˆÙ† ÙˆÙÙŠØ±Ù†Ø± ÙÙˆÙ† Ø³ÙŠÙ…Ù†Ø³. ÙˆÙÙŠ Ø¹Ø§Ù… 1752 Ø§Ø®ØªØ±Ø¹ Ø¨ÙŠÙ†ÙŠØ§Ù…ÙŠÙ† ÙØ±Ø§Ù†ÙƒÙ„ÙŠÙ† Ù…ÙˆØµÙ„Ø© Ø§Ù„ØµÙˆØ§Ø¹Ù‚ ÙˆÙ†Ø´Ø± Ø¨ÙŠÙ† 1751 Ùˆ1753 Ù†ØªØ§Ø¦Ø¬ ØªØ¬Ø§Ø±Ø¨Ù‡ ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† Â«ØªØ¬Ø§Ø±Ø¨ ÙˆÙ…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ù† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡Â» (Experiments and Observations on Electricity). ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1800 Ù‚Ø§Ù… Ø£Ù„Ø³Ø§Ù†Ø¯Ø±Ùˆ ÙÙˆÙ„ØªØ§ Ø¨Ø¨Ù†Ø§Ø¡ Ø¨Ø·Ø§Ø±ÙŠØªÙ‡ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù…Ø³Ù…Ø§Ø© Â«Ø¹Ù…ÙˆØ¯ ÙÙˆÙ„ØªØ§Â» Ø¨Ø¹Ø¯ Ø§Ø¹Ø¬Ø§Ø¨Ù‡ Ø¨ØªØ¬Ø±Ø¨Ø© Ø§Ø¬Ø±Ø§Ù‡Ø§ Ù„ÙˆÙŠØ¬ÙŠ Ø¬Ø§Ù„ÙØ§Ù†ÙŠ Ø¹Ø§Ù… 1792. ÙÙŠ Ø§Ù„Ø¹Ø§Ù… 1820 Ù‚Ø§Ù… Ù‡Ø§Ù†Ø² ÙƒØ±ÙŠØ³ØªÙŠØ§Ù† Ø§ÙˆØ±Ø³ØªØ¯ Ø¨Ø¹Ù…Ù„ ØªØ¬Ø§Ø±Ø¨ Ø¹Ù† Ø§Ù†Ø­Ù†Ø§Ø¡ Ø§Ø¨Ø±Ø© Ø§Ù„Ø¨ÙˆØµÙ„Ø© Ø¨ØªØ§Ø«ÙŠØ± Ø§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ. ÙˆÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¹Ø§Ù… ÙƒØ±Ø± Ø§Ù†Ø¯Ø±ÙŠÙ‡ Ù…Ø§Ø±ÙŠ Ø§Ù…Ø¨ÙŠØ± ØªÙ„Ùƒ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ø«Ø¨Øª Ø§Ù† Ø³Ù„ÙƒÙŠÙ† ÙŠÙ…Ø± ÙÙŠÙ‡Ù…Ø§ Ø§Ù„ØªÙŠØ§Ø± ÙŠØ¤Ø«Ø±Ø§Ù† Ø¨Ù‚ÙˆÙ‰ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶ ÙˆØ¹Ø±Ù Ø®Ù„Ø§Ù„Ù‡Ø§ Ø§Ù„Ø¬Ù‡Ø¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ.\\nÙ…Ø§ÙŠÙƒÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ (ÙŠÙ†Ø·Ù‚ Ø£ÙŠØ¶Ø§ Ù…ÙŠØ´ÙŠÙ„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ) Ù‚Ø¯Ù… Ø£Ø¹Ù…Ø§Ù„ ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙÙŠØ¶ÙŠÙ† Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠ ÙˆØ§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØŒ ÙˆØ¹Ø±Ù Ø£ÙŠØ¶Ø§ Ø®Ø·ÙˆØ· Ø§Ù„Ù…Ø¬Ø§Ù„. ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø§Ù„ ÙØ§Ø±Ø§Ø¯Ø§ÙŠ Ù‚Ø¯Ù… Ø¬ÙŠÙ…Ø³ ÙƒÙ„ÙŠØ±Ùƒ Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ Ø£Ø¹Ù…Ø§Ù„Ø§ ÙÙŠ Ø¥ÙƒÙ…Ø§Ù„ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆØ¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©Ù€ ÙˆÙ‚Ø¯Ù… Ø¹Ø§Ù… 1864 Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…Ø§ÙƒØ³ÙˆÙŠÙ„ ÙˆØ§Ù„ØªÙŠ ØªØ¹ØªØ¨Ø± Ø£Ø­Ø¯ Ø£Ù‡Ù… Ø£Ø³Ø³ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨ÙŠØ©.\\nØªØ·ÙˆØ± Ù…Ø¬Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¶ÙˆØ¡ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø£Ø¯Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø°ÙˆÙŠ ØªØ¬Ø±Ø¨Ø©. ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙØªØ±Ø© ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ù‡Ù„Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ®ØµØµØ§Øª Ø¨Ø£Ø·Ø± Ø£Ø¹Ù…Ø§Ù„ Ù‡Ù†Ø¯Ø³ÙŠØ©. Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù‡Ù„Øª Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø¨Ù†Ø§Ø¡ ÙˆÙ…Ù‡Ù†Ø¯Ø³Ùˆ Ù…Ø§ÙƒÙŠÙ†Ø§Øª Ù„ÙƒÙ†Ù‡Ø§ ÙØ´Ù„Øª Ø¨Ø§Ù† ØªØ¤Ù‡Ù„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¹Ù„Ù‰ Ø¶ÙˆØ¡ ØªØ³Ø§Ø±Ø¹ Ø§Ù„Ù…Ø¬Ø§Ù„ØŒ ÙˆÙ„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ù‡Ù†Ø¯Ø³Ùˆ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙŠØ¯Ø±Ø³ÙˆÙ† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù„Ø·Ù„Ø§Ø¨ Ù‡Ù†Ø¯Ø³Ù‡ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.Â \\nØ¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ù…Ù‡Ù…Ø© ØªØ¹Ù„ÙŠÙ… ÙˆØªØ£Ù‡ÙŠÙ„ Ù…Ù‡Ù†Ø¯Ø³ÙˆØ§ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆÙ‚Ø¹Øª Ø¹Ù„Ù‰ Ø¹Ø§ØªÙ‚ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠÙ†ØŒ Ø­ÙŠØ« Ø§Ù†Ù‡ Ù„Ù… ÙŠÙƒÙ† Ù„Ø§Ø­Ø¯ Ø§Ø®Ø± Ø§Ù„Ø¹Ù„Ù… Ø¨Ù…Ø¬Ø§Ù„ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙˆØ§Ù„Ù‚Ø¯Ø±Ø© Ù„ØªÙˆØµÙŠÙ„Ù‡ Ù„Ø§ÙƒØ¨Ø± Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨.\\n]   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                response  \\\n",
            "0                                                              Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø³ÙŠØ® Ù‡ÙŠ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1479 Ø¥Ù„Ù‰ 1516. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1516 Ø¥Ù„Ù‰ 1568. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1568 Ø¥Ù„Ù‰ 1700. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1700 Ø¥Ù„Ù‰ 1808. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1808 Ø¥Ù„Ù‰ 1833. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1833 Ø¥Ù„Ù‰ 1868. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1868 Ø¥Ù„Ù‰ 1870. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1870 Ø¥Ù„Ù‰ 1873. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1873 Ø¥Ù„Ù‰ 1874. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1874 Ø¥Ù„Ù‰ 1876. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1876 Ø¥Ù„Ù‰ 1877. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1877 Ø¥Ù„Ù‰ 1878. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1878 Ø¥Ù„Ù‰ 1880. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1880 Ø¥Ù„Ù‰ 1881. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1881 Ø¥Ù„Ù‰ 1882. ÙˆØªÙ…Øª Ø¥Ø¯Ø§Ø±ØªÙ‡Ø§ Ù…Ù† 1882 Ø¥Ù„Ù‰ 1883. Ùˆ   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ù„Ø³Ù„  \"ØµÙŠÙÙ†Ø§ Ø§Ù„Ø­Ø¨ÙŠØ¨ ØŸ\" ÙÙŠ  2023Ù….   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙ Ø¯ÙˆÙ„Ø© Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ù‡Ù…Ø§ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ØªØªØ±Ø§ÙˆØ­ Ø£Ø¹Ù…Ø§Ø±Ù‡Ù… Ø¨ÙŠÙ† 65 Ùˆ 100 Ø¹Ø§Ù….   \n",
            "3                                                                                                                                                                                                                                                                                                                        Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø·Ø§Ù‚Ø© ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ø£Ùˆ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©ØŒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©. ÙŠØ¹ØªØ¨Ø± Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù…Ù„ Ù†Ø¸Ø§Ù…Ù‹Ø§ Ù…ØªÙ‚Ø¯Ù…Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø§Ù‚Ø©ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ²ÙŠØ¯ Ù…Ù† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© ÙˆØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©.   \n",
            "4   Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù‡ÙŠ Ø£Ø¯ÙˆØ§Øª ØªØªØ¨Ø¹ ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† ÙˆØª   \n",
            "\n",
            "                                                                                                                                 reference  \\\n",
            "0  ÙƒØ´Ù…ÙŠØ± (Ø£Ù‚ØµÙ‰ Ø´Ù…Ø§Ù„ Ø´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©)ØŒ ÙˆÙ„Ø¯Ø§Ø® (Ø¥Ù‚Ù„ÙŠÙ… Ø§ØªØ­Ø§Ø¯ÙŠ ØªØ¯ÙŠØ±Ù‡ Ø§Ù„Ù‡Ù†Ø¯)ØŒ ÙˆØ¨ÙŠØ´Ø§ÙˆØ± (Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…Ù‚Ø§Ø·Ø¹Ø© Ø®ÙŠØ¨Ø± Ø¨Ø®ØªÙˆÙ†Ø®ÙˆØ§ Ø§Ù„Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§)   \n",
            "1                                                                                                                         Ù…Ù† 6 Ø¯ÙŠØ³Ù…Ø¨Ø± 2021   \n",
            "2                                                                                                              Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ø¹Ù…Ø±Ù‡Ù… Ø§Ù„ 65 Ø³Ù†Ø©   \n",
            "3                                            Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ­Ù…Ù„ Ø§Ù„Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ø°ÙŠ ÙŠØªØ³Ø¨Ø¨ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© ØªÙˆØ§ØªØ± Ù…Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯   \n",
            "4                                                                                                                                        .   \n",
            "\n",
            "   faithfulness  answer_relevancy  context_precision  context_recall  \\\n",
            "0           NaN          0.621423                NaN             NaN   \n",
            "1           0.0          0.693441                NaN             NaN   \n",
            "2           0.0               NaN                NaN             NaN   \n",
            "3           NaN          0.455248                0.0             NaN   \n",
            "4           NaN          0.644414                NaN             NaN   \n",
            "\n",
            "   answer_correctness  \n",
            "0                 NaN  \n",
            "1                 NaN  \n",
            "2                 NaN  \n",
            "3                 NaN  \n",
            "4                 NaN  \n"
          ]
        }
      ],
      "source": [
        "# ragas_df = result.to_pandas() # this is already overall_df\n",
        "# Displaying the overall_df which contains per-question scores\n",
        "print(overall_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6dfea3c-b664-4b41-a5a6-5e2c19c8afb7",
      "metadata": {
        "id": "d6dfea3c-b664-4b41-a5a6-5e2c19c8afb7"
      },
      "source": [
        "## Bar Chart + Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "35b343f3-e007-47fc-a948-1422ee7bbcfc",
      "metadata": {
        "id": "35b343f3-e007-47fc-a948-1422ee7bbcfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "13c173bf-85cf-4b51-e678-f6a08c987a8b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'metric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'metric'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2037490980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metric\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#4C72B0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RAGAS Metrics Overview\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'metric'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(ragas_df[\"metric\"], ragas_df[\"value\"], color=\"#4C72B0\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"RAGAS Metrics Overview\", fontsize=16)\n",
        "plt.ylabel(\"Score\", fontsize=14)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ragas_bar_chart.png\", dpi=400)\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ” Saved: ragas_bar_chart.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8795eb96-1352-4224-a265-b19825d2bbbb",
      "metadata": {
        "id": "8795eb96-1352-4224-a265-b19825d2bbbb"
      },
      "source": [
        "## Line Chart + Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b8d35baa-cc46-43ce-a927-b29bc8e0efbc",
      "metadata": {
        "id": "b8d35baa-cc46-43ce-a927-b29bc8e0efbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "379ede41-39e3-4850-c9b3-1badffa450a8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'metric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'metric'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-277172463.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metric\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#55A868\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RAGAS Metrics Trend\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'metric'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(overall_df[\"metric\"], overall_df[\"value\"], marker=\"o\", linewidth=2, color=\"#55A868\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"RAGAS Metrics Trend\", fontsize=16)\n",
        "plt.ylabel(\"Score\", fontsize=14)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ragas_line_chart.png\", dpi=400)\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ” Saved: ragas_line_chart.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9946037-c886-4923-b149-a4247a86f11f",
      "metadata": {
        "id": "d9946037-c886-4923-b149-a4247a86f11f"
      },
      "source": [
        "## Radar Chart (Spider Plot) + Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9790a61e-b38d-4b02-839b-06f418b2457c",
      "metadata": {
        "id": "9790a61e-b38d-4b02-839b-06f418b2457c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "18ee6e72-0082-401b-9d81-20d66f0a6a60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'metric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'metric'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3259337563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metric\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'metric'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = overall_df[\"metric\"].tolist()\n",
        "values = overall_df[\"value\"].tolist()\n",
        "\n",
        "# Close the circle\n",
        "values += values[:1]\n",
        "angles = np.linspace(0, 2*np.pi, len(values))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "\n",
        "ax.plot(angles, values, linewidth=2, color=\"#C44E52\")\n",
        "ax.fill(angles, values, alpha=0.25, color=\"#C44E52\")\n",
        "\n",
        "ax.set_thetagrids(angles[:-1] * 180/np.pi, metrics)\n",
        "ax.set_title(\"RAGAS Radar Chart\", fontsize=16)\n",
        "ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ragas_radar_chart.png\", dpi=400)\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ” Saved: ragas_radar_chart.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49432dcc-a2a0-4882-8ef7-1e523f3876ea",
      "metadata": {
        "id": "49432dcc-a2a0-4882-8ef7-1e523f3876ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c0e7da-46f7-48aa-9d22-77aa72cbc9d5",
      "metadata": {
        "id": "95c0e7da-46f7-48aa-9d22-77aa72cbc9d5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15eabfae8b5f4f66860fc8315d697d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_91edbef68eb34676a4b1c2113b207292"
          }
        },
        "aa831a17faf745a697dfdb99464112df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77c50ab999846e9bcf645117039f116",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bc85590f5d9941eb96c5bdb6773dda0a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "df74d8aadad1439fbb62b7950d14d581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8a22f4dc541d43dbab55ad6922423c23",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_695093a2ef9342e59192506d89b2c03b",
            "value": ""
          }
        },
        "2d5089806ae64af49a3f4b4eb890bca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_e4a5fed5a95e4aeeb232a93a4da22661",
            "style": "IPY_MODEL_aaeb675240224f379e336c79f64787f6",
            "value": true
          }
        },
        "c79129143ee24c0e8c744949ff52f4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_804ca7971e514a8a91d2966b36753535",
            "style": "IPY_MODEL_1bb00b2b25f74c00b42ac36352ba54a9",
            "tooltip": ""
          }
        },
        "0febab2713e143898bb5fa1e19067c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cdf3e9ee3ea467cbba997bcf04a1654",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_911ba697dd77497ebc75c4abf0ea888b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "91edbef68eb34676a4b1c2113b207292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c77c50ab999846e9bcf645117039f116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc85590f5d9941eb96c5bdb6773dda0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a22f4dc541d43dbab55ad6922423c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "695093a2ef9342e59192506d89b2c03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a5fed5a95e4aeeb232a93a4da22661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaeb675240224f379e336c79f64787f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "804ca7971e514a8a91d2966b36753535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb00b2b25f74c00b42ac36352ba54a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1cdf3e9ee3ea467cbba997bcf04a1654": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "911ba697dd77497ebc75c4abf0ea888b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "246ead667b7b4f8d903bbcd1a34daba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8615b9165174e01af76f346106dae2a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f6a3c4618fd049db9c15b217fecce8d8",
            "value": "Connecting..."
          }
        },
        "a8615b9165174e01af76f346106dae2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a3c4618fd049db9c15b217fecce8d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca2efc23701466bafa40c10f68c903f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5829965ac8d41e18f8aea6f20f34cf5",
              "IPY_MODEL_ac38cadeb5c247d58815b692baef1ff3",
              "IPY_MODEL_b4cb0d3f9f84482fab0dbeea0e3dacbe"
            ],
            "layout": "IPY_MODEL_81399104933e47d7b5daad44eec270bc"
          }
        },
        "f5829965ac8d41e18f8aea6f20f34cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f8f168741641bea4338cac597740a4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d8344e6e031145279248afc29e96e77d",
            "value": "README.md:â€‡"
          }
        },
        "ac38cadeb5c247d58815b692baef1ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3464cf19999a4cf3a6099933f7ce0bee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daac160ba1fe4fcfb04460b43e9975a3",
            "value": 1
          }
        },
        "b4cb0d3f9f84482fab0dbeea0e3dacbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b440082ea33409aabf014e3dc0fa6fe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6626c17c121d4f288cf6ea0b7891d99d",
            "value": "â€‡1.45k/?â€‡[00:00&lt;00:00,â€‡152kB/s]"
          }
        },
        "81399104933e47d7b5daad44eec270bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43f8f168741641bea4338cac597740a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8344e6e031145279248afc29e96e77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3464cf19999a4cf3a6099933f7ce0bee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "daac160ba1fe4fcfb04460b43e9975a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b440082ea33409aabf014e3dc0fa6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6626c17c121d4f288cf6ea0b7891d99d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "276e1787033e4adca799af84406b0496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64403210bcbc49759b75cd7faeab9919",
              "IPY_MODEL_93e80b6099ec4eb4b4e9c6176c606934",
              "IPY_MODEL_c655c4dd2fe74302951138408999195a"
            ],
            "layout": "IPY_MODEL_07a2b827d22447e0bd6631a2c7bfdc0b"
          }
        },
        "64403210bcbc49759b75cd7faeab9919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b7283351e084f8ab852f6c9c2a2b10e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b410a344553a4b55968591ed4985434b",
            "value": "MRC/test.json:â€‡100%"
          }
        },
        "93e80b6099ec4eb4b4e9c6176c606934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fffe1fd7922b46a6a5cfd1f009bad314",
            "max": 19236287,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_561836cb8b194f37b80b40bd6326d7b4",
            "value": 19236287
          }
        },
        "c655c4dd2fe74302951138408999195a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4b0041365cf4287ad2c94977368741a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_978209e344ca45e3bffedab613a9c39a",
            "value": "â€‡19.2M/19.2Mâ€‡[00:01&lt;00:00,â€‡13.8MB/s]"
          }
        },
        "07a2b827d22447e0bd6631a2c7bfdc0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7283351e084f8ab852f6c9c2a2b10e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b410a344553a4b55968591ed4985434b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fffe1fd7922b46a6a5cfd1f009bad314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "561836cb8b194f37b80b40bd6326d7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4b0041365cf4287ad2c94977368741a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978209e344ca45e3bffedab613a9c39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1189009805f54c5db282cb2cd793b3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_189b1432533743d6978a7e43294db82c",
              "IPY_MODEL_8cb6017a5f1f4365b4080a9b0a77ae9b",
              "IPY_MODEL_3fdfc39d23d047e6becdde184b030044"
            ],
            "layout": "IPY_MODEL_add2bc87ee4d44d1a3b75188b56f4b70"
          }
        },
        "189b1432533743d6978a7e43294db82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11cab03a451a43c2b02433dfc8923854",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d7df181fba1c45caa9b234d06f961d14",
            "value": "Generatingâ€‡testâ€‡split:â€‡"
          }
        },
        "8cb6017a5f1f4365b4080a9b0a77ae9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f35c504ebe1940b19b1b8ac3edd759d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f34a3bf099664526963205d30e9ccdf3",
            "value": 1
          }
        },
        "3fdfc39d23d047e6becdde184b030044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4648748534d74556b63b2bd4bff42cd7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_46515601138e4b96b845e367f89e3c37",
            "value": "â€‡2108/0â€‡[00:00&lt;00:00,â€‡2687.25â€‡examples/s]"
          }
        },
        "add2bc87ee4d44d1a3b75188b56f4b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11cab03a451a43c2b02433dfc8923854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7df181fba1c45caa9b234d06f961d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35c504ebe1940b19b1b8ac3edd759d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f34a3bf099664526963205d30e9ccdf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4648748534d74556b63b2bd4bff42cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46515601138e4b96b845e367f89e3c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10563c75a606450a875e740ea35eb318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42b298b735a64f8ca22bfb15044d1efc",
              "IPY_MODEL_d0b79e3642df4640b9fc3306f7a01383",
              "IPY_MODEL_4a1f71372b5643748a55c41abcdf9a13"
            ],
            "layout": "IPY_MODEL_20558a96219a45b693768956827335c0"
          }
        },
        "42b298b735a64f8ca22bfb15044d1efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5bbe01aa3ae49c385a8626d48f87c39",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_87c35e9000354d3b9017a85637971c0e",
            "value": "modules.json:â€‡100%"
          }
        },
        "d0b79e3642df4640b9fc3306f7a01383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86590d32648848b3b59f59ffdce8b110",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_462b60cbb0214116b80d431bd64952f6",
            "value": 349
          }
        },
        "4a1f71372b5643748a55c41abcdf9a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ffe4cf1b96469380c0dddb2b7c59b5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3c93a5f1969947738b17780392f851d7",
            "value": "â€‡349/349â€‡[00:00&lt;00:00,â€‡40.1kB/s]"
          }
        },
        "20558a96219a45b693768956827335c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5bbe01aa3ae49c385a8626d48f87c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87c35e9000354d3b9017a85637971c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86590d32648848b3b59f59ffdce8b110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "462b60cbb0214116b80d431bd64952f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00ffe4cf1b96469380c0dddb2b7c59b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c93a5f1969947738b17780392f851d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9e4b144fbf74e14b33c62fba302ef97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7827acb88c504301987cffc8bab7a371",
              "IPY_MODEL_7afddf68cdce4514b269473f0cf8d350",
              "IPY_MODEL_a78a60405da64fa5947c83a07c3ff98d"
            ],
            "layout": "IPY_MODEL_e81d486013be436eac613e9058aa2232"
          }
        },
        "7827acb88c504301987cffc8bab7a371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5a9717d523b4809988843cef40163c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e6aa2770bc7c4f76b8226f42cc2b686c",
            "value": "config_sentence_transformers.json:â€‡100%"
          }
        },
        "7afddf68cdce4514b269473f0cf8d350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea38b592774141deb22a78654cdb0940",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1eb8721a704a42cdbcb03977aa08adad",
            "value": 116
          }
        },
        "a78a60405da64fa5947c83a07c3ff98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54c02f2a708e43ca90c78cb16d14e432",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7ad5a6379f2d42ce8a74a8322e069ab2",
            "value": "â€‡116/116â€‡[00:00&lt;00:00,â€‡13.9kB/s]"
          }
        },
        "e81d486013be436eac613e9058aa2232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a9717d523b4809988843cef40163c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6aa2770bc7c4f76b8226f42cc2b686c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea38b592774141deb22a78654cdb0940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eb8721a704a42cdbcb03977aa08adad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54c02f2a708e43ca90c78cb16d14e432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad5a6379f2d42ce8a74a8322e069ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d42d0d040688495193b7d4177ae7e5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f12a85cc657d450ea612b94dc0d96a83",
              "IPY_MODEL_4992c383e42348b6bd659cf9230224d1",
              "IPY_MODEL_2bde0f3320044da48461406e9a21d29a"
            ],
            "layout": "IPY_MODEL_b610289fc687407e956d372f52be093d"
          }
        },
        "f12a85cc657d450ea612b94dc0d96a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc5a231f0cf4b629bd3ece963ac73b9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5d2d43b6f2224965b2956106933116ef",
            "value": "README.md:â€‡"
          }
        },
        "4992c383e42348b6bd659cf9230224d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d0fc64fa5ed44c394e277feca927b7b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13dd14803828466e94f111d523792829",
            "value": 1
          }
        },
        "2bde0f3320044da48461406e9a21d29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afa98715420c4da5930e72a4aa884c7f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3d178bd37e6e4adda5a8ecb5c68db2e7",
            "value": "â€‡10.5k/?â€‡[00:00&lt;00:00,â€‡1.09MB/s]"
          }
        },
        "b610289fc687407e956d372f52be093d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc5a231f0cf4b629bd3ece963ac73b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d2d43b6f2224965b2956106933116ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d0fc64fa5ed44c394e277feca927b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "13dd14803828466e94f111d523792829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afa98715420c4da5930e72a4aa884c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d178bd37e6e4adda5a8ecb5c68db2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0b18ea273734bc3b0ca84515851e5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb4b438d28ea4c8499b84c528971d109",
              "IPY_MODEL_5d7e27e98d9d466381399b2493b80c12",
              "IPY_MODEL_dc3bc90ebef545098ac357b56a460a62"
            ],
            "layout": "IPY_MODEL_95bb729f3b904924992136920ef4e253"
          }
        },
        "fb4b438d28ea4c8499b84c528971d109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7267ec223d3b4b039289b6adcd817a1b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e572086b64624906a7e47c67b2901d81",
            "value": "sentence_bert_config.json:â€‡100%"
          }
        },
        "5d7e27e98d9d466381399b2493b80c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55eb1a4141f940b49476fa4803c7e21e",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7740b715c9aa43b7b6b976b78d75ed24",
            "value": 53
          }
        },
        "dc3bc90ebef545098ac357b56a460a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89e853ae292144aea45056e3fd8bf464",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5b1764879a854b1597781ab33dab01f4",
            "value": "â€‡53.0/53.0â€‡[00:00&lt;00:00,â€‡4.66kB/s]"
          }
        },
        "95bb729f3b904924992136920ef4e253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7267ec223d3b4b039289b6adcd817a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e572086b64624906a7e47c67b2901d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55eb1a4141f940b49476fa4803c7e21e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7740b715c9aa43b7b6b976b78d75ed24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89e853ae292144aea45056e3fd8bf464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1764879a854b1597781ab33dab01f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93bf2f6dafc64faf98082a8dcc324cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f0b51f388b04b378d886ce9114e5a18",
              "IPY_MODEL_72680f0f8f9b4071901d0021fc3b08d6",
              "IPY_MODEL_0f8f3cd8c80c43ca9300bbcaf3302126"
            ],
            "layout": "IPY_MODEL_6d2f7d718d9840948bbc896d07159f84"
          }
        },
        "9f0b51f388b04b378d886ce9114e5a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c59158468b749d8baf3e6d758ed5023",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d42a355564e14367a07d5bc66441ff07",
            "value": "config.json:â€‡100%"
          }
        },
        "72680f0f8f9b4071901d0021fc3b08d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abfece9a7cf3400680b693ea329cdc32",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_422443d3fc2944bc9c50595e4c4f6646",
            "value": 612
          }
        },
        "0f8f3cd8c80c43ca9300bbcaf3302126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be2f0a6e71ba472399a4d0bc548ba904",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ac60a985cd4f4f118b93a2cc01189a46",
            "value": "â€‡612/612â€‡[00:00&lt;00:00,â€‡78.8kB/s]"
          }
        },
        "6d2f7d718d9840948bbc896d07159f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c59158468b749d8baf3e6d758ed5023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d42a355564e14367a07d5bc66441ff07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abfece9a7cf3400680b693ea329cdc32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "422443d3fc2944bc9c50595e4c4f6646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be2f0a6e71ba472399a4d0bc548ba904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac60a985cd4f4f118b93a2cc01189a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42723c8b89004de3aa2503a00a757b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cab57d0a146245b49ca5cf174e17d307",
              "IPY_MODEL_fc8c7db507c04f809e89e45f8a46b3bf",
              "IPY_MODEL_583913ea82264e2daa8da6a8f0e4af06"
            ],
            "layout": "IPY_MODEL_2e33ccd50cfc48e98d82d091b4e734ca"
          }
        },
        "cab57d0a146245b49ca5cf174e17d307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71bf7535a9a24070905026e32e56fe90",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2eea999b096b49dea1c431ef551b7cc2",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "fc8c7db507c04f809e89e45f8a46b3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b2c2fa1e154c238e6ed31952bdf996",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce783044e98a4375a9c3e63ecbc3de29",
            "value": 90868376
          }
        },
        "583913ea82264e2daa8da6a8f0e4af06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f55eb2ddd4e94b8794e92b8fdca794c8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_15bb56cd237648f39d2a21c29a8ed50b",
            "value": "â€‡90.9M/90.9Mâ€‡[00:00&lt;00:00,â€‡155MB/s]"
          }
        },
        "2e33ccd50cfc48e98d82d091b4e734ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71bf7535a9a24070905026e32e56fe90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eea999b096b49dea1c431ef551b7cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50b2c2fa1e154c238e6ed31952bdf996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce783044e98a4375a9c3e63ecbc3de29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f55eb2ddd4e94b8794e92b8fdca794c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bb56cd237648f39d2a21c29a8ed50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95e7622a4cec461ebc086569e388e41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b78dcf5d38cf4358b00a38e8cf638548",
              "IPY_MODEL_56d4213688be46fbb3094276e7e9e917",
              "IPY_MODEL_888f82537fb94963935bc7908907e39f"
            ],
            "layout": "IPY_MODEL_840fa17466364b45894c02b78cb28bb7"
          }
        },
        "b78dcf5d38cf4358b00a38e8cf638548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f22bcda7374b48008e0c5a9d5a4c7edd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_77cde7d04cad4254965c9b6bd3d23afb",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "56d4213688be46fbb3094276e7e9e917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5fc92af6929467c8b2dcd9f6f5e75d3",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb50f6ebaf9f40e88a5257f42bbc6e19",
            "value": 350
          }
        },
        "888f82537fb94963935bc7908907e39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8693452d66034eca807fc2171967212a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_675b607c09fc417ba080bd98d5c52b5b",
            "value": "â€‡350/350â€‡[00:00&lt;00:00,â€‡44.3kB/s]"
          }
        },
        "840fa17466364b45894c02b78cb28bb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22bcda7374b48008e0c5a9d5a4c7edd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77cde7d04cad4254965c9b6bd3d23afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5fc92af6929467c8b2dcd9f6f5e75d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb50f6ebaf9f40e88a5257f42bbc6e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8693452d66034eca807fc2171967212a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675b607c09fc417ba080bd98d5c52b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd273561337a4a099acdd46659a17dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f367852a02e44b7baa2b38a203fad414",
              "IPY_MODEL_5e3eae4284884fa4980e37ed1ad30ed5",
              "IPY_MODEL_0517a408ef42459d908a3b4259dd2398"
            ],
            "layout": "IPY_MODEL_8de8bccd26ad4e9fbab4f465729f7a27"
          }
        },
        "f367852a02e44b7baa2b38a203fad414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131cc16fba5a42d7b864aefb800d3ae6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ac582cf376de4077970abd0e01955ce1",
            "value": "vocab.txt:â€‡"
          }
        },
        "5e3eae4284884fa4980e37ed1ad30ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2305adf72d554299ac652cd497940b27",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68d714f3a8f448afb0002b523e0735ea",
            "value": 1
          }
        },
        "0517a408ef42459d908a3b4259dd2398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7ffaa6e88f74717a06d5c961be2a9d2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ae2110a8c32b45f69cb97493b0faa6f6",
            "value": "â€‡232k/?â€‡[00:00&lt;00:00,â€‡20.9MB/s]"
          }
        },
        "8de8bccd26ad4e9fbab4f465729f7a27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131cc16fba5a42d7b864aefb800d3ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac582cf376de4077970abd0e01955ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2305adf72d554299ac652cd497940b27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "68d714f3a8f448afb0002b523e0735ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7ffaa6e88f74717a06d5c961be2a9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2110a8c32b45f69cb97493b0faa6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bda54c4ec35b480bb3857977205dac0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f67a3c8c2d3349ebb092e54a42883d1b",
              "IPY_MODEL_a8d20d7b755f49d286139da56acb2b5a",
              "IPY_MODEL_18cb2616ac6f4d5a96d0212f6762b081"
            ],
            "layout": "IPY_MODEL_5627259f679f48d2a8991b388967624d"
          }
        },
        "f67a3c8c2d3349ebb092e54a42883d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4afc13f33cc1471d97762587f1f28a90",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b7c5ffc16ca14acb8faee3c265f6b00c",
            "value": "tokenizer.json:â€‡"
          }
        },
        "a8d20d7b755f49d286139da56acb2b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fda69e439f748c0ac030c8d6eeca29a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e7b948ddb994afca11efabab6e2e825",
            "value": 1
          }
        },
        "18cb2616ac6f4d5a96d0212f6762b081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5500c6a27aa946dab742bfabbd35a8c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a0d5b8a940a24135b3a9c5d4586b4a8c",
            "value": "â€‡466k/?â€‡[00:00&lt;00:00,â€‡34.2MB/s]"
          }
        },
        "5627259f679f48d2a8991b388967624d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4afc13f33cc1471d97762587f1f28a90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c5ffc16ca14acb8faee3c265f6b00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fda69e439f748c0ac030c8d6eeca29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7e7b948ddb994afca11efabab6e2e825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5500c6a27aa946dab742bfabbd35a8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0d5b8a940a24135b3a9c5d4586b4a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "906c41b7e690427b9dece97f22fa70ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4ffa1cc686a489c94bf7e8da1b71ba3",
              "IPY_MODEL_3edd8ec71a494d83aeb52dc8c9bc1021",
              "IPY_MODEL_732f5e35f3104e5f8f41cf103f845195"
            ],
            "layout": "IPY_MODEL_38f703000f154bb0b6f3f06a5fb02801"
          }
        },
        "f4ffa1cc686a489c94bf7e8da1b71ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21379d3cf4e4aacac8abfe8ada77661",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_782f5079c2a04b5f871c61bbc1ef3714",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "3edd8ec71a494d83aeb52dc8c9bc1021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4758750fbc6492c84cdacc21da986d3",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebc7093d9e684c799f0a788fe54972f1",
            "value": 112
          }
        },
        "732f5e35f3104e5f8f41cf103f845195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f601c5306704e5bbb8aebcde2aa1358",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d5a749b7576c42d88edade4ac192ab1a",
            "value": "â€‡112/112â€‡[00:00&lt;00:00,â€‡14.2kB/s]"
          }
        },
        "38f703000f154bb0b6f3f06a5fb02801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d21379d3cf4e4aacac8abfe8ada77661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "782f5079c2a04b5f871c61bbc1ef3714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4758750fbc6492c84cdacc21da986d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc7093d9e684c799f0a788fe54972f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f601c5306704e5bbb8aebcde2aa1358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a749b7576c42d88edade4ac192ab1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e231e0bd5f1947a7ac9d7664b09fa842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f27cbfbb85034276b6b261eb2ff91809",
              "IPY_MODEL_1535b6e133604ea29a564bb1d99a4299",
              "IPY_MODEL_05fe2bfaf68244a98fa3a75f415457c5"
            ],
            "layout": "IPY_MODEL_c493783657da43bf8ae5c904fc7668cd"
          }
        },
        "f27cbfbb85034276b6b261eb2ff91809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48c41116f8c14ea2bde0497f3113663e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ab976ec6901f4a1989cf0bcb91195e9d",
            "value": "config.json:â€‡100%"
          }
        },
        "1535b6e133604ea29a564bb1d99a4299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e83441f2556540468ae87ca89f92406b",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de507e9fcb7f4bfa98ea1f8525b08c0f",
            "value": 190
          }
        },
        "05fe2bfaf68244a98fa3a75f415457c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116f21794d4b42e4bccce037d6073b3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a6a09d0651b24639ac8371bbd028f432",
            "value": "â€‡190/190â€‡[00:00&lt;00:00,â€‡25.3kB/s]"
          }
        },
        "c493783657da43bf8ae5c904fc7668cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c41116f8c14ea2bde0497f3113663e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab976ec6901f4a1989cf0bcb91195e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e83441f2556540468ae87ca89f92406b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de507e9fcb7f4bfa98ea1f8525b08c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "116f21794d4b42e4bccce037d6073b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6a09d0651b24639ac8371bbd028f432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "451bf55ca4f44f14a63b4c5c380388e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a7787eb416846eda265d65e60fae4c7",
              "IPY_MODEL_f94900f5bcbf486b802473c833732036",
              "IPY_MODEL_ba96c6d95ef34a359fe3f4ca254a4c58"
            ],
            "layout": "IPY_MODEL_2c0861cfc19d40649dbb4ff7cb39fd57"
          }
        },
        "8a7787eb416846eda265d65e60fae4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed7628b710f4bb2a65c1548c603ffb8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c3215918da924f3f993cf85d79242df4",
            "value": "modules.json:â€‡100%"
          }
        },
        "f94900f5bcbf486b802473c833732036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640531c22fac4d3ca23785a8933ed972",
            "max": 461,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dc80644302747238e95fb80548f0443",
            "value": 461
          }
        },
        "ba96c6d95ef34a359fe3f4ca254a4c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68aba7ae201405195ae2f01d6ad3ced",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_29698dba001342ea801ef886ada98be3",
            "value": "â€‡461/461â€‡[00:00&lt;00:00,â€‡61.6kB/s]"
          }
        },
        "2c0861cfc19d40649dbb4ff7cb39fd57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed7628b710f4bb2a65c1548c603ffb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3215918da924f3f993cf85d79242df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "640531c22fac4d3ca23785a8933ed972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dc80644302747238e95fb80548f0443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e68aba7ae201405195ae2f01d6ad3ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29698dba001342ea801ef886ada98be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d930f62003d6487682c461d0b17a587c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3fc01e468b743929f3478989a2de785",
              "IPY_MODEL_15cb215c5ed04759aedfe7b5f1246e95",
              "IPY_MODEL_0244a5508c62460fb4ba80f304e0ab2f"
            ],
            "layout": "IPY_MODEL_b710d46644df4b4e8aaf3850564739a2"
          }
        },
        "b3fc01e468b743929f3478989a2de785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02d5ec36e08743868a287c90a98233dd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ca3288b1a2a5486a8970cb21d6e484ab",
            "value": "config_sentence_transformers.json:â€‡100%"
          }
        },
        "15cb215c5ed04759aedfe7b5f1246e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96615fe238ce48188bd773f7f6382ffb",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e65329ca0f39480f9968a1517fd6b5bc",
            "value": 122
          }
        },
        "0244a5508c62460fb4ba80f304e0ab2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_825d0691c1b34a1f9ec83977b1bcdd19",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3045e6c39d264ecabf79ca21814604ff",
            "value": "â€‡122/122â€‡[00:00&lt;00:00,â€‡15.8kB/s]"
          }
        },
        "b710d46644df4b4e8aaf3850564739a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02d5ec36e08743868a287c90a98233dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca3288b1a2a5486a8970cb21d6e484ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96615fe238ce48188bd773f7f6382ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65329ca0f39480f9968a1517fd6b5bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "825d0691c1b34a1f9ec83977b1bcdd19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3045e6c39d264ecabf79ca21814604ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5940eadc90194d59aa81c1d18a1c75f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f66647483db843bb889e9ed37b164325",
              "IPY_MODEL_7da20044020c4b79bc6e6c137aae7733",
              "IPY_MODEL_b74ea58c2b294e8096758210aa785210"
            ],
            "layout": "IPY_MODEL_145b8909aad44d1ea182fbc8cd870c72"
          }
        },
        "f66647483db843bb889e9ed37b164325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e273f91657a845749101f79b392925d4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f1e903ef0fb94642bac05c907995f9ce",
            "value": "README.md:â€‡"
          }
        },
        "7da20044020c4b79bc6e6c137aae7733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_082d211f3fc247debd7b5bbfd37a38ba",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74890f11b62a4b8c91fabda11179cfe5",
            "value": 1
          }
        },
        "b74ea58c2b294e8096758210aa785210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68c91b454b164fd79dd468376bdc2e74",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5748d35243164012aad722b758d3f5e0",
            "value": "â€‡2.02k/?â€‡[00:00&lt;00:00,â€‡245kB/s]"
          }
        },
        "145b8909aad44d1ea182fbc8cd870c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e273f91657a845749101f79b392925d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e903ef0fb94642bac05c907995f9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "082d211f3fc247debd7b5bbfd37a38ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "74890f11b62a4b8c91fabda11179cfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68c91b454b164fd79dd468376bdc2e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5748d35243164012aad722b758d3f5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e7e0ce1f2d9474990e9e79d8e6e0599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e452e8bae2a6401f9461bfd28443b59f",
              "IPY_MODEL_b9184984b61943739daacef2f5f66e09",
              "IPY_MODEL_9e854199f12946339cf5c63860b0c32c"
            ],
            "layout": "IPY_MODEL_60ff22f8c6c84adcb3a2ca65e4fc9c64"
          }
        },
        "e452e8bae2a6401f9461bfd28443b59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f4baeade8c843089039332c2e1b9871",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_47db75e67b054a7c9c6b0be57d5d4646",
            "value": "sentence_bert_config.json:â€‡100%"
          }
        },
        "b9184984b61943739daacef2f5f66e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68887d8ea6e7460c88beb7573d993685",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99f206bf17ea4c8984cec076a2c48f7c",
            "value": 53
          }
        },
        "9e854199f12946339cf5c63860b0c32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_414739cb67e9428da3e1881d6d5890c8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_943774a0d58449edaf1caa5f020614a8",
            "value": "â€‡53.0/53.0â€‡[00:00&lt;00:00,â€‡7.07kB/s]"
          }
        },
        "60ff22f8c6c84adcb3a2ca65e4fc9c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4baeade8c843089039332c2e1b9871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47db75e67b054a7c9c6b0be57d5d4646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68887d8ea6e7460c88beb7573d993685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99f206bf17ea4c8984cec076a2c48f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "414739cb67e9428da3e1881d6d5890c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943774a0d58449edaf1caa5f020614a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "027b6c839fad460289acfcbf18a1fc2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8647fb0b68f343cbb2a1edbe756e8842",
              "IPY_MODEL_a8b841315bca4cb5aa502f8159e9291b",
              "IPY_MODEL_56ded534d55f41db8489304da45f0f73"
            ],
            "layout": "IPY_MODEL_18c9ade6853e4a198dc9da7632c2ef02"
          }
        },
        "8647fb0b68f343cbb2a1edbe756e8842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f336339c465947ffb1c5606ca3298dc0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8f10c786e9b444ce8b3e595558d02654",
            "value": "config.json:â€‡100%"
          }
        },
        "a8b841315bca4cb5aa502f8159e9291b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bac8823a55447dda45efc2d7b505b0c",
            "max": 804,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_275dc9e9f2b04bcfa58ac10ad489b021",
            "value": 804
          }
        },
        "56ded534d55f41db8489304da45f0f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21332b1b2b4a4bceb1402324b7d3db8c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_341491e5ccb54632aa59a24f88e65ccb",
            "value": "â€‡804/804â€‡[00:00&lt;00:00,â€‡106kB/s]"
          }
        },
        "18c9ade6853e4a198dc9da7632c2ef02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f336339c465947ffb1c5606ca3298dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f10c786e9b444ce8b3e595558d02654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bac8823a55447dda45efc2d7b505b0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "275dc9e9f2b04bcfa58ac10ad489b021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21332b1b2b4a4bceb1402324b7d3db8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341491e5ccb54632aa59a24f88e65ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "956eb04c5dc14961a70d4c771433b7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c0f4031a2134fa5b17381abe7920ddc",
              "IPY_MODEL_e7c7b6d02a2e434aa461f296c6aa8fba",
              "IPY_MODEL_2728eda4af5f4e50bdf8f05c7cbcbc46"
            ],
            "layout": "IPY_MODEL_ea26d1fe3c3143f98c3b5e4ff5f9875a"
          }
        },
        "7c0f4031a2134fa5b17381abe7920ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac07ac748e6d46f781f3c6bb0a0d047c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_196680bca63945dc9b1cfb1cfd37f8fa",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "e7c7b6d02a2e434aa461f296c6aa8fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74bb9607d5aa45259a98a29cc7bf486a",
            "max": 1883734344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c222916d93bd48b6a1ce12a4c425be74",
            "value": 1883734344
          }
        },
        "2728eda4af5f4e50bdf8f05c7cbcbc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fe84526557f4a7bb863b912e183d0ab",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d8ba76c6050a4dbeb3809eda7e1a94f3",
            "value": "â€‡1.88G/1.88Gâ€‡[00:04&lt;00:00,â€‡689MB/s]"
          }
        },
        "ea26d1fe3c3143f98c3b5e4ff5f9875a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac07ac748e6d46f781f3c6bb0a0d047c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196680bca63945dc9b1cfb1cfd37f8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74bb9607d5aa45259a98a29cc7bf486a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c222916d93bd48b6a1ce12a4c425be74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fe84526557f4a7bb863b912e183d0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ba76c6050a4dbeb3809eda7e1a94f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8d99e8bd21f4ef4800db523f81e4035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd4700a901184174801d6d3df88fbaab",
              "IPY_MODEL_c0d198a7bfba466097bf5c9b0049036f",
              "IPY_MODEL_210468fd35e142a38cf6eb384bba2f04"
            ],
            "layout": "IPY_MODEL_9b6f07d3b7ce48428083fc0cfef6b03d"
          }
        },
        "dd4700a901184174801d6d3df88fbaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27abb6beddcc41be9bb3b945e4cb033a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ec626a94ae3481ea9daaea00ef9d01c",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "c0d198a7bfba466097bf5c9b0049036f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f2a90e6f8fd42ce9b85e9bc87c5a85d",
            "max": 397,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_480082e3c2a74b999649bd41c5fc8391",
            "value": 397
          }
        },
        "210468fd35e142a38cf6eb384bba2f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83927c966882465791371d6cee8701cc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_058ff631ef3c4b52935aa32043896547",
            "value": "â€‡397/397â€‡[00:00&lt;00:00,â€‡46.0kB/s]"
          }
        },
        "9b6f07d3b7ce48428083fc0cfef6b03d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27abb6beddcc41be9bb3b945e4cb033a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec626a94ae3481ea9daaea00ef9d01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f2a90e6f8fd42ce9b85e9bc87c5a85d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "480082e3c2a74b999649bd41c5fc8391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83927c966882465791371d6cee8701cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "058ff631ef3c4b52935aa32043896547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "443755b2a613411b9aa12ece661866e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f705aca58f044ceaf665ef4c912f45f",
              "IPY_MODEL_fbeda64cce684010ae720e14ca69db45",
              "IPY_MODEL_4e8683bf43ab41d2a6da69fe48ae4c1b"
            ],
            "layout": "IPY_MODEL_5b9eacd80d2347198bc31650456b0aa1"
          }
        },
        "6f705aca58f044ceaf665ef4c912f45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5935a527a79d4c24989e2a66e4087928",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a79eaf0b74be48c597951668d3de1b7b",
            "value": "vocab.txt:â€‡"
          }
        },
        "fbeda64cce684010ae720e14ca69db45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c16edec7a564af6922a3167acfd961c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bf826bc07d94ee0a7ac7164ede650b4",
            "value": 1
          }
        },
        "4e8683bf43ab41d2a6da69fe48ae4c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d1a112c75424537bd0da8d101013c88",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_adfe21709c5b45ed8db53278429ea85d",
            "value": "â€‡5.22M/?â€‡[00:00&lt;00:00,â€‡8.08MB/s]"
          }
        },
        "5b9eacd80d2347198bc31650456b0aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5935a527a79d4c24989e2a66e4087928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a79eaf0b74be48c597951668d3de1b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c16edec7a564af6922a3167acfd961c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3bf826bc07d94ee0a7ac7164ede650b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d1a112c75424537bd0da8d101013c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adfe21709c5b45ed8db53278429ea85d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8805b42db7846619c955aefe9ccdb4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e12df703270461ab5a06b347750ac94",
              "IPY_MODEL_09a8be294d1741a6a0a5b67c48e2f55d",
              "IPY_MODEL_eb5c56b0969547198b91c13dc80a44cc"
            ],
            "layout": "IPY_MODEL_4d48470bbc394fd6b68d0854153de703"
          }
        },
        "1e12df703270461ab5a06b347750ac94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b20918ee48c44daea1d6dc7cf4daccef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_387298101df14924a5c926a9431b995e",
            "value": "tokenizer.json:â€‡"
          }
        },
        "09a8be294d1741a6a0a5b67c48e2f55d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1500d7cff8ed4a6681e66a982cc39082",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d585a9e8211e40d98e6e5fdcc08664c2",
            "value": 1
          }
        },
        "eb5c56b0969547198b91c13dc80a44cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a1c7cc2d28b480bbe5732bc2af705b0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6fc9ba7171a741c8ac57c19fb64b0edd",
            "value": "â€‡9.62M/?â€‡[00:00&lt;00:00,â€‡14.1MB/s]"
          }
        },
        "4d48470bbc394fd6b68d0854153de703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20918ee48c44daea1d6dc7cf4daccef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "387298101df14924a5c926a9431b995e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1500d7cff8ed4a6681e66a982cc39082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d585a9e8211e40d98e6e5fdcc08664c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a1c7cc2d28b480bbe5732bc2af705b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc9ba7171a741c8ac57c19fb64b0edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2a810cacbe646d988592242c1fce30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5e4ca98b7d94e9c89e4e7027adb57aa",
              "IPY_MODEL_f4f955e6dd8c42c28e7de7f1a1b441ba",
              "IPY_MODEL_2f45bfcbddc3415596f31d39f8455fe9"
            ],
            "layout": "IPY_MODEL_9592ad33bca44acf80ff32a8687fe52f"
          }
        },
        "a5e4ca98b7d94e9c89e4e7027adb57aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1841a07954c4bcca4f3e843112bc39a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_89ec0ae7b78d4d6c80be8d11169041ec",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "f4f955e6dd8c42c28e7de7f1a1b441ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99d0b465e79c4ead80e46aacbb1c0dd0",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_313d20956cfd4cbebba59ecf49736d56",
            "value": 112
          }
        },
        "2f45bfcbddc3415596f31d39f8455fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb8768695b54a6d881f675948cb212d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_03b2d41ba8c640e993675b3359afc0e9",
            "value": "â€‡112/112â€‡[00:00&lt;00:00,â€‡13.7kB/s]"
          }
        },
        "9592ad33bca44acf80ff32a8687fe52f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1841a07954c4bcca4f3e843112bc39a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89ec0ae7b78d4d6c80be8d11169041ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99d0b465e79c4ead80e46aacbb1c0dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "313d20956cfd4cbebba59ecf49736d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0eb8768695b54a6d881f675948cb212d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03b2d41ba8c640e993675b3359afc0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fd0d87b14334731828df95932b2655b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bf7de3e296642e0a132a2bea24d3aa9",
              "IPY_MODEL_ab0041c1a9d14d28ac90b705405e743f",
              "IPY_MODEL_46590fd4bc384d10a28b68ef2bf24427"
            ],
            "layout": "IPY_MODEL_dee3374f15744cf39e2d8c5357b9cc2f"
          }
        },
        "8bf7de3e296642e0a132a2bea24d3aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e808543f907c427a9dd66780057f7a07",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_65f7d3b800114779a5cb0a87ede886ae",
            "value": "config.json:â€‡100%"
          }
        },
        "ab0041c1a9d14d28ac90b705405e743f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac4eb90b009742068bd3109c406a2a3b",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37188d5bbd6548eabdd8de9447c27300",
            "value": 190
          }
        },
        "46590fd4bc384d10a28b68ef2bf24427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2ed533a0c5e47c2bd3583423be8634d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_03408157efe045b88e4501bd1b5656bb",
            "value": "â€‡190/190â€‡[00:00&lt;00:00,â€‡24.3kB/s]"
          }
        },
        "dee3374f15744cf39e2d8c5357b9cc2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e808543f907c427a9dd66780057f7a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65f7d3b800114779a5cb0a87ede886ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac4eb90b009742068bd3109c406a2a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37188d5bbd6548eabdd8de9447c27300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2ed533a0c5e47c2bd3583423be8634d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03408157efe045b88e4501bd1b5656bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d8134afd6bd4dc49fdeb6b479af6e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e06599aebcde428ea19fa7a06a8b2bd3",
              "IPY_MODEL_d915397cbc1f4a8385c963fa93b86d51",
              "IPY_MODEL_46870b53139b44ccb2468d76bb137ebe"
            ],
            "layout": "IPY_MODEL_6a809be34f5a435dbc44daeed22c0f96"
          }
        },
        "e06599aebcde428ea19fa7a06a8b2bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_675040c1ecf14c7ab5810abf56fd8487",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_26ac0b1c5d7a46f5bd9d78d122cadcb6",
            "value": "config.json:â€‡100%"
          }
        },
        "d915397cbc1f4a8385c963fa93b86d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ccda064d07d4a49b7bea0d3545afcfa",
            "max": 114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5d81c15e1db4d2c873658499ee05ea6",
            "value": 114
          }
        },
        "46870b53139b44ccb2468d76bb137ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f2501083cbf456683d3493b0823157c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_41a2a02d63eb402c960b663365f60997",
            "value": "â€‡114/114â€‡[00:00&lt;00:00,â€‡15.1kB/s]"
          }
        },
        "6a809be34f5a435dbc44daeed22c0f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675040c1ecf14c7ab5810abf56fd8487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ac0b1c5d7a46f5bd9d78d122cadcb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ccda064d07d4a49b7bea0d3545afcfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d81c15e1db4d2c873658499ee05ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f2501083cbf456683d3493b0823157c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a2a02d63eb402c960b663365f60997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04343964cec64660aee32ecae9dd0629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87e1d29a34bb46a8968304b2429fb50f",
              "IPY_MODEL_f9290de12fd24320bda378b0ac970165",
              "IPY_MODEL_9e996fd531684e61aeb6162a9c27b9ed"
            ],
            "layout": "IPY_MODEL_a88d13392508486b8248db630819d55d"
          }
        },
        "87e1d29a34bb46a8968304b2429fb50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928354ac84b24198af3db60c68cc5e90",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_14672e3c8b2c4e40ab2ee009739bb5e2",
            "value": "2_Dense/model.safetensors:â€‡100%"
          }
        },
        "f9290de12fd24320bda378b0ac970165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8521950be3c4a2b96cbe9522bc21f34",
            "max": 2362528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_215d91dbf7c941a1940d1c9d65b9d724",
            "value": 2362528
          }
        },
        "9e996fd531684e61aeb6162a9c27b9ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_779b943511c0486fa82f1bf54b7cdc9d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_704acf7381fe4ba4ab791a684d140c3c",
            "value": "â€‡2.36M/2.36Mâ€‡[00:00&lt;00:00,â€‡3.93MB/s]"
          }
        },
        "a88d13392508486b8248db630819d55d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "928354ac84b24198af3db60c68cc5e90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14672e3c8b2c4e40ab2ee009739bb5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8521950be3c4a2b96cbe9522bc21f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215d91dbf7c941a1940d1c9d65b9d724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "779b943511c0486fa82f1bf54b7cdc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "704acf7381fe4ba4ab791a684d140c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b0c50c07d0249dc81d40df2c04785f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_750713b5acab407eb1e83ea7500740ed",
              "IPY_MODEL_bbfccdef77614bffb5caf16eda556fc1",
              "IPY_MODEL_daa085de0a2e4991a866bbded3956880"
            ],
            "layout": "IPY_MODEL_7965815e6ec94afab00f887e6bcbeb29"
          }
        },
        "750713b5acab407eb1e83ea7500740ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_586e84451055479cb7b88e7705d13f03",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_710acdc9ae3a4e958dbee4b8b5a0e3d6",
            "value": "Evaluating:â€‡100%"
          }
        },
        "bbfccdef77614bffb5caf16eda556fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a5e5667057e46ed842736508ebb4bd9",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4e4e2091a3946788a81b028eda7d1da",
            "value": 1500
          }
        },
        "daa085de0a2e4991a866bbded3956880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05ed0c372a24e45add72e3aab66b4a2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_65e5cac368eb4798aa81e452770e5ded",
            "value": "â€‡1500/1500â€‡[6:41:50&lt;00:00,â€‡â€‡6.69s/it]"
          }
        },
        "7965815e6ec94afab00f887e6bcbeb29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "586e84451055479cb7b88e7705d13f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "710acdc9ae3a4e958dbee4b8b5a0e3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a5e5667057e46ed842736508ebb4bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4e4e2091a3946788a81b028eda7d1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e05ed0c372a24e45add72e3aab66b4a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e5cac368eb4798aa81e452770e5ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}